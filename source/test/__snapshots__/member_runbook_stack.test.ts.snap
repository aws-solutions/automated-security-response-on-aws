// Jest Snapshot v1, https://jestjs.io/docs/snapshot-testing

exports[`MemberRolesStack tests Global Roles Stack 1`] = `
{
  "Description": "test;",
  "Parameters": {
    "Namespace": {
      "AllowedPattern": "(?!(^xn--|^sthree-|^sthree-configurator|^amzn-s3-demo-|.+-s3alias|.+--ol-s3|.+.mrap|.+--x-s3$))^[a-z0-9][a-z0-9-]{1,7}[a-z0-9]$",
      "ConstraintDescription": "The Namespace parameter must follow naming restrictions for S3 buckets and have a minimum length of 3 and a maximum length of 9. https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html",
      "Description": "Choose a unique namespace to be added as a suffix to remediation IAM role names. The same namespace should be used in the Member Roles and Member stacks. This string should be unique for each solution deployment, but does not need to be changed during stack updates.",
      "MaxLength": 9,
      "MinLength": 3,
      "Type": "String",
    },
    "SecHubAdminAccount": {
      "AllowedPattern": "^\\d{12}$",
      "Description": "Admin account number",
      "Type": "String",
    },
  },
  "Resources": {
    "OrchestratorMemberRoleMemberAccountRoleBE9AD9D5": {
      "Metadata": {
        "cfn_nag": {
          "rules_to_suppress": [
            {
              "id": "W11",
              "reason": "Resource * is required due to the administrative nature of the solution.",
            },
            {
              "id": "W28",
              "reason": "Static names chosen intentionally to provide integration in cross-account permissions",
            },
          ],
        },
        "guard": {
          "SuppressedRules": [
            "IAM_NO_INLINE_POLICY_CHECK",
          ],
        },
      },
      "Properties": {
        "AssumeRolePolicyDocument": {
          "Statement": [
            {
              "Action": "sts:AssumeRole",
              "Effect": "Allow",
              "Principal": {
                "AWS": {
                  "Fn::Join": [
                    "",
                    [
                      "arn:",
                      {
                        "Ref": "AWS::Partition",
                      },
                      ":iam::",
                      {
                        "Ref": "SecHubAdminAccount",
                      },
                      ":role/SO0111-ASR-Orchestrator-Admin",
                    ],
                  ],
                },
              },
            },
            {
              "Action": "sts:AssumeRole",
              "Effect": "Allow",
              "Principal": {
                "Service": "ssm.amazonaws.com",
              },
            },
          ],
          "Version": "2012-10-17",
        },
        "Policies": [
          {
            "PolicyDocument": {
              "Statement": [
                {
                  "Action": [
                    "iam:PassRole",
                    "iam:GetRole",
                  ],
                  "Effect": "Allow",
                  "Resource": {
                    "Fn::Join": [
                      "",
                      [
                        "arn:",
                        {
                          "Ref": "AWS::Partition",
                        },
                        ":iam::",
                        {
                          "Ref": "AWS::AccountId",
                        },
                        ":role/SO0111-*",
                      ],
                    ],
                  },
                },
                {
                  "Action": "ssm:StartAutomationExecution",
                  "Effect": "Allow",
                  "Resource": [
                    {
                      "Fn::Join": [
                        "",
                        [
                          "arn:",
                          {
                            "Ref": "AWS::Partition",
                          },
                          ":ssm:*:",
                          {
                            "Ref": "AWS::AccountId",
                          },
                          ":document/ASR-*",
                        ],
                      ],
                    },
                    {
                      "Fn::Join": [
                        "",
                        [
                          "arn:",
                          {
                            "Ref": "AWS::Partition",
                          },
                          ":ssm:*:",
                          {
                            "Ref": "AWS::AccountId",
                          },
                          ":automation-definition/*",
                        ],
                      ],
                    },
                    {
                      "Fn::Join": [
                        "",
                        [
                          "arn:",
                          {
                            "Ref": "AWS::Partition",
                          },
                          ":ssm:*::automation-definition/*",
                        ],
                      ],
                    },
                    {
                      "Fn::Join": [
                        "",
                        [
                          "arn:",
                          {
                            "Ref": "AWS::Partition",
                          },
                          ":ssm:*:",
                          {
                            "Ref": "AWS::AccountId",
                          },
                          ":automation-execution/*",
                        ],
                      ],
                    },
                  ],
                },
                {
                  "Action": [
                    "ssm:DescribeAutomationExecutions",
                    "ssm:GetAutomationExecution",
                  ],
                  "Effect": "Allow",
                  "Resource": "*",
                },
                {
                  "Action": "ssm:DescribeDocument",
                  "Effect": "Allow",
                  "Resource": {
                    "Fn::Join": [
                      "",
                      [
                        "arn:",
                        {
                          "Ref": "AWS::Partition",
                        },
                        ":ssm:*:*:document/*",
                      ],
                    ],
                  },
                },
                {
                  "Action": [
                    "ssm:GetParameters",
                    "ssm:GetParameter",
                  ],
                  "Effect": "Allow",
                  "Resource": {
                    "Fn::Join": [
                      "",
                      [
                        "arn:",
                        {
                          "Ref": "AWS::Partition",
                        },
                        ":ssm:*:*:parameter/Solutions/SO0111/*",
                      ],
                    ],
                  },
                },
                {
                  "Action": "config:DescribeConfigRules",
                  "Effect": "Allow",
                  "Resource": "*",
                },
                {
                  "Action": [
                    "cloudwatch:PutMetricData",
                    "securityhub:BatchUpdateFindings",
                  ],
                  "Effect": "Allow",
                  "Resource": "*",
                },
              ],
              "Version": "2012-10-17",
            },
            "PolicyName": "member_orchestrator",
          },
        ],
        "RoleName": "SO0111-ASR-Orchestrator-Member",
      },
      "Type": "AWS::IAM::Role",
    },
  },
}
`;

exports[`MemberRolesStack tests Regional Documents 1`] = `
{
  "Description": "test;",
  "Parameters": {
    "Namespace": {
      "AllowedPattern": "(?!(^xn--|^sthree-|^sthree-configurator|^amzn-s3-demo-|.+-s3alias|.+--ol-s3|.+.mrap|.+--x-s3$))^[a-z0-9][a-z0-9-]{1,7}[a-z0-9]$",
      "ConstraintDescription": "The Namespace parameter must follow naming restrictions for S3 buckets and have a minimum length of 3 and a maximum length of 9. https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html",
      "Description": "Choose a unique namespace to be added as a suffix to remediation IAM role names. The same namespace should be used in the Member Roles and Member stacks. This string should be unique for each solution deployment, but does not need to be changed during stack updates.",
      "MaxLength": 9,
      "MinLength": 3,
      "Type": "String",
    },
    "WaitProviderServiceToken": {
      "Type": "String",
    },
  },
  "Resources": {
    "ASRAttachSSMPermissionsToEC2": {
      "DependsOn": [
        "CreateWait9",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-AttachSSMPermissionsToEC2

## What does this document do?
This document adds the necessary permissions for SSM to begin managing the EC2 Instance.
If the EC2 instance has an existing instance profile, it will add SSM permissions to the existing role if they are not already present.
If the EC2 instance does not have an existing instance profile, it will attach a new profile and role with the correct permissions.

## Input Parameters
* InstanceArn: (Required) EC2 Instance ARN
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Security Standards / Controls
* AWS FSBP v1.0.0:   SSM.1
* PCI:            PCI.SSM.3

## Output Parameters
* AttachSSMPermissionsToEC2.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Adds necessary permissions for SSM to begin managing the EC2 Instance.
",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "InstanceArn": "{{InstanceArn}}",
                  "InstanceProfile": "{{InstanceProfile}}",
                  "RemediationRole": "{{RemediationRole}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import Optional

import boto3
from botocore.config import Config

SSM_MANAGED_POLICY_ARN = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"

boto_config = Config(retries={"mode": "standard"})


def connect_to_iam():
    return boto3.client("iam", config=boto_config)


def connect_to_ec2():
    return boto3.client("ec2", config=boto_config)


def lambda_handler(event, _):
    """
    Remediates SSM.1 by attaching the required
    permissions to the EC2 Instance to enable
    SSM management.
    """
    try:
        instance_arn = event["InstanceArn"]
        instance_id = instance_arn.split("/")[1]
        remediation_role = event["RemediationRole"]
        remediation_instance_profile_name = event["InstanceProfile"]

        existing_instance_profile = get_existing_instance_profile(instance_id)
        existing_iam_role_from_instance_profile = (
            get_iam_role_from_instance_profile(existing_instance_profile)
            if existing_instance_profile
            else None
        )

        if existing_iam_role_from_instance_profile:
            attach_ssm_managed_iam_policy(existing_iam_role_from_instance_profile)

        ssm_management_iam_role = (
            existing_iam_role_from_instance_profile or remediation_role
        )

        instance_profile = (
            existing_instance_profile or remediation_instance_profile_name
        )

        if existing_instance_profile and not existing_iam_role_from_instance_profile:
            setup_instance_profile(ssm_management_iam_role, instance_profile)

        if not existing_instance_profile:
            attach_instance_profile(instance_profile, instance_id)

        return {
            "message": f"Successfully added SSM permissions to instance {instance_id}.",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(f"Encountered error remediating SSM.1: {str(e)}")


def get_existing_instance_profile(instance_id: str) -> Optional[str]:
    ec2_client = connect_to_ec2()

    try:
        response = ec2_client.describe_iam_instance_profile_associations(
            Filters=[
                {"Name": "instance-id", "Values": [instance_id]},
                {"Name": "state", "Values": ["associated"]},
            ]
        )
        if response["IamInstanceProfileAssociations"]:
            return response["IamInstanceProfileAssociations"][0]["IamInstanceProfile"][
                "Arn"
            ].split("/")[1]
        else:
            return None
    except Exception as e:
        raise RuntimeError(
            f"Failed to describe profile attached to instance {instance_id}: {str(e)}"
        )


def get_iam_role_from_instance_profile(profile_name: str) -> Optional[str]:
    iam_client = connect_to_iam()

    try:
        response = iam_client.get_instance_profile(InstanceProfileName=profile_name)
        if response["InstanceProfile"]["Roles"]:
            # there can only be one role attached to an instance profile
            return response["InstanceProfile"]["Roles"][0]["RoleName"]
    except Exception as e:
        raise RuntimeError(
            f"Failed to get role from instance profile {profile_name}: {str(e)}"
        )

    return None


def attach_ssm_managed_iam_policy(role_name: str) -> None:
    iam_client = connect_to_iam()

    try:
        if not is_ssm_managed_policy_attached(role_name):
            iam_client.attach_role_policy(
                RoleName=role_name,
                PolicyArn=SSM_MANAGED_POLICY_ARN,
            )
    except Exception as e:
        raise RuntimeError(f"Failed to attach SSM managed policy to IAM role: {str(e)}")


def is_ssm_managed_policy_attached(role_name: str) -> bool:
    iam_client = connect_to_iam()

    try:
        paginator = iam_client.get_paginator("list_attached_role_policies")
        page_iterator = paginator.paginate(RoleName=role_name)

        attached_policies = []
        for page in page_iterator:
            attached_policies.extend(page["AttachedPolicies"])

        for policy in attached_policies:
            if policy["PolicyArn"] == SSM_MANAGED_POLICY_ARN:
                return True
    except Exception as e:
        print(f"Failed to list attached policies for IAM role {role_name}: {str(e)}")

    return False


def setup_instance_profile(role_name: str, instance_profile_name: str) -> None:
    iam_client = connect_to_iam()

    try:
        iam_client.add_role_to_instance_profile(
            InstanceProfileName=instance_profile_name,
            RoleName=role_name,
        )
    except Exception as e:
        raise RuntimeError(
            f"Failed to add role {role_name} to instance profile {instance_profile_name}: {str(e)}"
        )


def attach_instance_profile(profile_name: str, instance_id: str) -> None:
    ec2_client = connect_to_ec2()
    try:
        ec2_client.associate_iam_instance_profile(
            IamInstanceProfile={"Name": profile_name},
            InstanceId=instance_id,
        )
    except Exception as e:
        raise RuntimeError(
            f"Failed to associate instance profile {profile_name} with instance {instance_id}: {str(e)}\\n It is "
            f"likely that there is already an instance profile associated with the Instance, "
            f"use the describe-iam-instance-profile-associations command to verify."
        )",
              },
              "isEnd": true,
              "name": "AttachSSMPermissionsToEC2",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "AttachSSMPermissionsToEC2.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "InstanceArn": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):ec2:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:instance\\/(i-[0-9a-f]*)$",
              "description": "(Required) The document ARN.",
              "type": "String",
            },
            "InstanceProfile": {
              "allowedPattern": "^.+-AttachSSMPermissionsToEC2-InstanceProfile-.+$",
              "description": "(Required) The name of the Instance profile with SSM managed permissions for EC2.",
              "type": "String",
            },
            "RemediationRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that with SSM managed permissions for EC2.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-AttachSSMPermissionsToEC2",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRAttachServiceVPCEndpoint": {
      "DependsOn": [
        "CreateWait9",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-AttachServiceVPCEndpoint

## Overview
This document creates and attaches a service interface endpoint to the VPC.

## Pre-requisites
* None

## What does this document do?
Creates and attaches service interface endpoint to VPC; by default, all subnets in the VPC are given access to the endpoint.

## Input Parameters
* ResourceArn: (Required) VPC to be remediated.
* ServiceName: (Required) Name of the AWS Service for which an endpoint should be created (e.g. 'ec2').
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* AttachServiceVPCEndpoint.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Creates and attaches service interface endpoint to VPC.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "Region": "{{global:REGION}}",
                  "ServiceName": "{{ServiceName}}",
                  "VPCId": "{{VPCId}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import List, Optional, TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


def connect_to_ec2():
    return boto3.client("ec2", config=boto_config)


class Event(TypedDict):
    ServiceName: str
    Region: str
    VPCId: str


def handler(event: Event, _):
    """
    Remediates by creating and attaching
    an AWS service endpoint to the VPC.
    """
    try:
        service_name = event["ServiceName"]
        aws_region = event["Region"]
        vpc_id = event["VPCId"]

        subnets = get_subnets(vpc_id)
        service_endpoint_name = get_service_endpoint_name(aws_region, service_name)

        vpc_endpoint_id = attach_vpc_endpoint(vpc_id, subnets, service_endpoint_name)
        return {
            "Message": (
                f"Successfully attached service endpoint {service_endpoint_name} to VPC {vpc_id}."
            ),
            "Status": "success",
            "VpcEndpointId": vpc_endpoint_id,
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error while attaching service endpoint to VPC: {str(e)}"
        )


def get_subnets(vpc_id: str) -> Optional[List[str]]:
    ec2_client = connect_to_ec2()
    try:
        paginator = ec2_client.get_paginator("describe_subnets")
        page_iterator = paginator.paginate(
            Filters=[
                {
                    "Name": "vpc-id",
                    "Values": [vpc_id],
                }
            ]
        )

        # Collect subnets with their availability zones
        subnets_by_az = {}
        for page in page_iterator:
            for subnet in page["Subnets"]:
                if "SubnetId" in subnet and "AvailabilityZone" in subnet:
                    az = subnet["AvailabilityZone"]
                    # Keep only one subnet per AZ (first one found)
                    if az not in subnets_by_az:
                        subnets_by_az[az] = subnet["SubnetId"]

        return list(subnets_by_az.values())
    except Exception as e:
        raise RuntimeError(f"Failed to list subnets in VPC {vpc_id}: {str(e)}")


def get_service_endpoint_name(aws_region: str, service_name: str) -> str:
    prefix = "cn." if aws_region in ["cn-north-1", "cn-northwest-1"] else ""
    return f"{prefix}com.amazonaws.{aws_region}.{service_name}"


def attach_vpc_endpoint(
    vpc_id: str, subnets: List[str], service_endpoint_name: str
) -> str:
    ec2_client = connect_to_ec2()
    try:
        dns_enabled = is_dns_enabled(vpc_id)
        response = ec2_client.create_vpc_endpoint(
            VpcEndpointType="Interface",
            VpcId=vpc_id,
            ServiceName=service_endpoint_name,
            SubnetIds=subnets,
            PrivateDnsEnabled=dns_enabled,
        )
        return response["VpcEndpoint"]["VpcEndpointId"]
    except Exception as e:
        raise RuntimeError(
            f"Failed to attach service endpoint {service_endpoint_name} to VPC {vpc_id}: {str(e)}"
        )


def is_dns_enabled(vpc_id: str) -> bool:
    ec2_client = connect_to_ec2()
    try:
        dns_support_enabled = ec2_client.describe_vpc_attribute(
            Attribute="enableDnsSupport",
            VpcId=vpc_id,
        )["EnableDnsSupport"]["Value"]

        dns_hostnames_enabled = ec2_client.describe_vpc_attribute(
            Attribute="enableDnsHostnames",
            VpcId=vpc_id,
        )["EnableDnsHostnames"]["Value"]

        return dns_support_enabled and dns_hostnames_enabled
    except Exception as e:
        raise RuntimeError(f"Failed to get VPC attributes for {vpc_id}: {str(e)}")",
              },
              "name": "AttachServiceVPCEndpoint",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "AttachServiceVPCEndpoint.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ServiceName": {
              "allowedPattern": "^[A-Za-z0-9]*$",
              "description": "(Required) Name of the AWS Service for which an endpoint should be created (e.g. 'ec2').",
              "type": "String",
            },
            "VPCId": {
              "allowedPattern": "^vpc-[0-9a-f]{8,17}$",
              "description": "(Required) The VPC resource ARN.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-AttachServiceVPCEndpoint",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRBlockSSMDocumentPublicAccess": {
      "DependsOn": [
        "CreateWait9",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-BlockSSMDocumentPublicAccess

## What does this document do?
This document modifies SSM document permissions to prevent cross-account public access.

## Input Parameters
* DocumentArn: (Required) SSM Document name to be changed.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* BlockSSMDocumentPublicAccess.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Removes public access to the SSM Document
",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "document_arn": "{{DocumentArn}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
"""
Remediates SSM.4 and SSM.7 by disabling public access to SSM documents
"""
from typing import TypedDict

import boto3
from botocore.config import Config


class EventType(TypedDict):
    accountid: str
    name: str


BOTO_CONFIG = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_ssm():
    return boto3.client("ssm", config=BOTO_CONFIG)


def get_document_name(event):
    """
    Extract document name from event supporting multiple input formats

    Supports:
    - DocumentArn: arn:aws:ssm:region:account:document/name (SSM.4/SSM.7)
    - DocumentName: name (direct name)
    - document_arn: arn:aws:ssm:region:account:document/name (legacy)
    """
    for key in ["DocumentArn", "DocumentName", "document_arn"]:
        if key in event:
            value = event[key]
            # Extract name from ARN if it contains '/'
            return value.split("/")[-1] if "/" in value else value

    raise ValueError(
        "Event must contain 'DocumentArn', 'DocumentName', or 'document_arn'"
    )


def lambda_handler(event: EventType, _):
    """
    Removes public access from an SSM document

    Returns:
        Dict with 'response': {'isPublic': 'False'} on success
    """
    document_name = get_document_name(event)
    ssm = connect_to_ssm()

    # Check current permissions
    response = ssm.describe_document_permission(
        Name=document_name, PermissionType="Share"
    )

    # Only modify if publicly shared
    if "all" not in response.get("AccountIds", []):
        return {"response": {"isPublic": "False"}}

    # Remove public access
    ssm.modify_document_permission(
        Name=document_name, AccountIdsToRemove=["all"], PermissionType="Share"
    )

    return {"response": {"isPublic": "False"}}",
              },
              "name": "BlockSSMDocumentPublicAccess",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "BlockSSMDocumentPublicAccess.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "DocumentArn": {
              "allowedPattern": "^(arn:(?:aws|aws-cn|aws-us-gov):ssm:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:document\\/[A-Za-z0-9][A-Za-z0-9\\-_]{1,254})$",
              "description": "(Required) The document ARN.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-BlockSSMDocumentPublicAccess",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRConfigureAutoScalingLaunchConfigNoPublicIP": {
      "DependsOn": [
        "CreateWait14",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-ConfigureAutoScalingLaunchConfigNoPublicIP

## What does this document do?
  This document configures AutoScaling Launch Configuration to not assign a public IP address

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* LaunchConfigurationName: (Required) The Name of the Launch Configuration.

## Security Standards / Controls
* NIST 800-53 Rev5: AutoScaling.5
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "DescribeLaunchConfigurations",
                "LaunchConfigurationNames": [
                  "{{LaunchConfigurationName}}",
                ],
                "Service": "autoscaling",
              },
              "isEnd": false,
              "name": "GetLaunchConfiguration",
              "outputs": [
                {
                  "Name": "LaunchConfiguration",
                  "Selector": "$.LaunchConfigurations[0]",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "append_suffix",
                "InputPayload": {
                  "MaxLen": 255,
                  "OriginalString": "{{LaunchConfigurationName}}",
                  "Suffix": "-NoPublicIP",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict


class AppendSuffixEvent(TypedDict):
    OriginalString: str
    MaxLen: int
    Suffix: str


def append_suffix(event: AppendSuffixEvent, _) -> str:
    prefix_len = event["MaxLen"] - len(event["Suffix"])
    new_name = event["OriginalString"][:prefix_len] + event["Suffix"]
    return new_name",
              },
              "name": "GetNewLaunchConfigurationName",
              "outputs": [
                {
                  "Name": "LaunchConfigurationName",
                  "Selector": "$.Payload",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "event_handler",
                "InputPayload": {
                  "LaunchConfiguration": "{{GetLaunchConfiguration.LaunchConfiguration}}",
                  "LaunchConfigurationName": "{{GetNewLaunchConfigurationName.LaunchConfigurationName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import Dict, TypedDict


class Event(TypedDict):
    LaunchConfiguration: Dict
    LaunchConfigurationName: str


def event_handler(event: Event, _):
    launch_configuration = event["LaunchConfiguration"]
    launch_configuration["LaunchConfigurationName"] = event["LaunchConfigurationName"]
    launch_configuration["AssociatePublicIpAddress"] = False
    del launch_configuration["LaunchConfigurationARN"]
    del launch_configuration["CreatedTime"]
    return launch_configuration",
              },
              "name": "UpdateLaunchConfiguration",
              "outputs": [
                {
                  "Name": "LaunchConfiguration",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_auto_scaling_launch_configuration",
                "InputPayload": {
                  "LaunchConfiguration": "{{UpdateLaunchConfiguration.LaunchConfiguration}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Dict, TypedDict

if TYPE_CHECKING:
    from mypy_boto3_apigateway import AutoScalingClient
else:
    AutoScalingClient = object

import boto3
from botocore.config import Config


def connect_to_auto_scaling(boto_config: Config) -> AutoScalingClient:
    return boto3.client("autoscaling", config=boto_config)


class Event(TypedDict):
    LaunchConfiguration: Dict


def create_auto_scaling_launch_configuration(event: Event, _):
    try:
        # If these are blank, the api call will fail
        if event["LaunchConfiguration"]["KernelId"] == "":
            del event["LaunchConfiguration"]["KernelId"]

        if event["LaunchConfiguration"]["RamdiskId"] == "":
            del event["LaunchConfiguration"]["RamdiskId"]

        autoscaling_client = connect_to_auto_scaling(
            Config(retries={"mode": "standard"})
        )
        autoscaling_client.create_launch_configuration(**event["LaunchConfiguration"])

        return {
            "message": "Successfully created auto scaling launch configuration",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered an error creating auto scaling launch configuration: {str(e)}"
        )",
              },
              "isEnd": false,
              "name": "CreateNewLaunchConfiguration",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "update_auto_scaling_groups_with_launch_configuration",
                "InputPayload": {
                  "NewLaunchConfigurationName": "{{GetNewLaunchConfigurationName.LaunchConfigurationName}}",
                  "OldLaunchConfigurationName": "{{LaunchConfigurationName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, TypedDict

if TYPE_CHECKING:
    from mypy_boto3_apigateway import AutoScalingClient
else:
    AutoScalingClient = object

import boto3
from botocore.config import Config


def connect_to_auto_scaling(boto_config: Config) -> AutoScalingClient:
    return boto3.client("autoscaling", config=boto_config)


class Event(TypedDict):
    OldLaunchConfigurationName: str
    NewLaunchConfigurationName: str


def update_auto_scaling_groups_with_launch_configuration(event: Event, _):
    try:
        auto_scaling_groups = get_auto_scaling_groups()
        update_auto_scaling_groups(
            auto_scaling_groups,
            event["OldLaunchConfigurationName"],
            event["NewLaunchConfigurationName"],
        )

        return {
            "message": "Successfully updated auto scaling groups with launch configuration",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered an error updating auto scaling groups: {str(e)}"
        )


def get_auto_scaling_groups():
    autoscaling_client = connect_to_auto_scaling(Config(retries={"mode": "standard"}))
    paginator = autoscaling_client.get_paginator("describe_auto_scaling_groups")
    page_iterator = paginator.paginate()

    auto_scaling_groups = []
    for page in page_iterator:
        auto_scaling_groups += page["AutoScalingGroups"]

    return auto_scaling_groups


def update_auto_scaling_groups(
    auto_scaling_groups,
    old_launch_configuration_name: str,
    new_launch_configuation_name: str,
):
    autoscaling_client = connect_to_auto_scaling(Config(retries={"mode": "standard"}))

    for auto_scaling_group in auto_scaling_groups:
        if (
            auto_scaling_group["LaunchConfigurationName"]
            != old_launch_configuration_name
        ):
            continue

        autoscaling_client.update_auto_scaling_group(
            AutoScalingGroupName=auto_scaling_group["AutoScalingGroupName"],
            LaunchConfigurationName=new_launch_configuation_name,
        )",
              },
              "isEnd": false,
              "name": "UpdateAutoScalingGroupsWithLaunchConfiguration",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "DeleteLaunchConfiguration",
                "LaunchConfigurationName": "{{LaunchConfigurationName}}",
                "Service": "autoscaling",
              },
              "isEnd": true,
              "name": "DeleteLaunchConfiguration",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "LaunchConfigurationName": {
              "allowedPattern": "^.{1,255}$",
              "description": "(Required) The name of the Launch Configuration.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-ConfigureAutoScalingLaunchConfigNoPublicIP",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRConfigureAutoScalingLaunchConfigToRequireIMDSv2": {
      "DependsOn": [
        "CreateWait14",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-ConfigureAutoScalingLaunchConfigToRequireIMDSv2

## What does this document do?
  This document configures AutoScaling Launch Configuration to use IMDSv2

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* LaunchConfigurationName: (Required) The Name of the Launch Configuration.

## Security Standards / Controls
* NIST 800-53 Rev5: AutoScaling.3
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "DescribeLaunchConfigurations",
                "LaunchConfigurationNames": [
                  "{{LaunchConfigurationName}}",
                ],
                "Service": "autoscaling",
              },
              "isEnd": false,
              "name": "GetLaunchConfiguration",
              "outputs": [
                {
                  "Name": "LaunchConfiguration",
                  "Selector": "$.LaunchConfigurations[0]",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "append_suffix",
                "InputPayload": {
                  "MaxLen": 255,
                  "OriginalString": "{{LaunchConfigurationName}}",
                  "Suffix": "-IMDSv2",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict


class AppendSuffixEvent(TypedDict):
    OriginalString: str
    MaxLen: int
    Suffix: str


def append_suffix(event: AppendSuffixEvent, _) -> str:
    prefix_len = event["MaxLen"] - len(event["Suffix"])
    new_name = event["OriginalString"][:prefix_len] + event["Suffix"]
    return new_name",
              },
              "name": "GetNewLaunchConfigurationName",
              "outputs": [
                {
                  "Name": "LaunchConfigurationName",
                  "Selector": "$.Payload",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "update_launch_configuration",
                "InputPayload": {
                  "LaunchConfiguration": "{{GetLaunchConfiguration.LaunchConfiguration}}",
                  "LaunchConfigurationName": "{{GetNewLaunchConfigurationName.LaunchConfigurationName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict


class UpdateLaunchConfigurationEvent(TypedDict):
    LaunchConfiguration: dict
    LaunchConfigurationName: str


def update_launch_configuration(event: UpdateLaunchConfigurationEvent, _) -> dict:
    launch_configuration = event["LaunchConfiguration"]
    launch_configuration["LaunchConfigurationName"] = event["LaunchConfigurationName"]
    launch_configuration["MetadataOptions"] = {
        "HttpTokens": "required",
        "HttpEndpoint": "enabled",
    }
    del launch_configuration["LaunchConfigurationARN"]
    del launch_configuration["CreatedTime"]
    return launch_configuration",
              },
              "name": "UpdateLaunchConfiguration",
              "outputs": [
                {
                  "Name": "LaunchConfiguration",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_auto_scaling_launch_configuration",
                "InputPayload": {
                  "LaunchConfiguration": "{{UpdateLaunchConfiguration.LaunchConfiguration}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, Dict, TypedDict

if TYPE_CHECKING:
    from mypy_boto3_apigateway import AutoScalingClient
else:
    AutoScalingClient = object

import boto3
from botocore.config import Config


def connect_to_auto_scaling(boto_config: Config) -> AutoScalingClient:
    return boto3.client("autoscaling", config=boto_config)


class Event(TypedDict):
    LaunchConfiguration: Dict


def create_auto_scaling_launch_configuration(event: Event, _):
    try:
        # If these are blank, the api call will fail
        if event["LaunchConfiguration"]["KernelId"] == "":
            del event["LaunchConfiguration"]["KernelId"]

        if event["LaunchConfiguration"]["RamdiskId"] == "":
            del event["LaunchConfiguration"]["RamdiskId"]

        autoscaling_client = connect_to_auto_scaling(
            Config(retries={"mode": "standard"})
        )
        autoscaling_client.create_launch_configuration(**event["LaunchConfiguration"])

        return {
            "message": "Successfully created auto scaling launch configuration",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered an error creating auto scaling launch configuration: {str(e)}"
        )",
              },
              "isEnd": false,
              "name": "CreateNewLaunchConfiguration",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "update_auto_scaling_groups_with_launch_configuration",
                "InputPayload": {
                  "NewLaunchConfigurationName": "{{GetNewLaunchConfigurationName.LaunchConfigurationName}}",
                  "OldLaunchConfigurationName": "{{LaunchConfigurationName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, TypedDict

if TYPE_CHECKING:
    from mypy_boto3_apigateway import AutoScalingClient
else:
    AutoScalingClient = object

import boto3
from botocore.config import Config


def connect_to_auto_scaling(boto_config: Config) -> AutoScalingClient:
    return boto3.client("autoscaling", config=boto_config)


class Event(TypedDict):
    OldLaunchConfigurationName: str
    NewLaunchConfigurationName: str


def update_auto_scaling_groups_with_launch_configuration(event: Event, _):
    try:
        auto_scaling_groups = get_auto_scaling_groups()
        update_auto_scaling_groups(
            auto_scaling_groups,
            event["OldLaunchConfigurationName"],
            event["NewLaunchConfigurationName"],
        )

        return {
            "message": "Successfully updated auto scaling groups with launch configuration",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered an error updating auto scaling groups: {str(e)}"
        )


def get_auto_scaling_groups():
    autoscaling_client = connect_to_auto_scaling(Config(retries={"mode": "standard"}))
    paginator = autoscaling_client.get_paginator("describe_auto_scaling_groups")
    page_iterator = paginator.paginate()

    auto_scaling_groups = []
    for page in page_iterator:
        auto_scaling_groups += page["AutoScalingGroups"]

    return auto_scaling_groups


def update_auto_scaling_groups(
    auto_scaling_groups,
    old_launch_configuration_name: str,
    new_launch_configuation_name: str,
):
    autoscaling_client = connect_to_auto_scaling(Config(retries={"mode": "standard"}))

    for auto_scaling_group in auto_scaling_groups:
        if (
            auto_scaling_group["LaunchConfigurationName"]
            != old_launch_configuration_name
        ):
            continue

        autoscaling_client.update_auto_scaling_group(
            AutoScalingGroupName=auto_scaling_group["AutoScalingGroupName"],
            LaunchConfigurationName=new_launch_configuation_name,
        )",
              },
              "isEnd": false,
              "name": "UpdateAutoScalingGroupsWithLaunchConfiguration",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "DeleteLaunchConfiguration",
                "LaunchConfigurationName": "{{LaunchConfigurationName}}",
                "Service": "autoscaling",
              },
              "isEnd": true,
              "name": "DeleteLaunchConfiguration",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "LaunchConfigurationName": {
              "allowedPattern": "^.{1,255}$",
              "description": "(Required) The name of the Launch Configuration.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-ConfigureAutoScalingLaunchConfigToRequireIMDSv2",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRConfigureDynamoDBAutoScaling": {
      "DependsOn": [
        "CreateWait10",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-ConfigureDynamoDBAutoScaling

## Overview
This document registers a DynamoDB table in provisioned mode with Application Auto Scaling and creates a new scaling policy based on the 
parameters provided by the DynamoDB.1 control in Security Hub.

## Pre-requisites
* Configure the parameters for the DynamoDB.1 control in Security Hub.
* DynamoDB auto scaling requires the presence of a service linked role (AWSServiceRoleForApplicationAutoScaling_DynamoDBTable) that performs auto scaling actions on your behalf.

## What does this document do?
Registers the DynamoDB table with Application Auto Scaling; creates a new scaling policy with targetReadUtilization/targetWriteUtilization.
Since only the minProvisionedReadCapacity & minProvisionedWriteCapacity are provided in the finding data, the "max" counterpart is set to (2 * minProvisionedXXXXCapacity).

## Input Parameters
* TableId: (Required) VPC to be remediated.
* MinProvisionedReadCapacity: (Required) Minimum number of provisioned read capacity units for DynamoDB auto scaling.
* TargetReadUtilization: (Required) Target utilization percentage for read capacity.
* MinProvisionedWriteCapacity: (Required) Minimum number of provisioned write capacity units for DynamoDB auto scaling.
* TargetWriteUtilization: (Required) Target utilization percentage for write capacity.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* PutWriteScalingPolicy.Output
* PutReadScalingPolicy.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "calculate_max_capacity",
                "InputPayload": {
                  "MinProvisionedReadCapacity": "{{ MinProvisionedReadCapacity }}",
                  "MinProvisionedWriteCapacity": "{{ MinProvisionedWriteCapacity }}",
                },
                "Runtime": "python3.11",
                "Script": "def calculate_max_capacity(event, _):
    return { "MaxReadCapacity" : int(event['MinProvisionedReadCapacity']) * 2, "MaxWriteCapacity":  int(event['MinProvisionedWriteCapacity']) * 2}
",
              },
              "isEnd": false,
              "name": "CalculateMaxCapacity",
              "outputs": [
                {
                  "Name": "MaxReadCapacity",
                  "Selector": "$.Payload.MaxReadCapacity",
                  "Type": "Integer",
                },
                {
                  "Name": "MaxWriteCapacity",
                  "Selector": "$.Payload.MaxWriteCapacity",
                  "Type": "Integer",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "convert_string_to_int",
                "InputPayload": {
                  "MinProvisionedReadCapacity": "{{ MinProvisionedReadCapacity }}",
                  "MinProvisionedWriteCapacity": "{{ MinProvisionedWriteCapacity }}",
                  "TargetReadUtilization": "{{ TargetReadUtilization }}",
                  "TargetWriteUtilization": "{{ TargetWriteUtilization }}",
                },
                "Runtime": "python3.11",
                "Script": "def convert_string_to_int(event, _):
    return { "MinProvisionedReadCapacity" : int(event['MinProvisionedReadCapacity']), "MinProvisionedWriteCapacity":  int(event['MinProvisionedWriteCapacity']), "TargetReadUtilization":  int(event['TargetReadUtilization']), "TargetWriteUtilization": int(event['TargetWriteUtilization'])}
",
              },
              "isEnd": false,
              "name": "StringParamsToInt",
              "outputs": [
                {
                  "Name": "MinProvisionedReadCapacity",
                  "Selector": "$.Payload.MinProvisionedReadCapacity",
                  "Type": "Integer",
                },
                {
                  "Name": "MinProvisionedWriteCapacity",
                  "Selector": "$.Payload.MinProvisionedReadCapacity",
                  "Type": "Integer",
                },
                {
                  "Name": "TargetReadUtilization",
                  "Selector": "$.Payload.TargetReadUtilization",
                  "Type": "Integer",
                },
                {
                  "Name": "TargetWriteUtilization",
                  "Selector": "$.Payload.TargetWriteUtilization",
                  "Type": "Integer",
                },
              ],
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "RegisterScalableTarget",
                "MaxCapacity": "{{ CalculateMaxCapacity.MaxWriteCapacity }}",
                "MinCapacity": "{{ StringParamsToInt.MinProvisionedWriteCapacity }}",
                "ResourceId": "table/{{ TableId }}",
                "ScalableDimension": "dynamodb:table:WriteCapacityUnits",
                "Service": "application-autoscaling",
                "ServiceNamespace": "dynamodb",
              },
              "isEnd": false,
              "name": "RegisterScalableTarget_WriteUnits",
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "RegisterScalableTarget",
                "MaxCapacity": "{{ CalculateMaxCapacity.MaxReadCapacity }}",
                "MinCapacity": "{{ StringParamsToInt.MinProvisionedReadCapacity }}",
                "ResourceId": "table/{{ TableId }}",
                "ScalableDimension": "dynamodb:table:ReadCapacityUnits",
                "Service": "application-autoscaling",
                "ServiceNamespace": "dynamodb",
              },
              "isEnd": false,
              "name": "RegisterScalableTarget_ReadUnits",
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "PutScalingPolicy",
                "PolicyName": "SO0111-ASR-WriteCapacity-Scaling-Policy",
                "PolicyType": "TargetTrackingScaling",
                "ResourceId": "table/{{ TableId }}",
                "ScalableDimension": "dynamodb:table:WriteCapacityUnits",
                "Service": "application-autoscaling",
                "ServiceNamespace": "dynamodb",
                "TargetTrackingScalingPolicyConfiguration": {
                  "PredefinedMetricSpecification": {
                    "PredefinedMetricType": "DynamoDBWriteCapacityUtilization",
                  },
                  "TargetValue": "{{ StringParamsToInt.TargetWriteUtilization }}",
                },
              },
              "isEnd": false,
              "maxAttempts": 5,
              "name": "PutWriteScalingPolicy",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "PutScalingPolicy",
                "PolicyName": "SO0111-ASR-ReadCapacity-Scaling-Policy",
                "PolicyType": "TargetTrackingScaling",
                "ResourceId": "table/{{ TableId }}",
                "ScalableDimension": "dynamodb:table:ReadCapacityUnits",
                "Service": "application-autoscaling",
                "ServiceNamespace": "dynamodb",
                "TargetTrackingScalingPolicyConfiguration": {
                  "PredefinedMetricSpecification": {
                    "PredefinedMetricType": "DynamoDBReadCapacityUtilization",
                  },
                  "TargetValue": "{{ StringParamsToInt.TargetReadUtilization }}",
                },
              },
              "isEnd": true,
              "maxAttempts": 5,
              "name": "PutReadScalingPolicy",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "MinProvisionedReadCapacity": {
              "allowedPattern": "^\\d+$",
              "description": "(Required) Minimum number of provisioned read capacity units for DynamoDB auto scaling.",
              "type": "String",
            },
            "MinProvisionedWriteCapacity": {
              "allowedPattern": "^\\d+$",
              "description": "(Required) Minimum number of provisioned write capacity units for DynamoDB auto scaling.",
              "type": "String",
            },
            "TableId": {
              "allowedPattern": "^[a-zA-Z0-9._-]{3,255}$",
              "description": "(Required) DynamoDB Table to be remediated.",
              "type": "String",
            },
            "TargetReadUtilization": {
              "allowedPattern": "^\\d+$",
              "description": "(Required) Target utilization percentage for read capacity.",
              "type": "String",
            },
            "TargetWriteUtilization": {
              "allowedPattern": "^\\d+$",
              "description": "(Required) Target utilization percentage for write capacity.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-ConfigureDynamoDBAutoScaling",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRConfigureS3BucketPublicAccessBlock": {
      "DependsOn": [
        "CreateWait4",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - AWSConfigRemediation-ConfigureS3BucketPublicAccessBlock

## What does this document do?
This document is used to create or modify the PublicAccessBlock configuration for an Amazon S3 bucket.

## Input Parameters
* BucketName: (Required) Name of the S3 bucket (not the ARN).
* RestrictPublicBuckets: (Optional) Specifies whether Amazon S3 should restrict public bucket policies for this bucket. Setting this element to TRUE restricts access to this bucket to only AWS services and authorized users within this account if the bucket has a public policy.
  * Default: "true"
* BlockPublicAcls: (Optional) Specifies whether Amazon S3 should block public access control lists (ACLs) for this bucket and objects in this bucket.
  * Default: "true"
* IgnorePublicAcls: (Optional) Specifies whether Amazon S3 should ignore public ACLs for this bucket and objects in this bucket. Setting this element to TRUE causes Amazon S3 to ignore all public ACLs on this bucket and objects in this bucket.
  * Default: "true"
* BlockPublicPolicy: (Optional) Specifies whether Amazon S3 should block public bucket policies for this bucket. Setting this element to TRUE causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access.
  * Default: "true"
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* ConfigureS3PublicAccessBlock.Output - JSON formatted response from the ConfigureS3PublicAccessBlock script.

## Note: this is a local copy of the AWS-owned document to enable support in aws-cn and aws-us-gov partitions.
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## ConfigureS3PublicAccessBlock
Configures the S3 bucket PublicAccessBlock.
## Outputs
* Output: Response from the ConfigureS3PublicAccessBlock script.
",
              "inputs": {
                "Handler": "handle_s3_bucket",
                "InputPayload": {
                  "BlockPublicAcls": "{{ BlockPublicAcls }}",
                  "BlockPublicPolicy": "{{ BlockPublicPolicy }}",
                  "Bucket": "{{BucketName}}",
                  "IgnorePublicAcls": "{{ IgnorePublicAcls }}",
                  "RestrictPublicBuckets": "{{ RestrictPublicBuckets }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from time import sleep
from typing import Optional, TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


class PublicAccessConfiguration(TypedDict):
    BlockPublicAcls: bool
    IgnorePublicAcls: bool
    BlockPublicPolicy: bool
    RestrictPublicBuckets: bool


class ValidateBucketPublicAccessBlockResponse(TypedDict):
    Message: str
    Valid: bool
    PublicAccessConfig: Optional[PublicAccessConfiguration]


class BucketEvent(TypedDict):
    Bucket: str
    RestrictPublicBuckets: bool
    BlockPublicAcls: bool
    IgnorePublicAcls: bool
    BlockPublicPolicy: bool


class AccountEvent(TypedDict):
    AccountId: str
    RestrictPublicBuckets: bool
    BlockPublicAcls: bool
    IgnorePublicAcls: bool
    BlockPublicPolicy: bool


class HandlerResponse(TypedDict):
    Message: str
    Status: str
    PublicAccessConfig: Optional[PublicAccessConfiguration]


def connect_to_service(service):
    return boto3.client(service, config=boto_config)


def handle_account(event: AccountEvent, _) -> HandlerResponse:
    """
    Configures the S3 account-level public access block.
    """
    try:
        account_id = event["AccountId"]
        public_access_block_config: PublicAccessConfiguration = {
            "BlockPublicAcls": bool(event["BlockPublicAcls"]),
            "IgnorePublicAcls": bool(event["IgnorePublicAcls"]),
            "BlockPublicPolicy": bool(event["BlockPublicPolicy"]),
            "RestrictPublicBuckets": bool(event["RestrictPublicBuckets"]),
        }
        put_account_public_access_block(account_id, public_access_block_config)

        valid_account_public_access_block = validate_account_public_access_block(
            account_id, public_access_block_config
        )

        if valid_account_public_access_block["Valid"]:
            return {
                "Message": f"Account {account_id} public access block configuration successfully set.",
                "Status": "Success",
                "PublicAccessConfig": valid_account_public_access_block[
                    "PublicAccessConfig"
                ],
            }
        else:
            return {
                "Message": f"Account {account_id} public access block configuration does not match with parameters "
                f"provided. \\nExpected: {str(public_access_block_config)}",
                "Status": "Failed",
                "PublicAccessConfig": None,
            }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error configuring public access block for account: {str(e)}"
        )


def handle_s3_bucket(event: BucketEvent, _) -> HandlerResponse:
    """
    Configures the public access block for an S3 bucket.
    """
    try:
        bucket = event["Bucket"]
        public_access_block_config: PublicAccessConfiguration = {
            "BlockPublicAcls": bool(event["BlockPublicAcls"]),
            "IgnorePublicAcls": bool(event["IgnorePublicAcls"]),
            "BlockPublicPolicy": bool(event["BlockPublicPolicy"]),
            "RestrictPublicBuckets": bool(event["RestrictPublicBuckets"]),
        }
        put_s3_bucket_public_access_block(bucket, public_access_block_config)

        valid_bucket_public_access_block = validate_bucket_public_access_block(
            bucket, public_access_block_config
        )

        if valid_bucket_public_access_block["Valid"]:
            return {
                "Message": f"Bucket {bucket} public access block configuration successfully set.",
                "Status": "Success",
                "PublicAccessConfig": valid_bucket_public_access_block[
                    "PublicAccessConfig"
                ],
            }
        else:
            actual_config = valid_bucket_public_access_block["PublicAccessConfig"]
            return {
                "Message": f"Bucket {bucket} public access block configuration does not match with parameters provided."
                f"\\nExpected: {str(public_access_block_config)}\\nActual: {str(actual_config)}",
                "Status": "Failed",
                "PublicAccessConfig": actual_config,
            }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error configuring public access block for S3 Bucket: {str(e)}"
        )


def put_account_public_access_block(
    account_id: str,
    public_access_block_config: PublicAccessConfiguration,
) -> None:
    s3_client = connect_to_service("s3control")
    try:
        s3_client.put_public_access_block(
            AccountId=account_id,
            PublicAccessBlockConfiguration=public_access_block_config,
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered error putting public access block on account {account_id}: {str(e)}"
        )


def put_s3_bucket_public_access_block(
    bucket_name: str,
    public_access_block_config: PublicAccessConfiguration,
) -> None:
    s3_client = connect_to_service("s3")
    try:
        s3_client.put_public_access_block(
            Bucket=bucket_name,
            PublicAccessBlockConfiguration=public_access_block_config,
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered error putting public access block on bucket {bucket_name}: {str(e)}"
        )


def validate_account_public_access_block(
    account_id,
    expected_public_access_block_config,
) -> ValidateBucketPublicAccessBlockResponse:
    s3control_client = boto3.client("s3control")
    wait_time = 30
    max_time = 480
    max_retries = max_time // wait_time
    try:
        for _ in range(max_retries):
            sleep(wait_time)

            configuration = s3control_client.get_public_access_block(
                AccountId=account_id
            )["PublicAccessBlockConfiguration"]

            config_matches_expected = all(
                configuration.get(config_name)
                == expected_public_access_block_config.get(config_name)
                for config_name in expected_public_access_block_config
            )
            if config_matches_expected:
                return {
                    "Message": "Account public access block configuration successfully set.",
                    "Valid": True,
                    "PublicAccessConfig": configuration,
                }
        return {
            "Message": "Account public access block configuration does not match expected configuration.",
            "Valid": False,
            "PublicAccessConfig": None,
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error validating account-level public access block for {account_id}: {str(e)}"
        )


def validate_bucket_public_access_block(
    bucket_name: str,
    expected_public_access_block_config,
) -> ValidateBucketPublicAccessBlockResponse:
    s3_client = connect_to_service("s3")
    try:
        configuration: PublicAccessConfiguration = s3_client.get_public_access_block(
            Bucket=bucket_name
        )["PublicAccessBlockConfiguration"]

        for configuration_name, actual_configuration in configuration.items():
            if (
                actual_configuration
                != expected_public_access_block_config[configuration_name]
            ):
                return {
                    "Message": "Bucket public access block configuration does not match expected configuration.",
                    "Valid": False,
                    "PublicAccessConfig": configuration,
                }

        return {
            "Message": "Bucket public access block configuration successfully set.",
            "Valid": True,
            "PublicAccessConfig": configuration,
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error validating s3 bucket {bucket_name} public access block: {str(e)}"
        )",
              },
              "isCritical": true,
              "isEnd": true,
              "name": "ConfigureS3BucketPublicAccessBlock",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "ConfigureS3BucketPublicAccessBlock.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BlockPublicAcls": {
              "allowedValues": [
                true,
                false,
              ],
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should block public access control lists (ACLs) for this bucket and objects in this bucket.",
              "type": "Boolean",
            },
            "BlockPublicPolicy": {
              "allowedValues": [
                true,
                false,
              ],
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should block public bucket policies for this bucket. Setting this element to TRUE causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access.",
              "type": "Boolean",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "(Required) The bucket name (not the ARN).",
              "type": "String",
            },
            "IgnorePublicAcls": {
              "allowedValues": [
                true,
                false,
              ],
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should ignore public ACLs for this bucket and objects in this bucket. Setting this element to TRUE causes Amazon S3 to ignore all public ACLs on this bucket and objects in this bucket.",
              "type": "Boolean",
            },
            "RestrictPublicBuckets": {
              "allowedValues": [
                true,
                false,
              ],
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should restrict public bucket policies for this bucket. Setting this element to TRUE restricts access to this bucket to only AWS services and authorized users within this account if the bucket has a public policy.",
              "type": "Boolean",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-ConfigureS3BucketPublicAccessBlock",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRConfigureS3PublicAccessBlock": {
      "DependsOn": [
        "CreateWait5",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - AWSConfigRemediation-ConfigureS3PublicAccessBlock

## What does this document do?
This document is used to create or modify the S3 [PublicAccessBlock](https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html#access-control-block-public-access-options) configuration for an AWS account.

## Input Parameters
* AccountId: (Required) Account ID of the account for which the S3 Account Public Access Block is to be configured.
* RestrictPublicBuckets: (Optional) Specifies whether Amazon S3 should restrict public bucket policies for buckets in this account. Setting this element to TRUE restricts access to buckets with public policies to only AWS services and authorized users within this account.
  * Default: "true"
* BlockPublicAcls: (Optional) Specifies whether Amazon S3 should block public access control lists (ACLs) for buckets in this account.
  * Default: "true"
* IgnorePublicAcls: (Optional) Specifies whether Amazon S3 should ignore public ACLs for buckets in this account. Setting this element to TRUE causes Amazon S3 to ignore all public ACLs on buckets in this account and any objects that they contain.
  * Default: "true"
* BlockPublicPolicy: (Optional) Specifies whether Amazon S3 should block public bucket policies for buckets in this account. Setting this element to TRUE causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access.
  * Default: "true"
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* ConfigureS3PublicAccessBlock.Output - JSON formatted response from the ConfigureS3PublicAccessBlock script.
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## ConfigureS3PublicAccessBlock
Configures the S3 account-level PublicAccessBlock.
## Outputs
* Output: Response from the ConfigureS3PublicAccessBlock script.
",
              "inputs": {
                "Handler": "handle_account",
                "InputPayload": {
                  "AccountId": "{{ AccountId }}",
                  "BlockPublicAcls": "{{ BlockPublicAcls }}",
                  "BlockPublicPolicy": "{{ BlockPublicPolicy }}",
                  "IgnorePublicAcls": "{{ IgnorePublicAcls }}",
                  "RestrictPublicBuckets": "{{ RestrictPublicBuckets }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from time import sleep
from typing import Optional, TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


class PublicAccessConfiguration(TypedDict):
    BlockPublicAcls: bool
    IgnorePublicAcls: bool
    BlockPublicPolicy: bool
    RestrictPublicBuckets: bool


class ValidateBucketPublicAccessBlockResponse(TypedDict):
    Message: str
    Valid: bool
    PublicAccessConfig: Optional[PublicAccessConfiguration]


class BucketEvent(TypedDict):
    Bucket: str
    RestrictPublicBuckets: bool
    BlockPublicAcls: bool
    IgnorePublicAcls: bool
    BlockPublicPolicy: bool


class AccountEvent(TypedDict):
    AccountId: str
    RestrictPublicBuckets: bool
    BlockPublicAcls: bool
    IgnorePublicAcls: bool
    BlockPublicPolicy: bool


class HandlerResponse(TypedDict):
    Message: str
    Status: str
    PublicAccessConfig: Optional[PublicAccessConfiguration]


def connect_to_service(service):
    return boto3.client(service, config=boto_config)


def handle_account(event: AccountEvent, _) -> HandlerResponse:
    """
    Configures the S3 account-level public access block.
    """
    try:
        account_id = event["AccountId"]
        public_access_block_config: PublicAccessConfiguration = {
            "BlockPublicAcls": bool(event["BlockPublicAcls"]),
            "IgnorePublicAcls": bool(event["IgnorePublicAcls"]),
            "BlockPublicPolicy": bool(event["BlockPublicPolicy"]),
            "RestrictPublicBuckets": bool(event["RestrictPublicBuckets"]),
        }
        put_account_public_access_block(account_id, public_access_block_config)

        valid_account_public_access_block = validate_account_public_access_block(
            account_id, public_access_block_config
        )

        if valid_account_public_access_block["Valid"]:
            return {
                "Message": f"Account {account_id} public access block configuration successfully set.",
                "Status": "Success",
                "PublicAccessConfig": valid_account_public_access_block[
                    "PublicAccessConfig"
                ],
            }
        else:
            return {
                "Message": f"Account {account_id} public access block configuration does not match with parameters "
                f"provided. \\nExpected: {str(public_access_block_config)}",
                "Status": "Failed",
                "PublicAccessConfig": None,
            }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error configuring public access block for account: {str(e)}"
        )


def handle_s3_bucket(event: BucketEvent, _) -> HandlerResponse:
    """
    Configures the public access block for an S3 bucket.
    """
    try:
        bucket = event["Bucket"]
        public_access_block_config: PublicAccessConfiguration = {
            "BlockPublicAcls": bool(event["BlockPublicAcls"]),
            "IgnorePublicAcls": bool(event["IgnorePublicAcls"]),
            "BlockPublicPolicy": bool(event["BlockPublicPolicy"]),
            "RestrictPublicBuckets": bool(event["RestrictPublicBuckets"]),
        }
        put_s3_bucket_public_access_block(bucket, public_access_block_config)

        valid_bucket_public_access_block = validate_bucket_public_access_block(
            bucket, public_access_block_config
        )

        if valid_bucket_public_access_block["Valid"]:
            return {
                "Message": f"Bucket {bucket} public access block configuration successfully set.",
                "Status": "Success",
                "PublicAccessConfig": valid_bucket_public_access_block[
                    "PublicAccessConfig"
                ],
            }
        else:
            actual_config = valid_bucket_public_access_block["PublicAccessConfig"]
            return {
                "Message": f"Bucket {bucket} public access block configuration does not match with parameters provided."
                f"\\nExpected: {str(public_access_block_config)}\\nActual: {str(actual_config)}",
                "Status": "Failed",
                "PublicAccessConfig": actual_config,
            }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error configuring public access block for S3 Bucket: {str(e)}"
        )


def put_account_public_access_block(
    account_id: str,
    public_access_block_config: PublicAccessConfiguration,
) -> None:
    s3_client = connect_to_service("s3control")
    try:
        s3_client.put_public_access_block(
            AccountId=account_id,
            PublicAccessBlockConfiguration=public_access_block_config,
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered error putting public access block on account {account_id}: {str(e)}"
        )


def put_s3_bucket_public_access_block(
    bucket_name: str,
    public_access_block_config: PublicAccessConfiguration,
) -> None:
    s3_client = connect_to_service("s3")
    try:
        s3_client.put_public_access_block(
            Bucket=bucket_name,
            PublicAccessBlockConfiguration=public_access_block_config,
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered error putting public access block on bucket {bucket_name}: {str(e)}"
        )


def validate_account_public_access_block(
    account_id,
    expected_public_access_block_config,
) -> ValidateBucketPublicAccessBlockResponse:
    s3control_client = boto3.client("s3control")
    wait_time = 30
    max_time = 480
    max_retries = max_time // wait_time
    try:
        for _ in range(max_retries):
            sleep(wait_time)

            configuration = s3control_client.get_public_access_block(
                AccountId=account_id
            )["PublicAccessBlockConfiguration"]

            config_matches_expected = all(
                configuration.get(config_name)
                == expected_public_access_block_config.get(config_name)
                for config_name in expected_public_access_block_config
            )
            if config_matches_expected:
                return {
                    "Message": "Account public access block configuration successfully set.",
                    "Valid": True,
                    "PublicAccessConfig": configuration,
                }
        return {
            "Message": "Account public access block configuration does not match expected configuration.",
            "Valid": False,
            "PublicAccessConfig": None,
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error validating account-level public access block for {account_id}: {str(e)}"
        )


def validate_bucket_public_access_block(
    bucket_name: str,
    expected_public_access_block_config,
) -> ValidateBucketPublicAccessBlockResponse:
    s3_client = connect_to_service("s3")
    try:
        configuration: PublicAccessConfiguration = s3_client.get_public_access_block(
            Bucket=bucket_name
        )["PublicAccessBlockConfiguration"]

        for configuration_name, actual_configuration in configuration.items():
            if (
                actual_configuration
                != expected_public_access_block_config[configuration_name]
            ):
                return {
                    "Message": "Bucket public access block configuration does not match expected configuration.",
                    "Valid": False,
                    "PublicAccessConfig": configuration,
                }

        return {
            "Message": "Bucket public access block configuration successfully set.",
            "Valid": True,
            "PublicAccessConfig": configuration,
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error validating s3 bucket {bucket_name} public access block: {str(e)}"
        )",
              },
              "isCritical": true,
              "isEnd": true,
              "name": "ConfigureS3PublicAccessBlock",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "ConfigureS3PublicAccessBlock.Output",
          ],
          "parameters": {
            "AccountId": {
              "allowedPattern": "^\\d{12}$",
              "description": "(Required) The account ID for the AWS account whose PublicAccessBlock configuration you want to set.",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BlockPublicAcls": {
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should block public access control lists (ACLs) for buckets in this account.",
              "type": "Boolean",
            },
            "BlockPublicPolicy": {
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should block public bucket policies for buckets in this account. Setting this element to TRUE causes Amazon S3 to reject calls to PUT Bucket policy if the specified bucket policy allows public access.",
              "type": "Boolean",
            },
            "IgnorePublicAcls": {
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should ignore public ACLs for buckets in this account. Setting this element to TRUE causes Amazon S3 to ignore all public ACLs on buckets in this account and any objects that they contain.",
              "type": "Boolean",
            },
            "RestrictPublicBuckets": {
              "default": true,
              "description": "(Optional) Specifies whether Amazon S3 should restrict public bucket policies for buckets in this account. Setting this element to TRUE restricts access to buckets with public policies to only AWS services and authorized users within this account.",
              "type": "Boolean",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-ConfigureS3PublicAccessBlock",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRConfigureSNSTopicForStack": {
      "DependsOn": [
        "CreateWait4",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-ConfigureSNSTopicForStack

## What does this document do?
This document creates an SNS topic if it does not already exist, then updates the stack to notify the topic on changes

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* StackArn: (Required)  The ARN of the stack.

## Security Standards / Controls
* AWS FSBP v1.0.0:   CloudFormation.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "stack_arn": "{{ StackArn }}",
                  "topic_name": "SO0111-ASR-CloudFormationNotifications",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
"""
Configure a CloudFormation stack with an SNS topic for notifications, creating the topic if it does
not already exist
"""
from time import sleep, time
from typing import TYPE_CHECKING

import boto3
from botocore.config import Config

if TYPE_CHECKING:
    from mypy_boto3_sns.client import SNSClient
else:
    SNSClient = object

boto_config = Config(retries={"mode": "standard"})


def lambda_handler(event, _):
    """
    Configure a CloudFormation stack with an SNS topic for notifications,
    creating the topic if it does not already exist

    \`event\` should have the following keys and values:
    \`stack_arn\`: the ARN of the CloudFormation stack to be updated
    \`topic_name\`: the name of the SQS Queue to create and configure for notifications

    \`context\` is ignored
    """
    stack_arn = event["stack_arn"]
    topic_name = event["topic_name"]
    topic_arn = get_or_create_topic(topic_name)
    configure_notifications(stack_arn, topic_arn)
    wait_for_update(stack_arn)
    return assert_stack_configured(stack_arn, topic_arn)


def get_or_create_topic(topic_name: str) -> str:
    """Get the SQS topic arn for the given topic name, creating it if it does not already exist"""
    sns: SNSClient = boto3.client("sns", config=boto_config)
    response = sns.create_topic(Name=topic_name)
    return response["TopicArn"]


def configure_notifications(stack_arn: str, topic_arn: str) -> None:
    """Configure the stack with ARN \`stack_arn\` to notify the queue with ARN \`topic_arn\`"""
    cloudformation = boto3.resource("cloudformation", config=boto_config)
    stack = cloudformation.Stack(stack_arn)
    kwargs = {"UsePreviousTemplate": True, "NotificationARNs": [topic_arn]}
    if stack.parameters:
        kwargs["Parameters"] = [
            {"ParameterKey": param["ParameterKey"], "UsePreviousValue": True}
            for param in stack.parameters
        ]
    if stack.capabilities:
        kwargs["Capabilities"] = stack.capabilities
    stack.update(**kwargs)


class UpdateTimeoutException(Exception):
    """Timed out waiting for the CloudFormation stack to update"""


def wait_for_update(stack_arn: str) -> None:
    """Wait for the stack with ARN \`stack_arn\` to be in status \`UPDATE_COMPLETE\`"""
    wait_interval_seconds = 10
    timeout_seconds = 300
    start = time()
    while get_stack_status(stack_arn) != "UPDATE_COMPLETE":
        if time() - start > timeout_seconds:
            raise UpdateTimeoutException("Timed out waiting for stack update")
        wait_seconds(wait_interval_seconds)
        wait_interval_seconds = wait_interval_seconds * 2


def get_stack_status(stack_arn):
    """Get the status of the CloudFormation stack with ARN \`stack_arn\`"""
    cloudformation = boto3.client("cloudformation", config=boto_config)
    response = cloudformation.describe_stacks(StackName=stack_arn)
    return response["Stacks"][0]["StackStatus"]


def wait_seconds(seconds):
    """Wait for \`seconds\` seconds"""
    sleep(seconds)


def assert_stack_configured(stack_arn, topic_arn):
    """
    Verify that the CloudFormation stack with ARN \`stack_arn\` is configured to update the SQS topic
    with ARN \`topic_arn\`
    """
    cloudformation = boto3.resource("cloudformation", config=boto_config)
    stack = cloudformation.Stack(stack_arn)
    wait_interval_seconds = 10
    timeout_seconds = 300
    start = time()
    while stack.notification_arns != [topic_arn]:
        if time() - start > timeout_seconds:
            raise StackConfigurationFailedException(
                "Timed out waiting for stack configuration to take effect"
            )
        wait_seconds(wait_interval_seconds)
        wait_interval_seconds = wait_interval_seconds * 2
        stack.reload()
    return {"NotificationARNs": stack.notification_arns}


class StackConfigurationFailedException(Exception):
    """An error occurred updating the CloudFormation stack to notify the SQS topic"""",
              },
              "isEnd": true,
              "name": "ConfigureSNSTopic",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "ConfigureSNSTopic.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "StackArn": {
              "allowedPattern": "^(arn:(?:aws|aws-us-gov|aws-cn):cloudformation:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:stack/[a-zA-Z][a-zA-Z0-9-]{0,127}/[a-fA-F0-9]{8}-(?:[a-fA-F0-9]{4}-){3}[a-fA-F0-9]{12})$",
              "description": "(Required) The ARN of the CloudFormation stack.",
              "type": "String",
            },
            "TopicName": {
              "allowedPattern": "^[a-zA-Z0-9][a-zA-Z0-9-_]{0,255}$",
              "default": "SO0111-ASR-CloudFormationNotifications",
              "description": "(Optional) The name of the SNS topic to create and configure for notifications.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-ConfigureSNSTopicForStack",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRCreateAccessLoggingBucket": {
      "DependsOn": [
        "CreateWait1",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-CreateAccessLoggingBucket

## What does this document do?
Creates an S3 bucket for access logging.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* BucketName: (Required) Name of the bucket to create
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_logging_bucket",
                "InputPayload": {
                  "AWS_REGION": "{{global:REGION}}",
                  "BucketName": "{{BucketName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json
from typing import TYPE_CHECKING, TypedDict, cast

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

if TYPE_CHECKING:
    from aws_lambda_powertools.utilities.typing import LambdaContext
    from mypy_boto3_s3.client import S3Client
    from mypy_boto3_s3.literals import BucketLocationConstraintType
    from mypy_boto3_s3.type_defs import CreateBucketRequestRequestTypeDef
else:
    S3Client = object
    LambdaContext = object
    BucketLocationConstraintType = object
    CreateBucketRequestRequestTypeDef = object


def connect_to_s3(boto_config: Config) -> S3Client:
    s3: S3Client = boto3.client("s3", config=boto_config)
    return s3


class Event(TypedDict):
    BucketName: str
    AWS_REGION: str


class Output(TypedDict):
    Message: str


class Response(TypedDict):
    output: Output


def create_logging_bucket(event: Event, _: LambdaContext) -> Response:
    boto_config = Config(retries={"mode": "standard"})
    s3 = connect_to_s3(boto_config)

    try:
        kwargs: CreateBucketRequestRequestTypeDef = {
            "Bucket": event["BucketName"],
            "GrantWrite": "uri=http://acs.amazonaws.com/groups/s3/LogDelivery",
            "GrantReadACP": "uri=http://acs.amazonaws.com/groups/s3/LogDelivery",
            "ObjectOwnership": "ObjectWriter",
        }
        if event["AWS_REGION"] != "us-east-1":
            kwargs["CreateBucketConfiguration"] = {
                "LocationConstraint": cast(
                    BucketLocationConstraintType, event["AWS_REGION"]
                )
            }

        s3.create_bucket(**kwargs)

        s3.put_bucket_encryption(
            Bucket=event["BucketName"],
            ServerSideEncryptionConfiguration={
                "Rules": [
                    {"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}
                ]
            },
        )

        # Add SSL/TLS enforcement policy
        partition = "aws"
        if "cn-" in event["AWS_REGION"]:
            partition = "aws-cn"
        elif "us-gov" in event["AWS_REGION"]:
            partition = "aws-us-gov"
        ssl_policy = {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Sid": "AllowSSLRequestsOnly",
                    "Action": "s3:*",
                    "Effect": "Deny",
                    "Resource": [
                        f"arn:{partition}:s3:::{event['BucketName']}",
                        f"arn:{partition}:s3:::{event['BucketName']}/*",
                    ],
                    "Condition": {"Bool": {"aws:SecureTransport": "false"}},
                    "Principal": "*",
                }
            ],
        }

        # Apply the SSL policy to the bucket
        s3.put_bucket_policy(Bucket=event["BucketName"], Policy=json.dumps(ssl_policy))

        return {"output": {"Message": f'Bucket {event["BucketName"]} created'}}
    except ClientError as error:
        if error.response["Error"]["Code"] != "BucketAlreadyOwnedByYou":
            exit(str(error))
        else:
            return {
                "output": {
                    "Message": f'Bucket {event["BucketName"]} already exists and is owned by you'
                }
            }
    except Exception as e:
        print(e)
        exit(str(e))",
              },
              "isEnd": true,
              "name": "CreateAccessLoggingBucket",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "CreateAccessLoggingBucket.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "(Required) The bucket name (not the ARN).",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-CreateAccessLoggingBucket",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRCreateCloudTrailMultiRegionTrail": {
      "DependsOn": [
        "CreateWait0",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-CreateCloudTrailMultiRegionTrail
## What does this document do?
Creates a multi-region trail with KMS encryption and enables CloudTrail
Note: this remediation will create a NEW trail.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* KMSKeyArn (from SSM): Arn of the KMS key to be used to encrypt data

## Security Standards / Controls
* AWS FSBP v1.0.0:   CloudTrail.1
* CIS v1.2.0:     2.1
* PCI:            CloudTrail.2
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_logging_bucket",
                "InputPayload": {
                  "account": "{{global:ACCOUNT_ID}}",
                  "kms_key_arn": "{{KMSKeyArn}}",
                  "region": "{{global:REGION}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TYPE_CHECKING, Dict, Literal, TypedDict, cast

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

if TYPE_CHECKING:
    from aws_lambda_powertools.utilities.typing import LambdaContext
    from mypy_boto3_s3.client import S3Client
    from mypy_boto3_s3.literals import BucketLocationConstraintType
    from mypy_boto3_s3.type_defs import CreateBucketRequestRequestTypeDef
else:
    S3Client = object
    LambdaContext = object
    BucketLocationConstraintType = object
    CreateBucketRequestRequestTypeDef = object


def connect_to_s3() -> S3Client:
    s3: S3Client = boto3.client("s3", config=Config(retries={"mode": "standard"}))
    return s3


class Event(TypedDict):
    account: str
    region: str
    kms_key_arn: str


def create_logging_bucket(
    event: Event, _: LambdaContext
) -> Dict[Literal["logging_bucket"], str]:
    s3 = connect_to_s3()

    kms_key_arn: str = event["kms_key_arn"]
    aws_account: str = event["account"]
    aws_region: str = event["region"]
    bucket_name = "so0111-access-logs-" + aws_region + "-" + aws_account

    if create_bucket(s3, bucket_name, aws_region) == "bucket_exists":
        return {"logging_bucket": bucket_name}
    encrypt_bucket(s3, bucket_name, kms_key_arn)
    put_access_block(s3, bucket_name)
    put_bucket_acl(s3, bucket_name)

    return {"logging_bucket": bucket_name}


def create_bucket(s3: S3Client, bucket_name: str, aws_region: str) -> str:
    try:
        kwargs: CreateBucketRequestRequestTypeDef = {
            "Bucket": bucket_name,
            "ACL": "private",
            "ObjectOwnership": "ObjectWriter",
        }
        if aws_region != "us-east-1":
            kwargs["CreateBucketConfiguration"] = {
                "LocationConstraint": cast(BucketLocationConstraintType, aws_region)
            }

        s3.create_bucket(**kwargs)
        return "success"
    except ClientError as ex:
        exception_type = ex.response["Error"]["Code"]
        # bucket already exists - return
        if exception_type == "BucketAlreadyOwnedByYou":
            print("Bucket " + bucket_name + " already exists and is owned by you")
            return "bucket_exists"
        else:
            print(ex)
            exit("Error creating bucket " + bucket_name)
    except Exception as e:
        print(e)
        exit("Error creating bucket " + bucket_name)


def encrypt_bucket(s3: S3Client, bucket_name: str, kms_key_arn: str) -> None:
    try:
        s3.put_bucket_encryption(
            Bucket=bucket_name,
            ServerSideEncryptionConfiguration={
                "Rules": [
                    {
                        "ApplyServerSideEncryptionByDefault": {
                            "SSEAlgorithm": "aws:kms",
                            "KMSMasterKeyID": kms_key_arn.split("key/")[1],
                        }
                    }
                ]
            },
        )
    except Exception as e:
        exit("Error encrypting bucket " + bucket_name + ": " + str(e))


def put_access_block(s3: S3Client, bucket_name: str) -> None:
    try:
        s3.put_public_access_block(
            Bucket=bucket_name,
            PublicAccessBlockConfiguration={
                "BlockPublicAcls": True,
                "IgnorePublicAcls": True,
                "BlockPublicPolicy": True,
                "RestrictPublicBuckets": True,
            },
        )
    except Exception as e:
        exit(
            "Error setting public access block for bucket "
            + bucket_name
            + ": "
            + str(e)
        )


def put_bucket_acl(s3: S3Client, bucket_name: str) -> None:
    try:
        s3.put_bucket_acl(
            Bucket=bucket_name,
            GrantReadACP="uri=http://acs.amazonaws.com/groups/s3/LogDelivery",
            GrantWrite="uri=http://acs.amazonaws.com/groups/s3/LogDelivery",
        )
    except Exception as e:
        exit("Error setting ACL for bucket " + bucket_name + ": " + str(e))",
              },
              "isEnd": false,
              "name": "CreateLoggingBucket",
              "outputs": [
                {
                  "Name": "LoggingBucketName",
                  "Selector": "$.Payload.logging_bucket",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_encrypted_bucket",
                "InputPayload": {
                  "account": "{{global:ACCOUNT_ID}}",
                  "kms_key_arn": "{{KMSKeyArn}}",
                  "logging_bucket": "{{CreateLoggingBucket.LoggingBucketName}}",
                  "region": "{{global:REGION}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


def connect_to_s3(boto_config):
    return boto3.client("s3", config=boto_config)


def create_encrypted_bucket(event, _):
    boto_config = Config(retries={"mode": "standard"})
    s3 = connect_to_s3(boto_config)

    kms_key_arn = event["kms_key_arn"]
    aws_account = event["account"]
    aws_region = event["region"]
    logging_bucket = event["logging_bucket"]
    bucket_name = "so0111-aws-cloudtrail-" + aws_account

    if create_s3_bucket(s3, bucket_name, aws_region) == "bucket_exists":
        return {"cloudtrail_bucket": bucket_name}
    put_bucket_encryption(s3, bucket_name, kms_key_arn)
    put_public_access_block(s3, bucket_name)
    put_bucket_logging(s3, bucket_name, logging_bucket)

    return {"cloudtrail_bucket": bucket_name}


def create_s3_bucket(s3, bucket_name, aws_region):
    try:
        kwargs = {"Bucket": bucket_name, "ACL": "private"}
        if aws_region != "us-east-1":
            kwargs["CreateBucketConfiguration"] = {"LocationConstraint": aws_region}

        s3.create_bucket(**kwargs)

    except ClientError as client_ex:
        exception_type = client_ex.response["Error"]["Code"]
        if exception_type == "BucketAlreadyOwnedByYou":
            print("Bucket " + bucket_name + " already exists and is owned by you")
            return "bucket_exists"
        else:
            exit("Error creating bucket " + bucket_name + " " + str(client_ex))
    except Exception as e:
        exit("Error creating bucket " + bucket_name + " " + str(e))


def put_bucket_encryption(s3, bucket_name, kms_key_arn):
    try:
        s3.put_bucket_encryption(
            Bucket=bucket_name,
            ServerSideEncryptionConfiguration={
                "Rules": [
                    {
                        "ApplyServerSideEncryptionByDefault": {
                            "SSEAlgorithm": "aws:kms",
                            "KMSMasterKeyID": kms_key_arn.split("key/")[1],
                        }
                    }
                ]
            },
        )
    except Exception as e:
        print(e)
        exit(
            "Error applying encryption to bucket "
            + bucket_name
            + " with key "
            + kms_key_arn
        )


def put_public_access_block(s3, bucket_name):
    try:
        s3.put_public_access_block(
            Bucket=bucket_name,
            PublicAccessBlockConfiguration={
                "BlockPublicAcls": True,
                "IgnorePublicAcls": True,
                "BlockPublicPolicy": True,
                "RestrictPublicBuckets": True,
            },
        )
    except Exception as e:
        exit(f"Error setting public access block for bucket {bucket_name}: {str(e)}")


def put_bucket_logging(s3, bucket_name, logging_bucket):
    try:
        s3.put_bucket_logging(
            Bucket=bucket_name,
            BucketLoggingStatus={
                "LoggingEnabled": {
                    "TargetBucket": logging_bucket,
                    "TargetPrefix": "cloudtrail-access-logs",
                }
            },
        )
    except Exception as e:
        print(e)
        exit("Error setting public access block for bucket " + bucket_name)",
              },
              "isEnd": false,
              "name": "CreateCloudTrailBucket",
              "outputs": [
                {
                  "Name": "CloudTrailBucketName",
                  "Selector": "$.Payload.cloudtrail_bucket",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_bucket_policy",
                "InputPayload": {
                  "account": "{{global:ACCOUNT_ID}}",
                  "cloudtrail_bucket": "{{CreateCloudTrailBucket.CloudTrailBucketName}}",
                  "partition": "{{AWSPartition}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config


def connect_to_s3(boto_config):
    return boto3.client("s3", config=boto_config)


def create_bucket_policy(event, _):
    boto_config = Config(retries={"mode": "standard"})
    s3 = connect_to_s3(boto_config)

    cloudtrail_bucket = event["cloudtrail_bucket"]
    aws_partition = event["partition"]
    aws_account = event["account"]
    try:
        bucket_policy = {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Sid": "AWSCloudTrailAclCheck20150319",
                    "Effect": "Allow",
                    "Principal": {"Service": ["cloudtrail.amazonaws.com"]},
                    "Action": "s3:GetBucketAcl",
                    "Resource": "arn:" + aws_partition + ":s3:::" + cloudtrail_bucket,
                },
                {
                    "Sid": "AWSCloudTrailWrite20150319",
                    "Effect": "Allow",
                    "Principal": {"Service": ["cloudtrail.amazonaws.com"]},
                    "Action": "s3:PutObject",
                    "Resource": "arn:"
                    + aws_partition
                    + ":s3:::"
                    + cloudtrail_bucket
                    + "/AWSLogs/"
                    + aws_account
                    + "/*",
                    "Condition": {
                        "StringEquals": {"s3:x-amz-acl": "bucket-owner-full-control"},
                    },
                },
                {
                    "Sid": "AllowSSLRequestsOnly",
                    "Effect": "Deny",
                    "Principal": "*",
                    "Action": "s3:*",
                    "Resource": [
                        "arn:" + aws_partition + ":s3:::" + cloudtrail_bucket,
                        "arn:" + aws_partition + ":s3:::" + cloudtrail_bucket + "/*",
                    ],
                    "Condition": {"Bool": {"aws:SecureTransport": "false"}},
                },
            ],
        }
        s3.put_bucket_policy(Bucket=cloudtrail_bucket, Policy=json.dumps(bucket_policy))
        return {
            "output": {"Message": f"Set bucket policy for bucket {cloudtrail_bucket}"}
        }
    except Exception as e:
        print(e)
        exit("PutBucketPolicy failed: " + str(e))",
              },
              "isEnd": false,
              "name": "CreateCloudTrailBucketPolicy",
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "enable_cloudtrail",
                "InputPayload": {
                  "cloudtrail_bucket": "{{CreateCloudTrailBucket.CloudTrailBucketName}}",
                  "kms_key_arn": "{{KMSKeyArn}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


def connect_to_cloudtrail(boto_config):
    return boto3.client("cloudtrail", config=boto_config)


def enable_cloudtrail(event, _):
    boto_config = Config(retries={"mode": "standard"})
    ct = connect_to_cloudtrail(boto_config)

    trail_name = "multi-region-cloud-trail"

    try:
        existing_trails = ct.describe_trails(trailNameList=[trail_name])

        if existing_trails.get("trailList"):
            # Trail exists, update it
            print(f"Trail {trail_name} already exists, updating configuration")
            ct.update_trail(
                Name=trail_name,
                S3BucketName=event["cloudtrail_bucket"],
                IncludeGlobalServiceEvents=True,
                EnableLogFileValidation=True,
                IsMultiRegionTrail=True,
                KmsKeyId=event["kms_key_arn"],
            )
            message = f"CloudTrail Trail {trail_name} updated"
        else:
            # Trail doesn't exist, create it
            ct.create_trail(
                Name=trail_name,
                S3BucketName=event["cloudtrail_bucket"],
                IncludeGlobalServiceEvents=True,
                EnableLogFileValidation=True,
                IsMultiRegionTrail=True,
                KmsKeyId=event["kms_key_arn"],
            )
            message = f"CloudTrail Trail {trail_name} created"

        ct.start_logging(Name=trail_name)
        return {"output": {"Message": message}}

    except ClientError as e:
        error_code = e.response["Error"]["Code"]
        if error_code == "TrailAlreadyExistsException":
            print(f"Trail {trail_name} already exists, updating configuration")
            try:
                ct.update_trail(
                    Name=trail_name,
                    S3BucketName=event["cloudtrail_bucket"],
                    IncludeGlobalServiceEvents=True,
                    EnableLogFileValidation=True,
                    IsMultiRegionTrail=True,
                    KmsKeyId=event["kms_key_arn"],
                )
                ct.start_logging(Name=trail_name)
                return {"output": {"Message": f"CloudTrail Trail {trail_name} updated"}}
            except Exception as update_error:
                exit(f"Error updating CloudTrail trail: {str(update_error)}")
        else:
            exit(f"Error enabling CloudTrail: {str(e)}")
    except Exception as e:
        exit(f"Error enabling CloudTrail: {str(e)}")",
              },
              "isEnd": false,
              "name": "EnableCloudTrail",
              "outputs": [
                {
                  "Name": "CloudTrailBucketName",
                  "Selector": "$.Payload.cloudtrail_bucket",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "process_results",
                "InputPayload": {
                  "cloudtrail_bucket": "{{CreateCloudTrailBucket.CloudTrailBucketName}}",
                  "logging_bucket": "{{CreateLoggingBucket.LoggingBucketName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def process_results(event, _):
    print(f'Created encrypted CloudTrail bucket {event["cloudtrail_bucket"]}')
    print(
        f'Created access logging for CloudTrail bucket in bucket {event["logging_bucket"]}'
    )
    print("Enabled multi-region AWS CloudTrail")
    return {
        "response": {
            "message": "AWS CloudTrail successfully enabled",
            "status": "Success",
        }
    }",
              },
              "isEnd": true,
              "name": "Remediation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "Remediation.Output",
          ],
          "parameters": {
            "AWSPartition": {
              "allowedValues": [
                "aws",
                "aws-cn",
                "aws-us-gov",
              ],
              "default": "aws",
              "description": "Partition for creation of ARNs.",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "KMSKeyArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "default": "{{ssm:/Solutions/SO0111/CMK_REMEDIATION_ARN}}",
              "description": "The ARN of the KMS key created by ASR for this remediation",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-CreateCloudTrailMultiRegionTrail",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRCreateIAMSupportRole": {
      "DependsOn": [
        "CreateWait4",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-CreateIAMSupportRole

## What does this document do?
This document creates a role to allow AWS Support access.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* CreateIAMRole.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## CreateIAMSupportRole

This step deactivates IAM user access keys that have not been rotated in more than MaxCredentialUsageAge days
## Outputs
* Output: Success message or failure Exception.
",
              "inputs": {
                "Handler": "create_iam_role",
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json
from typing import Dict, Final, List, Literal, TypedDict

import boto3
from botocore.config import Config

BOTO_CONFIG = Config(retries={"mode": "standard"})


class Response(TypedDict):
    Account: str
    RoleName: Literal["aws_incident_support_role"]


responses: Dict[Literal["CreateIAMRoleResponse"], List[Response]] = {
    "CreateIAMRoleResponse": []
}


def connect_to_iam(boto_config):
    return boto3.client("iam", config=boto_config)


def get_account(boto_config):
    return boto3.client("sts", config=boto_config).get_caller_identity()["Account"]


def get_partition(boto_config):
    return (
        boto3.client("sts", config=boto_config)
        .get_caller_identity()["Arn"]
        .split(":")[1]
    )


def create_iam_role(_, __):
    account = get_account(BOTO_CONFIG)
    partition = get_partition(BOTO_CONFIG)

    aws_support_policy = {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": "sts:AssumeRole",
                "Principal": {"AWS": f"arn:{partition}:iam::{account}:root"},
            }
        ],
    }

    role_name: Final = "aws_incident_support_role"
    iam = connect_to_iam(BOTO_CONFIG)
    if not does_role_exist(iam, role_name):
        iam.create_role(
            RoleName=role_name,
            AssumeRolePolicyDocument=json.dumps(aws_support_policy),
            Description="Created by ASR security hub remediation 1.20 rule",
            Tags=[
                {"Key": "Name", "Value": "CIS 1.20 aws support access role"},
            ],
        )

    iam.attach_role_policy(
        RoleName=role_name,
        PolicyArn=f"arn:{partition}:iam::aws:policy/AWSSupportAccess",
    )

    responses["CreateIAMRoleResponse"].append(
        {"Account": account, "RoleName": role_name}
    )

    return {"output": "IAM role creation is successful.", "http_responses": responses}


def does_role_exist(iam_client, role_name) -> bool:
    role_exists = False

    try:
        response = iam_client.get_role(RoleName=role_name)

        if "Role" in response:
            role_exists = True

    except iam_client.exceptions.NoSuchEntityException:
        role_exists = False

    return role_exists",
              },
              "isEnd": true,
              "name": "CreateIAMSupportRole",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 300,
            },
          ],
          "outputs": [
            "CreateIAMSupportRole.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-CreateIAMSupportRole",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRCreateLogMetricFilterAndAlarm": {
      "DependsOn": [
        "CreateWait0",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-CreateLogMetricFilterAndAlarm
## What does this document do?
Creates a CloudWatch metric filter and alarm for log monitoring by:
1. Creating an encrypted SNS topic for alarm notifications
2. Ensuring the specified log group exists (creates if missing)
3. Creating a metric filter on the log group with the specified pattern
4. Creating a CloudWatch alarm that triggers when the metric threshold is exceeded

This runbook is commonly used for security monitoring (e.g., failed login attempts,
unauthorized API calls, etc.) as required by various compliance standards.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* LogGroupName: (Required) Name of the CloudWatch log group to monitor
* FilterName: (Required) Name for the metric filter
* FilterPattern: (Required) Log filter pattern to match events
* MetricName: (Required) Name of the CloudWatch metric
* MetricNamespace: (Required) Namespace for the metric
* MetricValue: (Required) Value to record when pattern matches
* AlarmName: (Required) Name for the CloudWatch alarm
* AlarmDesc: (Required) Description for the alarm
* AlarmThreshold: (Required) Threshold value that triggers the alarm
* SNSTopicName: (Required) Name for the SNS topic for notifications
* KMSKeyArn: (Required) KMS key ARN for encrypting the SNS topic

## Security Standards / Controls
* CIS v1.2.0:     3.1-3.14
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_encrypted_topic",
                "InputPayload": {
                  "kms_key_arn": "{{KMSKeyArn}}",
                  "topic_name": "{{SNSTopicName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

boto_config = Config(retries={"mode": "standard"})


def connect_to_sns():
    return boto3.client("sns", config=boto_config)


def connect_to_ssm():
    return boto3.client("ssm", config=boto_config)


def create_encrypted_topic(event, _):
    kms_key_arn = event["kms_key_arn"]
    new_topic = False
    topic_arn = ""
    topic_name = event["topic_name"]

    try:
        sns = connect_to_sns()
        topic_arn = sns.create_topic(
            Name=topic_name, Attributes={"KmsMasterKeyId": kms_key_arn.split("key/")[1]}
        )["TopicArn"]
        new_topic = True

    except ClientError as client_exception:
        exception_type = client_exception.response["Error"]["Code"]
        if exception_type == "InvalidParameter":
            print(
                f"Topic {topic_name} already exists. This remediation may have been run before."
            )
            print("Ignoring exception - remediation continues.")
            topic_arn = sns.create_topic(Name=topic_name)["TopicArn"]
        else:
            exit(f"ERROR: Unhandled client exception: {client_exception}")

    except Exception as e:
        exit(f"ERROR: could not create SNS Topic {topic_name}: {str(e)}")

    if new_topic:
        try:
            ssm = connect_to_ssm()
            ssm.put_parameter(
                Name="/Solutions/SO0111/SNS_Topic_CIS3.x",
                Description="SNS Topic for AWS Config updates",
                Type="String",
                Overwrite=True,
                Value=topic_arn,
            )
        except Exception as e:
            exit(f"ERROR: could not create SNS Topic {topic_name}: {str(e)}")

    create_topic_policy(topic_arn)

    return {"topic_arn": topic_arn}


def create_topic_policy(topic_arn):
    sns = connect_to_sns()
    try:
        topic_policy = {
            "Id": "Policy_ID",
            "Statement": [
                {
                    "Sid": "AWSConfigSNSPolicy",
                    "Effect": "Allow",
                    "Principal": {"Service": "cloudwatch.amazonaws.com"},
                    "Action": "SNS:Publish",
                    "Resource": topic_arn,
                }
            ],
        }

        sns.set_topic_attributes(
            TopicArn=topic_arn,
            AttributeName="Policy",
            AttributeValue=json.dumps(topic_policy),
        )
    except Exception as e:
        exit(f"ERROR: Failed to SetTopicAttributes for {topic_arn}: {str(e)}")",
              },
              "name": "CreateTopic",
              "outputs": [
                {
                  "Name": "TopicArn",
                  "Selector": "$.Payload.topic_arn",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "verify",
                "InputPayload": {
                  "AlarmDesc": "{{AlarmDesc}}",
                  "AlarmName": "{{AlarmName}}",
                  "AlarmThreshold": "{{AlarmThreshold}}",
                  "FilterName": "{{FilterName}}",
                  "FilterPattern": "{{FilterPattern}}",
                  "LogGroupName": "{{LogGroupName}}",
                  "MetricName": "{{MetricName}}",
                  "MetricNamespace": "{{MetricNamespace}}",
                  "MetricValue": "{{MetricValue}}",
                  "TopicArn": "{{CreateTopic.TopicArn}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import logging
import os

import boto3
from botocore.config import Config


class LogGroupCreationError(Exception):
    pass


class LogGroupVerificationError(Exception):
    pass


class MetricFilterCreationError(Exception):
    pass


class MetricAlarmCreationError(Exception):
    pass


class RemediationError(Exception):
    pass


boto_config = Config(retries={"max_attempts": 10, "mode": "standard"})

log = logging.getLogger()
LOG_LEVEL = str(os.getenv("LogLevel", "INFO"))
log.setLevel(LOG_LEVEL)


def get_service_client(service_name):
    """
    Returns the service client for given the service name
    :param service_name: name of the service
    :return: service client
    """
    log.debug("Getting the service client for service: {}".format(service_name))
    return boto3.client(service_name, config=boto_config)


def _get_error_code(exception):
    return getattr(exception, "response", {}).get("Error", {}).get("Code", "")


def _check_log_group_exists(logs_client, log_group_name):
    response = logs_client.describe_log_groups(logGroupNamePrefix=log_group_name)
    for group in response.get("logGroups", []):
        if group["logGroupName"] == log_group_name:
            return True
    return False


def _create_log_group_with_fallback(logs_client, log_group_name):
    try:
        logs_client.create_log_group(logGroupName=log_group_name)
        log.info(f"Successfully created log group {log_group_name}")
        return {"exists": True, "created": True}
    except Exception as create_error:
        error_code = _get_error_code(create_error)
        if error_code == "ResourceAlreadyExistsException":
            log.info(f"Log group {log_group_name} already exists")
            return {"exists": True, "created": False}
        else:
            log.error(f"Failed to create log group: {str(create_error)}")
            raise LogGroupCreationError(
                f"Cannot create log group {log_group_name}: {str(create_error)}"
            )


def ensure_log_group_exists(logs_client, log_group_name):
    """
    Ensures a CloudWatch log group exists, creating it if necessary.

    :param logs_client: CloudWatch Logs client
    :param log_group_name: Name of the log group to ensure exists
    :return: dict with 'exists' (bool) and 'created' (bool) keys indicating the result
    :raises LogGroupCreationError: If log group creation fails
    :raises LogGroupVerificationError: If log group existence cannot be verified
    """
    try:
        log.info(f"Checking if log group {log_group_name} exists")
        if _check_log_group_exists(logs_client, log_group_name):
            log.info(f"Log group {log_group_name} already exists")
            return {"exists": True, "created": False}

        log.info(f"Log group {log_group_name} not found, creating it")
        logs_client.create_log_group(logGroupName=log_group_name)
        log.info(f"Successfully created log group {log_group_name}")
        return {"exists": True, "created": True}

    except Exception as e:
        error_code = _get_error_code(e)

        if error_code == "AccessDeniedException" and "DescribeLogGroups" in str(e):
            log.info(
                f"Cannot describe log groups due to permissions, attempting to create {log_group_name} directly"
            )
            return _create_log_group_with_fallback(logs_client, log_group_name)
        elif error_code == "ResourceAlreadyExistsException":
            log.info(f"Log group {log_group_name} was created by another process")
            return {"exists": True, "created": False}
        else:
            log.error(f"Failed to ensure log group exists: {str(e)}")
            raise LogGroupVerificationError(
                f"Cannot create or verify log group {log_group_name}: {str(e)}"
            )


def put_metric_filter(
    cw_log_group,
    filter_name,
    filter_pattern,
    metric_name,
    metric_namespace,
    metric_value,
):
    """
    Puts the metric filter on the CloudWatch log group with provided values
    :param cw_log_group: Name of the CloudWatch log group
    :param filter_name: Name of the filter
    :param filter_pattern: Pattern for the filter
    :param metric_name: Name of the metric
    :param metric_namespace: Namespace where metric is logged
    :param metric_value: Value to be logged for the metric
    """
    logs_client = get_service_client("logs")
    log.info(f"Creating metric filter '{filter_name}' for log group '{cw_log_group}'")
    log.debug(
        f"Filter details: pattern='{filter_pattern}', metric='{metric_name}', namespace='{metric_namespace}', value='{metric_value}'"
    )

    # Ensure log group exists first
    log_group_result = ensure_log_group_exists(logs_client, cw_log_group)
    if not log_group_result["exists"]:
        raise LogGroupVerificationError(
            f"Cannot proceed without log group {cw_log_group}"
        )

    if log_group_result["created"]:
        log.info(f"Log group {cw_log_group} was created for this operation")
    else:
        log.info(f"Using existing log group {cw_log_group}")

    try:
        logs_client.put_metric_filter(
            logGroupName=cw_log_group,
            filterName=filter_name,
            filterPattern=filter_pattern,
            metricTransformations=[
                {
                    "metricName": metric_name,
                    "metricNamespace": metric_namespace,
                    "metricValue": str(metric_value),
                    "unit": "Count",
                }
            ],
        )
        log.info(
            f"Successfully created metric filter '{filter_name}' on log group '{cw_log_group}'"
        )

    except Exception as e:
        error_msg = f"Failed to create metric filter '{filter_name}' on log group '{cw_log_group}': {str(e)}"
        log.error(error_msg)
        raise MetricFilterCreationError(error_msg)


def put_metric_alarm(
    alarm_name, alarm_desc, alarm_threshold, metric_name, metric_namespace, topic_arn
):
    """
    Puts the metric alarm for the metric name with provided values
    :param alarm_name: Name for the alarm
    :param alarm_desc: Description for the alarm
    :param alarm_threshold: Threshold value for the alarm
    :param metric_name: Name of the metric
    :param metric_namespace: Namespace where metric is logged
    :param topic_arn: SNS topic ARN for alarm notifications
    """
    cw_client = get_service_client("cloudwatch")
    log.info(
        f"Creating CloudWatch alarm '{alarm_name}' for metric '{metric_name}' in namespace '{metric_namespace}'"
    )
    log.debug(f"Alarm details: threshold={alarm_threshold}, topic={topic_arn}")

    try:
        cw_client.put_metric_alarm(
            AlarmName=alarm_name,
            AlarmDescription=alarm_desc,
            ActionsEnabled=True,
            OKActions=[topic_arn],
            AlarmActions=[topic_arn],
            MetricName=metric_name,
            Namespace=metric_namespace,
            Statistic="Sum",
            Period=300,
            Unit="Count",
            EvaluationPeriods=12,
            DatapointsToAlarm=1,
            Threshold=alarm_threshold,
            ComparisonOperator="GreaterThanOrEqualToThreshold",
            TreatMissingData="notBreaching",
        )
        log.info(f"Successfully created CloudWatch alarm '{alarm_name}'")

    except Exception as e:
        error_msg = f"Failed to create CloudWatch alarm '{alarm_name}': {str(e)}"
        log.error(error_msg)
        raise MetricAlarmCreationError(error_msg)


def verify(event, _):
    log.info("Starting CreateLogMetricFilterAndAlarm remediation")
    log.debug(f"Event parameters: {event}")

    required_params = [
        "FilterName",
        "FilterPattern",
        "MetricName",
        "MetricNamespace",
        "MetricValue",
        "AlarmName",
        "AlarmDesc",
        "AlarmThreshold",
        "LogGroupName",
        "TopicArn",
    ]

    for param in required_params:
        if param not in event:
            raise ValueError(f"Missing required parameter: {param}")

    filter_name = event["FilterName"]
    filter_pattern = event["FilterPattern"]
    metric_name = event["MetricName"]
    metric_namespace = event["MetricNamespace"]
    metric_value = event["MetricValue"]
    alarm_name = event["AlarmName"]
    alarm_desc = event["AlarmDesc"]
    alarm_threshold = event["AlarmThreshold"]
    cw_log_group = event["LogGroupName"]
    topic_arn = event["TopicArn"]

    try:
        log.info("Step 1: Creating metric filter")
        put_metric_filter(
            cw_log_group,
            filter_name,
            filter_pattern,
            metric_name,
            metric_namespace,
            metric_value,
        )

        log.info("Step 2: Creating CloudWatch alarm")
        put_metric_alarm(
            alarm_name,
            alarm_desc,
            alarm_threshold,
            metric_name,
            metric_namespace,
            topic_arn,
        )

        success_message = f"Successfully created metric filter '{filter_name}' and alarm '{alarm_name}' for log group '{cw_log_group}'"
        log.info(success_message)

        return {
            "response": {
                "message": success_message,
                "status": "Success",
                "filterName": filter_name,
                "alarmName": alarm_name,
                "logGroupName": cw_log_group,
                "metricName": metric_name,
            }
        }

    except Exception as e:
        error_message = f"Failed to create metric filter and alarm: {str(e)}"
        log.error(error_message)
        raise RemediationError(error_message)",
              },
              "name": "CreateMetricFilerAndAlarm",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "CreateMetricFilerAndAlarm.Output",
            "CreateTopic.TopicArn",
          ],
          "parameters": {
            "AlarmDesc": {
              "allowedPattern": ".*",
              "description": "Description of the Alarm to be created for the metric filter",
              "type": "String",
            },
            "AlarmName": {
              "allowedPattern": ".*",
              "description": "Name of the Alarm to be created for the metric filter",
              "type": "String",
            },
            "AlarmThreshold": {
              "description": "Threshold value for the alarm",
              "type": "Integer",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "FilterName": {
              "allowedPattern": ".*",
              "description": "Name for the metric filter",
              "type": "String",
            },
            "FilterPattern": {
              "allowedPattern": ".*",
              "description": "Filter pattern to create metric filter",
              "type": "String",
            },
            "KMSKeyArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "description": "The ARN of a KMS key to use for encryption of the SNS Topic and Config bucket",
              "type": "String",
            },
            "LogGroupName": {
              "allowedPattern": ".*",
              "description": "Name of the log group to be used to create metric filter",
              "type": "String",
            },
            "MetricName": {
              "allowedPattern": ".*",
              "description": "Name of the metric for metric filter",
              "type": "String",
            },
            "MetricNamespace": {
              "allowedPattern": ".*",
              "description": "Namespace where the metrics will be sent",
              "type": "String",
            },
            "MetricValue": {
              "description": "Value of the metric for metric filter",
              "type": "Integer",
            },
            "SNSTopicName": {
              "allowedPattern": "^[a-zA-Z0-9][a-zA-Z0-9-_]{0,255}$",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-CreateLogMetricFilterAndAlarm",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRDisablePublicAccessToRDSInstance": {
      "DependsOn": [
        "CreateWait7",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-DisablePublicAccessToRDSInstance

## What does this document do?
The runbook disables public accessibility for the Amazon RDS database instance you specify using
the [ModifyDBInstance](https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ModifyDBInstance.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* RDSInstanceARN: (Required) ARN for the DB instance you want to disable public accessibility.

## Output Parameters
* DisablePubliclyAccessibleOnRDS.Response: The standard HTTP response from the ModifyDBInstance API.

## Troubleshooting
* ModifyDBInstance isn't supported for a DB instance in a Multi-AZ DB Cluster.
 - This remediation will not work on an instance within a MySQL or PostgreSQL Multi-AZ Cluster due to limitations with the RDS API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## GetRDSInstanceIdentifier
Gathers the DB instance identifier from the DB instance resource identifier.
## Outputs
* DbInstanceIdentifier: The Amazon RDS DB instance identifier.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ RDSInstanceARN }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "GetRDSInstanceIdentifier",
              "outputs": [
                {
                  "Name": "DbInstanceIdentifier",
                  "Selector": "$.DBInstances[0].DBInstanceIdentifier",
                  "Type": "String",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyDBInstanceStatus
Verifies the DB instances is in an AVAILABLE state.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ GetRDSInstanceIdentifier.DbInstanceIdentifier }}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.DBInstances[0].DBInstanceStatus",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "VerifyDBInstanceStatus",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## DisablePubliclyAccessibleOnRDS
Disables public accessibility on your DB instance.
## Outputs
* Response: The standard HTTP response from the ModifyDBInstance API.
",
              "inputs": {
                "Api": "ModifyDBInstance",
                "DBInstanceIdentifier": "{{ GetRDSInstanceIdentifier.DbInstanceIdentifier }}",
                "PubliclyAccessible": false,
                "Service": "rds",
              },
              "isEnd": false,
              "name": "DisablePubliclyAccessibleOnRDS",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "description": "## WaitForDBInstanceStatusToModify
Waits for the DB instance to change to a MODIFYING state.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ GetRDSInstanceIdentifier.DbInstanceIdentifier }}",
                "DesiredValues": [
                  "modifying",
                ],
                "PropertySelector": "$.DBInstances[0].DBInstanceStatus",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "WaitForDBInstanceStatusToModify",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "description": "## WaitForDBInstanceStatusToAvailableAfterModify
Waits for the DB instance to change to an AVAILABLE state
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ GetRDSInstanceIdentifier.DbInstanceIdentifier }}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.DBInstances[0].DBInstanceStatus",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "WaitForDBInstanceStatusToAvailableAfterModify",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyDBInstancePubliclyAccess
Confirms public accessibility is disabled on the DB instance.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ GetRDSInstanceIdentifier.DbInstanceIdentifier }}",
                "DesiredValues": [
                  "False",
                ],
                "PropertySelector": "$.DBInstances[0].PubliclyAccessible",
                "Service": "rds",
              },
              "isEnd": true,
              "name": "VerifyDBInstancePubliclyAccess",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "DisablePubliclyAccessibleOnRDS.Response",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RDSInstanceARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):rds:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:db:.+$",
              "description": "(Required) ARN for the DB instance you want to disable public accessibility.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-DisablePublicAccessToRDSInstance",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRDisablePublicAccessToRedshiftCluster": {
      "DependsOn": [
        "CreateWait3",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-DisablePublicAccessToRedshiftCluster

## What does this document do?
The runbook disables public accessibility for the Amazon Redshift cluster you specify using the [ModifyCluster]
(https://docs.aws.amazon.com/redshift/latest/APIReference/API_ModifyCluster.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* ClusterIdentifier: (Required) The unique identifier of the cluster you want to disable the public accessibility.

## Output Parameters
* DisableRedshiftPubliclyAccessible.Response: The standard HTTP response from the ModifyCluster API call.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## DisableRedshiftPubliclyAccessible
Disables public accessibility for the cluster specified in the ClusterIdentifer parameter.
## Outputs
* Response: The standard HTTP response from the ModifyCluster API call.
",
              "inputs": {
                "Api": "ModifyCluster",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "PubliclyAccessible": false,
                "Service": "redshift",
              },
              "isEnd": false,
              "name": "DisableRedshiftPubliclyAccessible",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "description": "## WaitForRedshiftClusterAvailability
Waits for the state of the cluster to change to available.
",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.Clusters[0].ClusterStatus",
                "Service": "redshift",
              },
              "isEnd": false,
              "name": "WaitForRedshiftClusterAvailability",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyRedshiftPubliclyAccessible
Confirms the public accessibility setting is disabled on the cluster.
",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "DesiredValues": [
                  "False",
                ],
                "PropertySelector": "$.Clusters[0].PubliclyAccessible",
                "Service": "redshift",
              },
              "isEnd": true,
              "name": "VerifyRedshiftPubliclyAccessible",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "DisableRedshiftPubliclyAccessible.Response",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ClusterIdentifier": {
              "allowedPattern": "^(?!.*--)[a-z][a-z0-9-]{0,62}(?<!-)$",
              "description": "(Required) The unique identifier of the cluster you want to disable the public accessibility.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-DisablePublicAccessToRedshiftCluster",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRDisablePublicIPAutoAssign": {
      "DependsOn": [
        "CreateWait8",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-DisablePublicIPAutoAssign

## What does this document do?
  This document disables public IP auto assignment on given subnet using
  [ModifySubnetAttribute](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifySubnetAttribute.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* SubnetARN: (Required)  The ARN of the Amazon EC2 Subnet.

## Security Standards / Controls
* AWS FSBP v1.0.0:  EC2.15
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "subnet_arn": "{{ SubnetARN }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_ec2():
    return boto3.client("ec2", config=boto_config)


def lambda_handler(event, _):
    """
    Disable public IP auto assignment on a subnet.

    \`event\` should have the following keys and values:
    \`subnet_arn\`: the ARN of the subnet that has public IP auto assignment enabled.

    \`context\` is ignored
    """

    subnet_arn = event["subnet_arn"]

    subnet_id = subnet_arn.split("/")[1]

    disable_publicip_auto_assign(subnet_id)

    subnet_attributes = describe_subnet(subnet_id)

    public_ip_on_launch = subnet_attributes["Subnets"][0]["MapPublicIpOnLaunch"]

    if public_ip_on_launch is False:
        return {"MapPublicIpOnLaunch": public_ip_on_launch}

    raise RuntimeError(
        f"ASR Remediation failed - {subnet_id} did not have public IP auto assignment turned off."
    )


def disable_publicip_auto_assign(subnet_id):
    """
    Disables public IP Auto Assign on the subnet \`subnet_id\`
    """
    ec2 = connect_to_ec2()
    try:
        ec2.modify_subnet_attribute(
            MapPublicIpOnLaunch={"Value": False}, SubnetId=subnet_id
        )

    except Exception as e:
        exit("There was an error turning off public IP auto assignment: " + str(e))


def describe_subnet(subnet_id):
    """
    Grabs Subnet Attributes to verify subnet values were set as expected.
    """
    ec2 = connect_to_ec2()
    try:
        subnet_attributes = ec2.describe_subnets(SubnetIds=[subnet_id])
        return subnet_attributes

    except Exception as e:
        exit("Failed to get attributes of subnet: " + str(e))",
              },
              "name": "DisablePublicIPAutoAssign",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "DisablePublicIPAutoAssign.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "SubnetARN": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):ec2:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:subnet\\/(subnet-[0-9a-f]*)$",
              "description": "(Required) The ARN of the Amazon EC2 Subnet.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-DisablePublicIPAutoAssign",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRDisableTGWAutoAcceptSharedAttachments": {
      "DependsOn": [
        "CreateWait12",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-DisableTGWAutoAcceptSharedAttachments

## What does this document do?
  This document turns off AutoAcceptSharedAttachments on a transit gateway to ensure that only authorized VPC attachment requests are accepted.
  [ModifyTransitGateway](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyTransitGateway.html) API.


## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* TransitGatewayId: (Required)  The Id of the transit gateway.

## Security Standards / Controls
* AFSBP v1.0.0:  EC2.23
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "TransitGatewayId": "{{ TransitGatewayId }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_ec2():
    return boto3.client("ec2", config=boto_config)


def lambda_handler(event, _):
    tgw_id = event["TransitGatewayId"]

    ec2 = connect_to_ec2()

    try:
        ec2.modify_transit_gateway(
            TransitGatewayId=tgw_id, Options={"AutoAcceptSharedAttachments": "disable"}
        )

        tgw_updated = ec2.describe_transit_gateways(TransitGatewayIds=[tgw_id])
        if (
            tgw_updated["TransitGateways"][0]["Options"]["AutoAcceptSharedAttachments"]
            == "disable"
        ):
            return {
                "response": {
                    "message": "Transit Gateway AutoAcceptSharedAttachments option disabled.",
                    "status": "Success",
                }
            }
        else:
            return {
                "response": {
                    "message": "Failed to disable AutoAcceptSharedAttachments on Transit Gateway.",
                    "status": "Failed",
                }
            }

    except Exception as e:
        exit("Failed to disable AutoAcceptSharedAttachments: " + str(e))",
              },
              "maxAttempts": 3,
              "name": "DisableTGWAutoAcceptSharedAttachments",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "DisableTGWAutoAcceptSharedAttachments.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "TransitGatewayId": {
              "allowedPattern": "^tgw-[a-z0-9\\-]+$",
              "description": "(Required) The Id of the Transit Gateway.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-DisableTGWAutoAcceptSharedAttachments",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRDisableUnrestrictedAccessToHighRiskPorts": {
      "DependsOn": [
        "CreateWait11",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-DisableUnrestrictedAccessToHighRiskPorts

## What does this document do?
  This document disables unrestricted access to high risk ports using 
  [DescribeSecurityGroupRules](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeSecurityGroupRules.html) API,
  [ModifySecurityGroupRules](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifySecurityGroupRules.html) API.


## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* SecurityGroupId: (Required)  The Id of the security group.

## Security Standards / Controls
* AFSBP v1.0.0:  EC2.19
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "SecurityGroupId": "{{ SecurityGroupId }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})

# List of high risk ports to check for unrestricted access
PORTS_TO_CHECK = {
    20,
    21,
    22,
    23,
    25,
    110,
    135,
    143,
    445,
    1433,
    1434,
    3000,
    3306,
    3389,
    4333,
    5000,
    5432,
    5500,
    5601,
    8080,
    8088,
    8888,
    9200,
    9300,
}
# IPV4 and IPV6 open access
OPENIPV4 = "0.0.0.0/0"
OPENIPV6 = "::/0"
PROTOCOLS = {"tcp", "udp", "-1"}


def connect_to_ec2():
    return boto3.client("ec2", config=boto_config)


class Event(TypedDict):
    SecurityGroupId: str


def lambda_handler(event: Event, _):
    rules_deleted = []
    try:
        security_group_id = event["SecurityGroupId"]

        security_group_rules = get_security_group_rules(security_group_id)

        rules_deleted = delete_rules_with_access_to_high_risk_ports(
            security_group_id, security_group_rules
        )
    except Exception as e:
        raise RuntimeError("Failed to remove security group rules: " + str(e))

    if not rules_deleted:
        raise RuntimeError(
            f"Could not find rules to delete for Security Group {security_group_id}. Please check the inbound "
            f"rules manually."
        )
    return {
        "message": "Successfully removed security group rules on " + security_group_id,
        "status": "Success",
        "rules_deleted": rules_deleted,
    }


def get_security_group_rules(security_group_id: str) -> list:
    ec2 = connect_to_ec2()
    try:
        paginator = ec2.get_paginator("describe_security_group_rules")
        page_iterator = paginator.paginate(
            Filters=[
                {
                    "Name": "group-id",
                    "Values": [security_group_id],
                },
            ]
        )

        security_group_rules = []
        for page in page_iterator:
            security_group_rules.extend(page.get("SecurityGroupRules", []))

        return security_group_rules
    except Exception as e:
        exit("Failed to describe security group rules: " + str(e))


def delete_rules_with_access_to_high_risk_ports(
    security_group_id: str, security_group_rules: list
):
    ec2 = connect_to_ec2()
    rules_deleted = []
    for rule in security_group_rules:
        if rule_has_access_to_high_risk_ports(rule) and is_open_cidr(rule):
            try:
                ec2.revoke_security_group_ingress(
                    GroupId=security_group_id,
                    SecurityGroupRuleIds=[
                        rule["SecurityGroupRuleId"],
                    ],
                )
                rules_deleted.append(rule["SecurityGroupRuleId"])
            except Exception as e:
                print(f"Failed to delete rule {rule['SecurityGroupRuleId']}: {str(e)}")
    return rules_deleted


def rule_has_access_to_high_risk_ports(rule: dict) -> bool:
    return (
        rule["IpProtocol"] in PROTOCOLS
        and not rule["IsEgress"]
        and (
            any(
                port in range(rule["FromPort"], rule["ToPort"] + 1)
                for port in PORTS_TO_CHECK
            )
            or (rule["FromPort"] == rule["ToPort"] == -1)
        )
    )


def is_open_cidr(rule: dict) -> bool:
    return ("CidrIpv4" in rule and rule["CidrIpv4"] == OPENIPV4) or (
        "CidrIpv6" in rule and rule["CidrIpv6"] == OPENIPV6
    )",
              },
              "maxAttempts": 3,
              "name": "DisableUnrestrictedAccessToHighRiskPorts",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "DisableUnrestrictedAccessToHighRiskPorts.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "SecurityGroupId": {
              "allowedPattern": "^sg-[a-z0-9\\-]+$",
              "description": "(Required) The Id of the Seurity Group.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-DisableUnrestrictedAccessToHighRiskPorts",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAPIGatewayCacheDataEncryption": {
      "DependsOn": [
        "CreateWait14",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableAPIGatewayCacheDataEncryption

## What does this document do?
  This document enables encryption on API Gateway REST API cache data

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* APIGatewayStageName: (Required) The name of the API Gateway REST API stage.

## Security Standards / Controls
* NIST 800-53 Rev5: APIGateway.5
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Enable encryption on API Gateway REST API cache data
",
              "inputs": {
                "Handler": "enable_data_encryption",
                "InputPayload": {
                  "APIGatewayStageArn": "{{ APIGatewayStageArn }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

from typing import TYPE_CHECKING, TypedDict

if TYPE_CHECKING:
    from mypy_boto3_apigateway import APIGatewayClient
else:
    APIGatewayClient = object

import boto3
from botocore.config import Config


def connect_to_apigateway(boto_config: Config) -> APIGatewayClient:
    return boto3.client("apigateway", config=boto_config)


class MethodSettings(TypedDict):
    ResourcePath: str
    HttpMethod: str


class Event(TypedDict):
    APIGatewayStageArn: str


def enable_data_encryption(event: Event, _):
    boto_config = Config(retries={"mode": "standard"})
    apigateway = connect_to_apigateway(boto_config)

    try:
        api_id = event["APIGatewayStageArn"].split("/")[2]
        stage_name = event["APIGatewayStageArn"].split("/")[4]
        print(api_id, stage_name)
        stage_details = apigateway.get_stage(restApiId=api_id, stageName=stage_name)
        for method_key, method_value in stage_details["methodSettings"].items():
            apigateway.update_stage(
                restApiId=api_id,
                stageName=stage_name,
                patchOperations=[
                    {
                        "op": "replace",
                        "path": f"/{method_key}/caching/dataEncrypted",
                        "value": "true",
                    },
                ],
            )

        return {
            "message": "Successfully enabled cache data encryption.",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(f"Encountered error setting cache data encryption: {str(e)}")",
              },
              "isEnd": true,
              "name": "EnableAPIGatewayCacheDataEncryption",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "parameters": {
            "APIGatewayStageArn": {
              "allowedPattern": "^(arn:(?:aws|aws-cn|aws-us-gov):apigateway:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d)::\\/restapis\\/(.+)\\/stages\\/.+)$",
              "description": "(Required) The ARN of the API Gateway stage.",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAPIGatewayCacheDataEncryption",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAPIGatewayExecutionLogs": {
      "DependsOn": [
        "CreateWait14",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableAPIGatewayExecutionLogs

## What does this document do?
  This document enables logging on the given API Gateway Stage.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* APIGatewayStageArnSuffix: (Required) The suffix of the API Gateway stage ARN.
* LoggingLevel: (Required) The log level which should be set in the Stage.

## Security Standards / Controls
* AFSBP: APIGateway.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Enable logging in the API Gateway Stage
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "APIGatewayStageArnSuffix": "{{ APIGatewayStageArnSuffix }}",
                  "LoggingLevel": "{{ LoggingLevel }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import logging
import re
import traceback
from typing import Any, TypedDict

import boto3
from botocore.config import Config

logger = logging.getLogger()
boto_config = Config(retries={"mode": "standard"})


def connect_to_service(client: str) -> Any:
    return boto3.client(client, config=boto_config)


class Event(TypedDict):
    APIGatewayStageArnSuffix: str
    LoggingLevel: str  # Security Hub parameter


class Response(TypedDict):
    Message: str
    LoggingLevel: str
    ApiId: str
    StageName: str
    ApiType: str  # REST or WebSocket


class APIStageDetails(TypedDict):
    StageName: str
    ApiId: str


def handler(event: Event, _):
    stage_arn_suffix = event["APIGatewayStageArnSuffix"]
    logging_level = event["LoggingLevel"]

    stage_is_rest_api = is_rest_api(stage_arn_suffix)
    api_stage_details: APIStageDetails = extract_details_from_arn_suffix(
        stage_arn_suffix, stage_is_rest_api
    )

    if stage_is_rest_api:
        enable_rest_execution_logging(api_stage_details, logging_level)
    else:
        enable_websocket_execution_logging(api_stage_details, logging_level)

    stage_name = api_stage_details["StageName"]
    api_id = api_stage_details["ApiId"]
    return {
        "Message": f"successfully enabled execution logging at {logging_level} for stage {stage_name} in API {api_id}",
        "LoggingLevel": logging_level,
        "ApiId": api_id,
        "StageName": stage_name,
        "ApiType": "REST" if stage_is_rest_api else "WebSocket",
    }


def is_rest_api(arn_suffix: str) -> bool:
    if arn_suffix.startswith("/restapis/"):
        return True
    return False


def extract_details_from_arn_suffix(arn_suffix: str, is_rest: bool) -> APIStageDetails:
    regex_pattern = (
        r"^/restapis/(?P<api_id>.+)/stages/(?P<stage_name>.+)$"
        if is_rest
        else r"^/apis/(?P<api_id>.+)/stages/(?P<stage_name>.+)$"
    )
    match = re.fullmatch(regex_pattern, arn_suffix)

    if (
        not match
        or "api_id" not in match.groupdict()
        or "stage_name" not in match.groupdict()
    ):
        raise RuntimeError(
            f"Encountered malformed API stage ARN: {arn_suffix}\\n Expected to be a REST or WebSocket API of the form "
            f"/apis/api-id/stages/stage-name OR /restapis/api-id/stages/stage-name"
        )
    return {"ApiId": match.group("api_id"), "StageName": match.group("stage_name")}


def enable_rest_execution_logging(
    stage_details: APIStageDetails, log_level: str
) -> None:
    rest_apigateway_client = connect_to_service("apigateway")
    api_id = stage_details["ApiId"]
    stage_name = stage_details["StageName"]
    try:
        rest_apigateway_client.update_stage(
            restApiId=api_id,
            stageName=stage_name,
            patchOperations=[
                {
                    "op": "replace",
                    "path": "/*/*/logging/loglevel",
                    "value": log_level,
                },
            ],
        )

        logger.info(
            f"Set log level to {log_level} for all routes in stage {stage_name} in REST API {api_id}"
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered exception enabling execution logging for REST API stage {stage_name} in API {api_id}: {str(e)}\\n\\n{traceback.format_exc()}"
        )


def enable_websocket_execution_logging(
    stage_details: APIStageDetails, log_level: str
) -> None:
    websocket_apigateway_client = connect_to_service("apigatewayv2")
    api_id = stage_details["ApiId"]
    stage_name = stage_details["StageName"]
    try:
        response = websocket_apigateway_client.get_stage(
            ApiId=api_id, StageName=stage_name
        )  # fetch all routes in this stage that don't meet logging requirements
        routes_to_update = [
            route
            for route, route_setting in response["RouteSettings"].items()
            if "LoggingLevel" not in route_setting
            or route_setting["LoggingLevel"] != log_level
        ]
        route_settings = {
            route: {"LoggingLevel": log_level} for route in routes_to_update
        }

        websocket_apigateway_client.update_stage(
            ApiId=api_id,
            DefaultRouteSettings={
                "LoggingLevel": log_level,
            },
            RouteSettings=route_settings,
            StageName=stage_name,
        )

        updated_routes = [*routes_to_update, "default"]
        logger.info(
            f"Updated the following routes to log level {log_level} in stage {stage_name} in WebSocket API {api_id}: {str(updated_routes)}"
        )

    except Exception as e:
        raise RuntimeError(
            f"Encountered exception enabling execution logging for WebSocket API stage {stage_name} in API {api_id}: {str(e)}\\n\\n{traceback.format_exc()}"
        )",
              },
              "isEnd": true,
              "name": "EnableAPIGatewayExecutionLogs",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableAPIGatewayExecutionLogs.Output",
          ],
          "parameters": {
            "APIGatewayStageArnSuffix": {
              "allowedPattern": "\\/apis\\/(.+)\\/stages\\/(.+)|\\/restapis\\/(.+)\\/stages\\/(.+)",
              "description": "(Required) The suffix of the API Gateway stage ARN.",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "LoggingLevel": {
              "allowedPattern": "INFO|ERROR",
              "description": "(Required) The logging level to set in the Stage.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAPIGatewayExecutionLogs",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAWSConfig": {
      "DependsOn": [
        "CreateWait0",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableAWSConfig

## What does this document do?
Enables AWS Config:
* Turns on recording for all resources.
* Creates an encrypted bucket for Config logging.
* Creates a logging bucket for access logs for the config bucket
* Creates an SNS topic for Config notifications
* Creates a service-linked role

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* KMSKeyArn: KMS Customer-managed key to use for encryption of Config log data and SNS Topic
* AWSServiceRoleForConfig: (Optional) The name of the exiting IAM role to use for the Config service. Default: aws-service-role/config.amazonaws.com/AWSServiceRoleForConfig
* SNSTopicName: (Required) Name of the SNS Topic to use to post AWS Config messages.

## Output Parameters
* Remediation.Output: STDOUT and messages from the remediation steps.
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_encrypted_topic",
                "InputPayload": {
                  "kms_key_arn": "{{KMSKeyArn}}",
                  "topic_name": "{{SNSTopicName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

boto_config = Config(retries={"mode": "standard"})


def connect_to_sns():
    return boto3.client("sns", config=boto_config)


def connect_to_ssm():
    return boto3.client("ssm", config=boto_config)


def create_encrypted_topic(event, _):
    kms_key_arn = event["kms_key_arn"]
    new_topic = False
    topic_arn = ""
    topic_name = event["topic_name"]

    try:
        sns = connect_to_sns()
        topic_arn = sns.create_topic(
            Name=topic_name, Attributes={"KmsMasterKeyId": kms_key_arn.split("key/")[1]}
        )["TopicArn"]
        new_topic = True

    except ClientError as client_exception:
        exception_type = client_exception.response["Error"]["Code"]
        if exception_type == "InvalidParameter":
            print(
                f"Topic {topic_name} already exists. This remediation may have been run before."
            )
            print("Ignoring exception - remediation continues.")
            topic_arn = sns.create_topic(Name=topic_name)["TopicArn"]
        else:
            exit(f"ERROR: Unhandled client exception: {client_exception}")

    except Exception as e:
        exit(f"ERROR: could not create SNS Topic {topic_name}: {str(e)}")

    if new_topic:
        try:
            ssm = connect_to_ssm()
            ssm.put_parameter(
                Name="/Solutions/SO0111/SNS_Topic_Config.1",
                Description="SNS Topic for AWS Config updates",
                Type="String",
                Overwrite=True,
                Value=topic_arn,
            )
        except Exception as e:
            exit(f"ERROR: could not create SNS Topic {topic_name}: {str(e)}")

    create_topic_policy(topic_arn)

    return {"topic_arn": topic_arn}


def create_topic_policy(topic_arn):
    sns = connect_to_sns()
    try:
        topic_policy = {
            "Id": "Policy_ID",
            "Statement": [
                {
                    "Sid": "AWSConfigSNSPolicy",
                    "Effect": "Allow",
                    "Principal": {"Service": "config.amazonaws.com"},
                    "Action": "SNS:Publish",
                    "Resource": topic_arn,
                }
            ],
        }

        sns.set_topic_attributes(
            TopicArn=topic_arn,
            AttributeName="Policy",
            AttributeValue=json.dumps(topic_policy),
        )
    except Exception as e:
        exit(f"ERROR: Failed to SetTopicAttributes for {topic_arn}: {str(e)}")",
              },
              "isEnd": false,
              "name": "CreateTopic",
              "outputs": [
                {
                  "Name": "TopicArn",
                  "Selector": "$.Payload.topic_arn",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeAutomation",
              "inputs": {
                "DocumentName": "ASR-CreateAccessLoggingBucket",
                "RuntimeParameters": {
                  "AutomationAssumeRole": {
                    "Fn::Join": [
                      "",
                      [
                        "arn:{{global:AWS_PARTITION}}:iam::{{global:ACCOUNT_ID}}:role/SO0111-CreateAccessLoggingBucket-",
                        {
                          "Ref": "Namespace",
                        },
                      ],
                    ],
                  },
                  "BucketName": "so0111-accesslogs-{{global:ACCOUNT_ID}}-{{global:REGION}}",
                },
              },
              "isEnd": false,
              "name": "CreateAccessLoggingBucket",
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "create_encrypted_bucket",
                "InputPayload": {
                  "account": "{{global:ACCOUNT_ID}}",
                  "kms_key_arn": "{{KMSKeyArn}}",
                  "logging_bucket": "so0111-accesslogs-{{global:ACCOUNT_ID}}-{{global:REGION}}",
                  "partition": "{{global:AWS_PARTITION}}",
                  "region": "{{global:REGION}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

boto_config = Config(retries={"mode": "standard"})


def connect_to_s3(boto_config):
    return boto3.client("s3", config=boto_config)


def create_bucket(bucket_name, aws_region):
    s3 = connect_to_s3(boto_config)
    try:
        if aws_region == "us-east-1":
            s3.create_bucket(ACL="private", Bucket=bucket_name)
        else:
            s3.create_bucket(
                ACL="private",
                Bucket=bucket_name,
                CreateBucketConfiguration={"LocationConstraint": aws_region},
            )
        return "created"

    except ClientError as ex:
        exception_type = ex.response["Error"]["Code"]
        # bucket already exists - return
        if exception_type in ["BucketAlreadyExists", "BucketAlreadyOwnedByYou"]:
            print("Bucket " + bucket_name + " already exists")
            return "already exists"
        else:
            exit(f"ERROR creating bucket {bucket_name}: {str(ex)}")
    except Exception as e:
        exit(f"ERROR creating bucket {bucket_name}: {str(e)}")


def encrypt_bucket(bucket_name, kms_key):
    s3 = connect_to_s3(boto_config)
    try:
        s3.put_bucket_encryption(
            Bucket=bucket_name,
            ServerSideEncryptionConfiguration={
                "Rules": [
                    {
                        "ApplyServerSideEncryptionByDefault": {
                            "SSEAlgorithm": "aws:kms",
                            "KMSMasterKeyID": kms_key,
                        }
                    }
                ]
            },
        )
    except Exception as e:
        exit(f"ERROR putting bucket encryption for {bucket_name}: {str(e)}")


def block_public_access(bucket_name):
    s3 = connect_to_s3(boto_config)
    try:
        s3.put_public_access_block(
            Bucket=bucket_name,
            PublicAccessBlockConfiguration={
                "BlockPublicAcls": True,
                "IgnorePublicAcls": True,
                "BlockPublicPolicy": True,
                "RestrictPublicBuckets": True,
            },
        )
    except Exception as e:
        exit(f"ERROR setting public access block for bucket {bucket_name}: {str(e)}")


def enable_access_logging(bucket_name, logging_bucket):
    s3 = connect_to_s3(boto_config)
    try:
        s3.put_bucket_logging(
            Bucket=bucket_name,
            BucketLoggingStatus={
                "LoggingEnabled": {
                    "TargetBucket": logging_bucket,
                    "TargetPrefix": f"access-logs/{bucket_name}",
                }
            },
        )
    except Exception as e:
        exit(f"Error setting access logging for bucket {bucket_name}: {str(e)}")


def create_bucket_policy(config_bucket, aws_partition):
    s3 = connect_to_s3(boto_config)
    try:
        bucket_policy = {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Sid": "AWSConfigBucketPermissionsCheck",
                    "Effect": "Allow",
                    "Principal": {"Service": ["config.amazonaws.com"]},  # NOSONAR
                    "Action": "s3:GetBucketAcl",
                    "Resource": "arn:" + aws_partition + ":s3:::" + config_bucket,
                },
                {
                    "Sid": "AWSConfigBucketExistenceCheck",
                    "Effect": "Allow",
                    "Principal": {"Service": ["config.amazonaws.com"]},
                    "Action": "s3:ListBucket",
                    "Resource": "arn:" + aws_partition + ":s3:::" + config_bucket,
                },
                {
                    "Sid": "AWSConfigBucketDelivery",
                    "Effect": "Allow",
                    "Principal": {"Service": ["config.amazonaws.com"]},
                    "Action": "s3:PutObject",
                    "Resource": "arn:"
                    + aws_partition
                    + ":s3:::"
                    + config_bucket
                    + "/*",
                    "Condition": {
                        "StringEquals": {"s3:x-amz-acl": "bucket-owner-full-control"}
                    },
                },
                {
                    "Sid": "AllowSSLRequestsOnly",
                    "Action": "s3:*",
                    "Effect": "Deny",
                    "Resource": [
                        "arn:" + aws_partition + ":s3:::" + config_bucket + "/*",
                        "arn:" + aws_partition + ":s3:::" + config_bucket,
                    ],
                    "Condition": {"Bool": {"aws:SecureTransport": "false"}},
                    "Principal": "*",
                },
            ],
        }
        s3.put_bucket_policy(Bucket=config_bucket, Policy=json.dumps(bucket_policy))
    except Exception as e:
        exit(f"ERROR: PutBucketPolicy failed for {config_bucket}: {str(e)}")


def create_encrypted_bucket(event, _):
    kms_key_arn = event["kms_key_arn"]
    aws_partition = event["partition"]
    aws_account = event["account"]
    aws_region = event["region"]
    logging_bucket = event["logging_bucket"]
    bucket_name = "so0111-aws-config-" + aws_region + "-" + aws_account

    if create_bucket(bucket_name, aws_region) == "already exists":
        return {"config_bucket": bucket_name}

    encrypt_bucket(bucket_name, kms_key_arn.split("key/")[1])
    block_public_access(bucket_name)
    enable_access_logging(bucket_name, logging_bucket)
    create_bucket_policy(bucket_name, aws_partition)

    return {"config_bucket": bucket_name}",
              },
              "isEnd": false,
              "name": "CreateConfigBucket",
              "outputs": [
                {
                  "Name": "ConfigBucketName",
                  "Selector": "$.Payload.config_bucket",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "enable_config",
                "InputPayload": {
                  "account": "{{global:ACCOUNT_ID}}",
                  "aws_service_role": "{{AWSServiceRoleForConfig}}",
                  "config_bucket": "{{CreateConfigBucket.ConfigBucketName}}",
                  "partition": "{{global:AWS_PARTITION}}",
                  "region": "{{global:REGION}}",
                  "topic_arn": "{{CreateTopic.TopicArn}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import logging
import traceback
from typing import Any, TypedDict

import boto3
from botocore.config import Config

logger = logging.getLogger()

DEFAULT_CHANNEL_NAME = DEFAULT_RECORDER_NAME = "default"

boto_config = Config(retries={"mode": "standard"})


class Event(TypedDict):
    account: str
    partition: str
    aws_service_role: str
    config_bucket: str
    topic_arn: str


class Response(TypedDict):
    Message: str


class ExistingRecorderDetails(TypedDict):
    name: str
    recording: bool


def connect_to_config() -> Any:
    return boto3.client("config", config=boto_config)


def enable_config(event: Event, _: Any):
    aws_account = event["account"]
    aws_partition = event["partition"]
    aws_service_role = event["aws_service_role"]
    config_bucket = event["config_bucket"]
    topic_arn = event["topic_arn"]

    existing_recorder_details = get_existing_config_recorder()
    existing_recorder = existing_recorder_details["name"]
    existing_recorder_is_recording = existing_recorder_details["recording"]

    create_or_update_config_recorder(
        aws_partition, aws_account, aws_service_role, existing_recorder
    )

    if not has_existing_delivery_channel():
        create_delivery_channel(config_bucket, aws_account, topic_arn)

    if (not existing_recorder) or (
        existing_recorder and not existing_recorder_is_recording
    ):
        start_recorder(existing_recorder)

    return {
        "Message": f"Successfully completed setting up recorder {existing_recorder or DEFAULT_RECORDER_NAME}"
    }


def get_existing_config_recorder() -> ExistingRecorderDetails:
    config_client = connect_to_config()
    try:
        recorder_name = ""
        recording = False
        describe_recorder_response = config_client.describe_configuration_recorders()
        if (
            describe_recorder_response
            and "ConfigurationRecorders" in describe_recorder_response
        ):
            recorder_name = describe_recorder_response["ConfigurationRecorders"][0][
                "name"
            ]  # there is only ever 1 configuration recorder per account per region

        if recorder_name:
            describe_recorder_status_response = (
                config_client.describe_configuration_recorder_status(
                    ConfigurationRecorderNames=[
                        recorder_name,
                    ]
                )
            )
            recording = (
                describe_recorder_status_response["ConfigurationRecordersStatus"][0][
                    "recording"
                ]
                if "ConfigurationRecordersStatus" in describe_recorder_response
                else False
            )

        return {"name": recorder_name, "recording": recording}
    except Exception as e:
        logger.warning(
            f"Encountered an error fetching existing Config Recorder - continuing to create a new recorder: {str(e)} \\n\\n {traceback.format_exc()}"
        )
        return {"name": "", "recording": False}


def create_or_update_config_recorder(
    aws_partition: str,
    aws_account: str,
    aws_service_role: str,
    recorder_name: str,
) -> None:
    if not recorder_name:
        recorder_name = DEFAULT_RECORDER_NAME

    config_client = connect_to_config()
    try:
        config_service_role_arn = (
            "arn:"
            + aws_partition
            + ":iam::"
            + aws_account
            + ":role/"
            + aws_service_role
        )
        config_client.put_configuration_recorder(
            ConfigurationRecorder={
                "name": recorder_name,
                "roleARN": config_service_role_arn,
                "recordingGroup": {
                    "allSupported": True,
                    "includeGlobalResourceTypes": True,
                },
            }
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered an error putting {recorder_name} config recorder: {str(e)} \\n\\n{traceback.format_exc()}"
        )


def has_existing_delivery_channel() -> bool:
    config_client = connect_to_config()
    try:
        response = config_client.describe_delivery_channels()
        if response and "DeliveryChannels" in response:
            return (
                len(response["DeliveryChannels"]) > 0
            )  # there is only ever one delivery channel per account per region
        return False
    except Exception as e:
        logger.warning(
            f"Encountered an error fetching existing delivery channel - continuing to create a new channel: {str(e)} \\n\\n {traceback.format_exc()}"
        )
        return False


def create_delivery_channel(
    config_bucket: str, aws_account: str, topic_arn: str
) -> None:
    config_client = connect_to_config()
    try:
        config_client.put_delivery_channel(
            DeliveryChannel={
                "name": DEFAULT_CHANNEL_NAME,
                "s3BucketName": config_bucket,
                "s3KeyPrefix": aws_account,
                "snsTopicARN": topic_arn,
                "configSnapshotDeliveryProperties": {
                    "deliveryFrequency": "Twelve_Hours"
                },
            }
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered an error creating delivery channel 'default': {str(e)} \\n\\n{traceback.format_exc()}"
        )


def start_recorder(recorder_name: str) -> None:
    if not recorder_name:
        recorder_name = DEFAULT_RECORDER_NAME
    config_client = connect_to_config()
    try:
        config_client.start_configuration_recorder(
            ConfigurationRecorderName=recorder_name
        )
    except Exception as e:
        raise RuntimeError(
            f"Encountered an error starting config recorder 'default': {str(e)} \\n\\n{traceback.format_exc()}"
        )",
              },
              "isEnd": false,
              "name": "EnableConfig",
              "outputs": [
                {
                  "Name": "OutputMessage",
                  "Selector": "$.Payload.Message",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "process_results",
                "InputPayload": {
                  "config_bucket": "{{CreateConfigBucket.ConfigBucketName}}",
                  "enable_config_message": "{{EnableConfig.OutputMessage}}",
                  "logging_bucket": "so0111-accesslogs-{{global:ACCOUNT_ID}}-{{global:REGION}}",
                  "sns_topic_arn": "{{CreateTopic.TopicArn}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def process_results(event, _):
    print(f'Created encrypted SNS topic {event["sns_topic_arn"]}')
    print(f'Created encrypted Config bucket {event["config_bucket"]}')
    print(
        f'Created access logging for Config bucket in bucket {event["logging_bucket"]}'
    )
    remediation_message = event["enable_config_message"]
    return {"Message": remediation_message, "Status": "Success"}",
              },
              "isEnd": true,
              "name": "Remediation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "Remediation.Output",
          ],
          "parameters": {
            "AWSServiceRoleForConfig": {
              "allowedPattern": "^(:?[\\w+=,.@-]+/)+[\\w+=,.@-]+$",
              "default": "aws-service-role/config.amazonaws.com/AWSServiceRoleForConfig",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "KMSKeyArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "description": "The ARN of a KMS key to use for encryption of the SNS Topic and Config bucket",
              "type": "String",
            },
            "SNSTopicName": {
              "allowedPattern": "^[a-zA-Z0-9][a-zA-Z0-9-_]{0,255}$",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAWSConfig",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAthenaWorkGroupLogging": {
      "DependsOn": [
        "CreateWait15",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-AttachServiceVPCEndpoint

## Overview
This document enables logging in the given Athena WorkGroup.

## Pre-requisites
* None

## What does this document do?
  This document enables CloudWatch logging for the given Athena Work Group.

## Input Parameters
* WorkGroupName: (Required) Athena Work Group to be remediated.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableWorkGroupLogging.Output
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## Remediation
Creates and attaches service interface endpoint to VPC.
",
              "inputs": {
                "Api": "UpdateWorkGroup",
                "ConfigurationUpdates": {
                  "PublishCloudWatchMetricsEnabled": true,
                },
                "Service": "athena",
                "WorkGroup": "{{WorkGroupName}}",
              },
              "isEnd": true,
              "maxAttempts": 2,
              "name": "EnableWorkGroupLogging",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "EnableWorkGroupLogging.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "WorkGroupName": {
              "allowedPattern": "^[a-zA-Z0-9._-]{1,128}$",
              "description": "(Required) The WorkGroup name.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAthenaWorkGroupLogging",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAutoScalingGroupELBHealthCheck": {
      "DependsOn": [
        "CreateWait0",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableAutoScalingGroupELBHealthCheck

## What does this document do?
This runbook enables health checks for the Amazon EC2 Auto Scaling (Auto Scaling) group you specify using the [UpdateAutoScalingGroup](https://docs.aws.amazon.com/autoscaling/ec2/APIReference/API_UpdateAutoScalingGroup.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* AutoScalingGroupARN: (Required) The Amazon Resource Name (ARN) of the auto scaling group that you want to enable health checks on.
* HealthCheckGracePeriod: (Optional) The amount of time, in seconds, that Auto Scaling waits before checking the health status of an Amazon Elastic Compute Cloud (Amazon EC2) instance that has come into service.

## Output Parameters

* Remediation.Output - stdout messages from the remediation

## Security Standards / Controls
* AWS FSBP v1.0.0: Autoscaling.1
* CIS v1.2.0:   2.1
* PCI:          Autoscaling.1
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "Enable ELB health check type on ASG",
              "inputs": {
                "Api": "UpdateAutoScalingGroup",
                "AutoScalingGroupName": "{{AutoScalingGroupName}}",
                "HealthCheckGracePeriod": "{{HealthCheckGracePeriod}}",
                "HealthCheckType": "ELB",
                "Service": "autoscaling",
              },
              "name": "EnableELBHealthCheck",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "verify",
                "InputPayload": {
                  "AsgName": "{{AutoScalingGroupName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config


def connect_to_autoscaling(boto_config):
    return boto3.client("autoscaling", config=boto_config)


def verify(event, _):
    boto_config = Config(retries={"mode": "standard"})
    asg_client = connect_to_autoscaling(boto_config)
    asg_name = event["AsgName"]
    try:
        desc_asg = asg_client.describe_auto_scaling_groups(
            AutoScalingGroupNames=[asg_name]
        )
        if len(desc_asg["AutoScalingGroups"]) < 1:
            exit(f"No AutoScaling Group found matching {asg_name}")

        health_check = desc_asg["AutoScalingGroups"][0]["HealthCheckType"]
        print(json.dumps(desc_asg["AutoScalingGroups"][0], default=str))
        if health_check == "ELB":
            return {
                "response": {
                    "message": "Autoscaling Group health check type updated to ELB",
                    "status": "Success",
                }
            }
        else:
            return {
                "response": {
                    "message": "Autoscaling Group health check type is not ELB",
                    "status": "Failed",
                }
            }
    except Exception as e:
        exit("Exception while executing remediation: " + str(e))",
              },
              "name": "Remediation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "Remediation.Output",
          ],
          "parameters": {
            "AutoScalingGroupName": {
              "allowedPattern": "^.{1,255}$",
              "description": "(Required) The Amazon Resource Name (ARN) of the auto scaling group that you want to enable health checks on.",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "HealthCheckGracePeriod": {
              "allowedPattern": "^[0-9]\\d*$",
              "default": 300,
              "description": "(Optional) The amount of time, in seconds, that Auto Scaling waits before checking the health status of an Amazon Elastic Compute Cloud (Amazon EC2) instance that has come into service.",
              "type": "Integer",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAutoScalingGroupELBHealthCheck",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAutoSecretRotation": {
      "DependsOn": [
        "CreateWait13",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableAutoSecretRotation

## What does this document do?
  This document enables automatic rotation on a Secrets Manager secret if a Lambda function is already associated with it.
  [RotateSecret](https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_RotateSecret.html) API.


## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* SecretARN: (Required)  The ARN of the Secrets Manager secret.
* MaximumAllowedRotationFrequency: (Optional) The number of days that a secret must be automatically rotated within.

## Security Standards / Controls
* AFSBP v1.0.0:  SecretsManager.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "MaximumAllowedRotationFrequency": "{{ MaximumAllowedRotationFrequency }}",
                  "SecretARN": "{{ SecretARN }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import boto3
from botocore.config import Config

BOTO_CONFIG = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_secretsmanager():
    return boto3.client("secretsmanager", config=BOTO_CONFIG)


# Check if secret rotation is enabled on the secet.
def check_secret_rotation(secret_arn, secretsmanager_client):
    response = secretsmanager_client.describe_secret(SecretId=secret_arn)
    if "RotationEnabled" in response:
        if response["RotationEnabled"]:
            return True
    else:
        return False


def lambda_handler(event, _):
    secret_arn = event["SecretARN"]
    number_of_days = event["MaximumAllowedRotationFrequency"]

    secretsmanager = connect_to_secretsmanager()

    try:
        # Set rotation schedule following best practices
        secretsmanager.rotate_secret(
            SecretId=secret_arn,
            RotationRules={
                "AutomaticallyAfterDays": int(number_of_days),
            },
            RotateImmediately=False,
        )

        # Verify secret rotation is enabled.
        if check_secret_rotation(secret_arn, secretsmanager):
            return {
                "message": f"Enabled automatic secret rotation every {number_of_days} days with previously set rotation function.",
                "status": "Success",
            }
        else:
            raise RuntimeError(
                "Failed to set automatic rotation schedule. Please manually set rotation on the secret."
            )

    # If a Lambda function ARN is not associated, an exception will be thrown.
    except Exception as e:
        # Verify secret rotation is enabled.
        if check_secret_rotation(secret_arn, secretsmanager):
            return {
                "message": f"Enabled automatic secret rotation every {number_of_days} days with previously set function.",
                "status": "Success",
            }
        else:
            exit(f"Error when setting automatic rotation schedule: {str(e)}")",
              },
              "maxAttempts": 3,
              "name": "EnableAutoSecretRotation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableAutoSecretRotation.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "MaximumAllowedRotationFrequency": {
              "default": 90,
              "description": "(Optional) The number of days that a secret must be automatically rotated within.",
              "type": "Integer",
            },
            "SecretARN": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):secretsmanager:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:secret:([A-Za-z0-9\\/_+=.@-]+)$",
              "description": "(Required) The ARN of the Secrets Manager secret.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAutoSecretRotation",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAutomaticSnapshotsOnRedshiftCluster": {
      "DependsOn": [
        "CreateWait4",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{AutomationAssumeRole}}",
          "description": "### Document name - ASR-EnableAutomaticSnapshotsOnRedshiftCluster

## What does this document do?
The runbook enables automatic snapshots on a Redshift cluster.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* ClusterIdentifier: (Required) The unique identifier of the cluster.
* RetentionPeriod: (Optional) The minimum retention period for the automatic snapshots in days.

## Output Parameters
* QueryRetentionPeriod.CurrentRetentionPeriod: The retention period of the cluster in days at the start of the automation.
* ModifyRetentionPeriod.Response: The response of the API call to modify the retention period of the cluster.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "Service": "redshift",
              },
              "name": "QueryRetentionPeriod",
              "outputs": [
                {
                  "Name": "CurrentRetentionPeriod",
                  "Selector": "$.Clusters[0].AutomatedSnapshotRetentionPeriod",
                  "Type": "Integer",
                },
              ],
            },
            {
              "action": "aws:branch",
              "inputs": {
                "Choices": [
                  {
                    "NextStep": "CastCurrentRetentionPeriodToString",
                    "NumericGreaterOrEquals": "{{MinRetentionPeriod}}",
                    "Variable": "{{QueryRetentionPeriod.CurrentRetentionPeriod}}",
                  },
                ],
                "Default": "ModifyRetentionPeriod",
              },
              "name": "ChooseModifyRetentionPeriod",
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "cast_to_string",
                "InputPayload": {
                  "DesiredParameter": "RetentionPeriod",
                  "RetentionPeriod": "{{QueryRetentionPeriod.CurrentRetentionPeriod}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def cast_to_string(event, _) -> str:
    parameter_to_cast = event["DesiredParameter"]
    return str(event[parameter_to_cast])",
              },
              "name": "CastCurrentRetentionPeriodToString",
              "outputs": [
                {
                  "Name": "CurrentRetentionPeriodString",
                  "Selector": "$.Payload",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "DesiredValues": [
                  "{{CastCurrentRetentionPeriodToString.CurrentRetentionPeriodString}}",
                ],
                "PropertySelector": "$.Clusters[0].AutomatedSnapshotRetentionPeriod",
                "Service": "redshift",
              },
              "isEnd": true,
              "name": "VerifyCurrentRetentionPeriod",
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "ModifyCluster",
                "AutomatedSnapshotRetentionPeriod": "{{MinRetentionPeriod}}",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "Service": "redshift",
              },
              "name": "ModifyRetentionPeriod",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.Clusters[0].ClusterStatus",
                "Service": "redshift",
              },
              "name": "WaitForClusterAvailability",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "cast_to_string",
                "InputPayload": {
                  "DesiredParameter": "RetentionPeriod",
                  "RetentionPeriod": "{{MinRetentionPeriod}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def cast_to_string(event, _) -> str:
    parameter_to_cast = event["DesiredParameter"]
    return str(event[parameter_to_cast])",
              },
              "name": "CastRetentionPeriodToString",
              "outputs": [
                {
                  "Name": "MinRetentionPeriodString",
                  "Selector": "$.Payload",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "DesiredValues": [
                  "{{CastRetentionPeriodToString.MinRetentionPeriodString}}",
                ],
                "PropertySelector": "$.Clusters[0].AutomatedSnapshotRetentionPeriod",
                "Service": "redshift",
              },
              "isEnd": true,
              "name": "VerifyModifiedRetentionPeriod",
            },
          ],
          "outputs": [
            "QueryRetentionPeriod.CurrentRetentionPeriod",
            "ModifyRetentionPeriod.Response",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ClusterIdentifier": {
              "allowedPattern": "^(?!.*--)[a-z][a-z0-9-]{0,62}(?<!-)$",
              "description": "(Required) The unique identifier of the cluster.",
              "type": "String",
            },
            "MinRetentionPeriod": {
              "default": 7,
              "description": "(Optional) The minimum retention period for the automatic snapshots in days.",
              "type": "Integer",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAutomaticSnapshotsOnRedshiftCluster",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableAutomaticVersionUpgradeOnRedshiftCluster": {
      "DependsOn": [
        "CreateWait3",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{AutomationAssumeRole}}",
          "description": "### Document name - ASR-EnableAutomaticVersionUpgradeOnRedshiftCluster

## What does this document do?
The runbook enables automatic version upgrade on a Redshift cluster.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* ClusterIdentifier: (Required) The unique identifier of the cluster.
* AllowVersionUpgrade: (Optional) Whether to allow version upgrade on the cluster.

## Output Parameters
* EnableAutomaticVersionUpgrade.Response: The response of the API call to enable automatic version upgrade on the cluster.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "AllowVersionUpgrade": "{{AllowVersionUpgrade}}",
                "Api": "ModifyCluster",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "Service": "redshift",
              },
              "name": "EnableAutomaticVersionUpgrade",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.Clusters[0].ClusterStatus",
                "Service": "redshift",
              },
              "name": "WaitForClusterAvailability",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "cast_to_string",
                "InputPayload": {
                  "AllowVersionUpgrade": "{{AllowVersionUpgrade}}",
                  "DesiredParameter": "AllowVersionUpgrade",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
def cast_to_string(event, _) -> str:
    parameter_to_cast = event["DesiredParameter"]
    return str(event[parameter_to_cast])",
              },
              "name": "CastAllowVersionUpgradeToString",
              "outputs": [
                {
                  "Name": "AllowVersionUpgradeString",
                  "Selector": "$.Payload",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "inputs": {
                "Api": "DescribeClusters",
                "ClusterIdentifier": "{{ClusterIdentifier}}",
                "DesiredValues": [
                  "{{CastAllowVersionUpgradeToString.AllowVersionUpgradeString}}",
                ],
                "PropertySelector": "$.Clusters[0].AllowVersionUpgrade",
                "Service": "redshift",
              },
              "isEnd": true,
              "name": "VerifyAutomaticVersionUpgrade",
            },
          ],
          "outputs": [
            "EnableAutomaticVersionUpgrade.Response",
          ],
          "parameters": {
            "AllowVersionUpgrade": {
              "default": true,
              "description": "(Optional) Whether to allow version upgrade on the cluster.",
              "type": "Boolean",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ClusterIdentifier": {
              "allowedPattern": "^(?!.*--)[a-z][a-z0-9-]{0,62}(?<!-)$",
              "description": "(Required) The unique identifier of the cluster.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableAutomaticVersionUpgradeOnRedshiftCluster",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableBucketEventNotifications": {
      "DependsOn": [
        "CreateWait9",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableBucketEventNotifications

## What does this document do?
This document creates an SNS topic if it does not already exist, then configures notifications on an S3 bucket that posts event notifications to that topic.

## Input Parameters
* AccountId: (Required) Account ID of the account for the finding
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* BucketName: (Required) Name of bucket that event notifications will be triggered on.
* TopicName: (Required) The name of the SNS topic to create and configure for notifications.

## Security Standards / Controls
* AWS FSBP v1.0.0:   S3.11
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "account_id": "{{ AccountId }}",
                  "bucket_name": "{{ BucketName }}",
                  "event_types": "{{ EventTypes }}",
                  "topic_name": "{{ TopicName }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
"""
Configure a CloudFormation stack with an SNS topic for notifications, creating the topic if it does
not already exist
"""
import json
from typing import TYPE_CHECKING, List

import boto3
from botocore.config import Config
from botocore.exceptions import UnknownRegionError

if TYPE_CHECKING:
    from mypy_boto3_sns.client import SNSClient
else:
    SNSClient = object

boto_config = Config(retries={"mode": "standard"})


def lambda_handler(event, _):
    """
    Configure a bucket with an SNS topic for notifications,
    creating the topic if it does not already exist

    \`event\` should have the following keys and values:
    \`bucket_name\`: the ARN of the CloudFormation stack to be updated
    \`topic_name\`: the name of the SQS Queue to create and configure for notifications
    \`account_id\`: account id that contains the bucket that will have event notifications configured
    \`event_types\`: the list of events that will have notifications alerted on.

    \`context\` is ignored
    """
    bucket_name = event["bucket_name"]
    topic_name = event["topic_name"]
    account_id = event["account_id"]
    event_types = event["event_types"]
    topic_arn = get_or_create_topic(topic_name, bucket_name, account_id)
    configure_notifications(bucket_name, topic_arn, event_types)
    return assert_bucket_notifcations_configured(bucket_name, account_id)


def partition_from_region(session: boto3.session.Session):
    """
    returns the partition for a given region
    On success returns a string
    On failure returns aws
    """
    try:
        partition = session.get_partition_for_region(session.region_name)
    except UnknownRegionError:
        return "aws"

    return partition


def get_or_create_topic(topic_name: str, bucket_name: str, account_id: str) -> str:
    """Get the SNS topic arn that will be used to configure notifications, creating it if it does not already exist"""
    sns: SNSClient = boto3.client("sns", config=boto_config)
    # get partition and region to buildArn here, replace sourceArn under condition
    session = boto3.session.Session()
    region = session.region_name
    partition = partition_from_region(session)
    expected_topic_arn = f"arn:{partition}:sns:{region}:{account_id}:{topic_name}"
    policy = {
        "Version": "2012-10-17",
        "Id": "ASR Notification Policy",
        "Statement": [
            {
                "Sid": bucket_name + " ASR Notification Policy",
                "Effect": "Allow",
                "Principal": {"Service": "s3.amazonaws.com"},
                "Action": ["SNS:Publish"],
                "Resource": expected_topic_arn,
                "Condition": {
                    "ArnLike": {
                        "aws:SourceArn": [f"arn:{partition}:s3:::" + bucket_name]
                    },
                    "StringEquals": {"aws:SourceAccount": [account_id]},
                },
            }
        ],
    }

    try:
        topic_attributes = sns.get_topic_attributes(TopicArn=expected_topic_arn)
        topic_attributes_policy = topic_attributes["Attributes"]["Policy"]  # str
        topic_attributes_policy_dict = json.loads(topic_attributes_policy)  # dict
        for statement in topic_attributes_policy_dict["Statement"]:
            if statement["Sid"] == bucket_name + " ASR Notification Policy":
                return expected_topic_arn
        topic_attributes_policy_dict["Statement"].append(policy["Statement"][0])
        new_topic_attributes_policy = json.dumps(topic_attributes_policy_dict)
        response = sns.set_topic_attributes(
            TopicArn=expected_topic_arn,
            AttributeName="Policy",
            AttributeValue=new_topic_attributes_policy,
        )
        return expected_topic_arn
    except Exception:
        string_policy = json.dumps(policy)
        response = sns.create_topic(
            Name=topic_name,
            Attributes={"Policy": string_policy},
        )
    return response["TopicArn"]


def configure_notifications(
    bucket_name: str, topic_arn: str, event_types: List[str]
) -> None:
    """Configure the bucket \`bucket_name\` to notify the sns topic with ARN \`topic_arn\`"""
    s3 = boto3.client("s3", config=boto_config)
    s3.put_bucket_notification_configuration(
        Bucket=bucket_name,
        NotificationConfiguration={
            "TopicConfigurations": [
                {
                    "Id": "ASR Bucket Notification Topic Config",
                    "Events": event_types,
                    "TopicArn": topic_arn,
                }
            ]
        },
    )


def assert_bucket_notifcations_configured(bucket_name, account_id):
    """
    Verify that the bucket \`bucket_name\` is configured to update the SNS topic
    with ARN \`topic_arn\`
    """
    s3 = boto3.client("s3", config=boto_config)
    notification_configuration = s3.get_bucket_notification_configuration(
        Bucket=bucket_name, ExpectedBucketOwner=account_id
    )
    try:
        return {
            "NotificationARNs": notification_configuration["TopicConfigurations"][0][
                "TopicArn"
            ]
        }
    except Exception:
        raise RuntimeError(
            f"ERROR: {bucket_name} was not configured with notifications"
        )",
              },
              "isEnd": true,
              "name": "EnableBucketEventNotifications",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableBucketEventNotifications.Output",
          ],
          "parameters": {
            "AccountId": {
              "allowedPattern": "^[0-9]{12}$",
              "description": "Account ID of the account for the finding",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "(Required) The name of the S3 Bucket.",
              "type": "String",
            },
            "EventTypes": {
              "default": [
                "s3:ReducedRedundancyLostObject",
                "s3:ObjectCreated:*",
                "s3:ObjectRemoved:*",
                "s3:ObjectRestore:*",
                "s3:Replication:*",
                "s3:LifecycleExpiration:*",
                "s3:LifecycleTransition",
                "s3:IntelligentTiering",
                "s3:ObjectTagging:*",
                "s3:ObjectAcl:Put",
              ],
              "description": "(Optional) The event types to add notifications for.",
              "type": "StringList",
            },
            "TopicName": {
              "allowedPattern": "^[a-zA-Z0-9][a-zA-Z0-9-_]{0,255}$",
              "default": "SO0111-ASR-S3BucketNotifications",
              "description": "(Optional) The name of the SNS topic to create and configure for notifications.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableBucketEventNotifications",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableCloudFrontDefaultRootObject": {
      "DependsOn": [
        "CreateWait8",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-EnableCloudFrontDefaultRootObject

## What does this document do?
This runbook configures the default root object for the Amazon CloudFront distribution you specify using the [UpdateDistribution](https://docs.aws.amazon.com/cloudfront/latest/APIReference/API_UpdateDistribution.html) API.

## Input Parameters
* CloudFrontDistribution: (Required) The ARN of the CloudFront distribution you want to configure the default root object for.
* DefaultRootObject: (Required) The object that you want CloudFront to return when a viewer request points to your root URL.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* UpdateDistributionAndVerify.Output: The standard HTTP response from the UpdateDistribution API.",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## UpdateDistributionAndVerify
Configures the default root object for the CloudFront distribution you specify in the CloudFrontDistributionId parameter and verifies it's successful modification.
## outputs
* Output: The standard HTTP response from the UpdateDistribution API.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "cloudfront_distribution": "{{ CloudFrontDistribution }}",
                  "root_object": "{{ DefaultRootObject }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import datetime
import json

import boto3


def default(obj):
    if isinstance(obj, (datetime.date, datetime.datetime)):
        return obj.isoformat()
    else:
        raise TypeError("Incorrect HTTPResponse format.")


def verify_enable_cloudfront_default_root_object(
    cloudfront_client, cloudfront_distribution
):
    response = cloudfront_client.get_distribution_config(Id=cloudfront_distribution)
    if response["DistributionConfig"]["DefaultRootObject"]:
        return "Verification of 'EnableCloudFrontDefaultRootObject' is successful."
    error = f"VERIFICATION FAILED. DEFAULT ROOT OBJECT FOR AMAZON CLOUDFRONT DISTRIBUTION {cloudfront_distribution} IS NOT SET."
    raise RuntimeError(error)


def handler(event, _):
    cloudfront_client = boto3.client("cloudfront")
    cloudfront_distribution_arn = event["cloudfront_distribution"]
    cloudfront_distribution_id = cloudfront_distribution_arn.split("/")[1]
    response = cloudfront_client.get_distribution_config(Id=cloudfront_distribution_id)
    response["DistributionConfig"]["DefaultRootObject"] = event["root_object"]
    update_response = cloudfront_client.update_distribution(
        DistributionConfig=response["DistributionConfig"],
        Id=cloudfront_distribution_id,
        IfMatch=response["ETag"],
    )
    output = verify_enable_cloudfront_default_root_object(
        cloudfront_client, cloudfront_distribution_id
    )
    return {
        "Output": {
            "Message": output,
            "HTTPResponse": json.dumps(update_response, default=default),
        }
    }",
              },
              "isEnd": true,
              "name": "UpdateDistributionAndVerify",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.Output",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "UpdateDistributionAndVerify.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "CloudFrontDistribution": {
              "allowedPattern": "^(arn:(?:aws|aws-us-gov|aws-cn):cloudfront::\\d{12}:distribution\\/([A-Z0-9]+))$",
              "description": "(Required) The ID of the CloudFront distribution you want to configure the default root object for.",
              "type": "String",
            },
            "DefaultRootObject": {
              "allowedPattern": "^[\\w._-~]{1,255}$",
              "default": "index.html",
              "description": "(Required) The object that you want CloudFront to return when a viewer request points to your root URL.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableCloudFrontDefaultRootObject",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableCloudTrailEncryption": {
      "DependsOn": [
        "CreateWait1",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableCloudTrailEncryption
## What does this document do?
Enables encryption on a CloudTrail using the provided KMS CMK

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* KMSKeyArn (from SSM): Arn of the KMS key to be used to encrypt data
* TrailRegion: region of the CloudTrail to encrypt
* TrailArn: ARN of the CloudTrail to encrypt

## Security Standards / Controls
* AWS FSBP v1.0.0:   CloudTrail.2
* CIS v1.2.0:     2.7
* PCI:            CloudTrail.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "enable_trail_encryption",
                "InputPayload": {
                  "exec_region": "{{global:REGION}}",
                  "kms_key_arn": "{{KMSKeyArn}}",
                  "region": "{{global:REGION}}",
                  "trail": "{{TrailArn}}",
                  "trail_region": "{{TrailRegion}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config


def connect_to_cloudtrail(region, boto_config):
    return boto3.client("cloudtrail", region_name=region, config=boto_config)


def enable_trail_encryption(event, _):
    """
    remediates CloudTrail.2 by enabling SSE-KMS
    On success returns a string map
    On failure returns NoneType
    """
    boto_config = Config(retries={"mode": "standard"})

    if event["trail_region"] != event["exec_region"]:
        exit("ERROR: cross-region remediation is not yet supported")

    ctrail_client = connect_to_cloudtrail(event["trail_region"], boto_config)
    kms_key_arn = event["kms_key_arn"]

    try:
        ctrail_client.update_trail(Name=event["trail"], KmsKeyId=kms_key_arn)
        return {
            "response": {
                "message": f'Enabled KMS CMK encryption on {event["trail"]}',
                "status": "Success",
            }
        }
    except Exception as e:
        exit(f"Error enabling SSE-KMS encryption: {str(e)}")",
              },
              "isEnd": true,
              "name": "Remediation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "Remediation.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "KMSKeyArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "default": "{{ssm:/Solutions/SO0111/CMK_REMEDIATION_ARN}}",
              "description": "The ARN of the KMS key created by ASR for this remediation",
              "type": "String",
            },
            "TrailArn": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):cloudtrail:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:trail/[A-Za-z0-9._-]{3,128}$",
              "description": "ARN of the CloudTrail",
              "type": "String",
            },
            "TrailRegion": {
              "allowedPattern": "^[a-z]{2}(?:-gov)?-[a-z]+-\\d$",
              "description": "Region the CloudTrail is in",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableCloudTrailEncryption",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableCloudTrailLogFileValidation": {
      "DependsOn": [
        "CreateWait5",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-EnableCloudTrailLogFileValidation

## What does this document do?
This runbook enables log file validation for your AWS CloudTrail trail using the [UpdateTrail](https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/API_UpdateTrail.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* TrailName: (Required) The name or Amazon Resource Name (ARN) of the trail you want to enable log file validation for.

## Output Parameters
* UpdateTrail.Output: The response of the UpdateTrail API call.

## Note: this is a local copy of the AWS-owned document to enable support in aws-cn and aws-us-gov partitions.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## UpdateTrail
Enables log file validation for the AWS CloudTrail trail you specify in the TrailName parameter.
## Outputs
* Output: Response from the UpdateTrail API call.
",
              "inputs": {
                "Api": "UpdateTrail",
                "EnableLogFileValidation": true,
                "Name": "{{ TrailName }}",
                "Service": "cloudtrail",
              },
              "isEnd": false,
              "name": "UpdateTrail",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyTrail
Verifies log file validation is enabled for your trail.
",
              "inputs": {
                "Api": "GetTrail",
                "DesiredValues": [
                  "True",
                ],
                "Name": "{{ TrailName }}",
                "PropertySelector": "$.Trail.LogFileValidationEnabled",
                "Service": "cloudtrail",
              },
              "isEnd": true,
              "name": "VerifyTrail",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "UpdateTrail.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "TrailName": {
              "allowedPattern": "(^arn:(aws[a-zA-Z-]*)?:cloudtrail:[a-z0-9-]+:\\d{12}:trail\\/(?![-_.])(?!.*[-_.]{2})(?!.*[-_.]$)(?!^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$)[-\\w.]{3,128}$)|(^(?![-_.])(?!.*[-_.]{2})(?!.*[-_.]$)(?!^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$)[-\\w.]{3,128}$)",
              "description": "(Required) The name or Amazon Resource Name (ARN) of the trail you want to enable log file validation for.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableCloudTrailLogFileValidation",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableCloudTrailToCloudWatchLogging": {
      "DependsOn": [
        "CreateWait0",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableCloudTrailToCloudWatchLogging
## What does this document do?
Enables CloudTrail logging to CloudWatch Logs by:
1. Creating or reusing an existing CloudWatch log group
2. Fixing S3 bucket policy with required SourceArn condition for security
3. Updating the CloudTrail to send logs to CloudWatch

This fixes the common "InsufficientS3BucketPolicyException" error by ensuring
the S3 bucket policy includes the AWS:SourceArn condition required by AWS.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* TrailName: (Required) The name of the CloudTrail to enable logging for.
* CloudWatchLogsRole: (Required) The ARN of the role that allows CloudTrail to log to CloudWatch.
* LogGroupName: (Required) The name of the CloudWatch Log Group for CloudTrail logs.

## Security Standards / Controls
* AWS FSBP v1.0.0:   N/A
* CIS v1.2.0:        2.4
* PCI:               CloudTrail.4
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "Create log group if it doesn't exist, or get existing log group ARN",
              "inputs": {
                "Handler": "create_or_get_loggroup",
                "InputPayload": {
                  "LogGroup": "{{LogGroupName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import time

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


class LogGroupOperationError(Exception):
    pass


class LogGroupNotFoundError(LogGroupOperationError):
    pass


def connect_to_logs(boto_config):
    return boto3.client("logs", config=boto_config)


def sleep_between_attempts():
    time.sleep(2)


def _find_existing_log_group(cwl_client, log_group_name):
    try:
        describe_group = cwl_client.describe_log_groups(
            logGroupNamePrefix=log_group_name
        )
        for group in describe_group["logGroups"]:
            if group["logGroupName"] == log_group_name:
                print(
                    f"Log group '{log_group_name}' already exists with ARN: {group['arn']}"
                )
                return str(group["arn"])
    except ClientError as err:
        print(f"Error checking for existing log groups: {str(err)}")
    return None


def _create_log_group(cwl_client, log_group_name):
    try:
        print(f"Attempting to create log group '{log_group_name}'...")
        cwl_client.create_log_group(logGroupName=log_group_name)
        print(f"Successfully created log group '{log_group_name}'")
    except ClientError as e:
        error_code = e.response["Error"]["Code"]
        if error_code == "ResourceAlreadyExistsException":
            print(
                f"Log group '{log_group_name}' was created by another process, continuing..."
            )
        else:
            print(f"Error creating log group: {error_code} - {str(e)}")
            raise e
    except Exception as e:
        print(f"Unexpected error creating log group: {str(e)}")
        raise LogGroupOperationError(
            f"Failed to create log group {log_group_name}: {str(e)}"
        ) from e


def _wait_for_log_group_creation(cwl_client, log_group_name, max_retries=3):
    attempts = 0
    while attempts < max_retries:
        try:
            describe_group = cwl_client.describe_log_groups(
                logGroupNamePrefix=log_group_name
            )
            print(f"Found {len(describe_group['logGroups'])} log groups")
            for group in describe_group["logGroups"]:
                if group["logGroupName"] == log_group_name:
                    return str(group["arn"])

            # Log group not found yet, wait and retry
            sleep_between_attempts()
            attempts += 1

        except ClientError as err:
            print(f"Error describing log groups: {str(err)}")
            if attempts >= max_retries - 1:
                raise LogGroupNotFoundError(
                    f"Failed to find Log Group {log_group_name}: {str(err)}"
                )
            sleep_between_attempts()
            attempts += 1

    raise LogGroupNotFoundError(
        f"Failed to find Log Group {log_group_name}: Timed out after {max_retries} attempts"
    )


def create_or_get_loggroup(event, _):
    boto_config = Config(retries={"mode": "standard"})
    cwl_client = connect_to_logs(boto_config)
    log_group_name = event["LogGroup"]

    existing_arn = _find_existing_log_group(cwl_client, log_group_name)
    if existing_arn:
        return existing_arn

    _create_log_group(cwl_client, log_group_name)

    return _wait_for_log_group_creation(cwl_client, log_group_name)


# Keep the old function for backward compatibility
def wait_for_loggroup(event, _):
    return create_or_get_loggroup(event, _)",
              },
              "name": "CreateOrGetLogGroup",
              "outputs": [
                {
                  "Name": "CloudWatchLogsGroupArn",
                  "Selector": "$.Payload",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "description": "Fix S3 bucket policy to allow CloudTrail access with SourceArn condition",
              "inputs": {
                "Handler": "fix_cloudtrail_bucket_policy_for_logging",
                "InputPayload": {
                  "account": "{{global:ACCOUNT_ID}}",
                  "partition": "{{global:AWS_PARTITION}}",
                  "trail_name": "{{TrailName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json
import time
from typing import Any, Dict

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


def create_cloudtrail_bucket_policy(bucket, trail_arn, partition, account):
    return {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "AWSCloudTrailAclCheck20150319",
                "Effect": "Allow",
                "Principal": {"Service": "cloudtrail.amazonaws.com"},
                "Action": "s3:GetBucketAcl",
                "Resource": f"arn:{partition}:s3:::{bucket}",
                "Condition": {"StringEquals": {"AWS:SourceArn": trail_arn}},
            },
            {
                "Sid": "AWSCloudTrailWrite20150319",
                "Effect": "Allow",
                "Principal": {"Service": "cloudtrail.amazonaws.com"},
                "Action": "s3:PutObject",
                "Resource": f"arn:{partition}:s3:::{bucket}/AWSLogs/{account}/*",
                "Condition": {
                    "StringEquals": {
                        "s3:x-amz-acl": "bucket-owner-full-control",
                        "AWS:SourceArn": trail_arn,
                    }
                },
            },
            {
                "Sid": "AllowSSLRequestsOnly",
                "Effect": "Deny",
                "Principal": "*",
                "Action": "s3:*",
                "Resource": [
                    f"arn:{partition}:s3:::{bucket}",
                    f"arn:{partition}:s3:::{bucket}/*",
                ],
                "Condition": {"Bool": {"aws:SecureTransport": "false"}},
            },
        ],
    }


def merge_bucket_policies(
    existing: Dict[str, Any], new: Dict[str, Any]
) -> Dict[str, Any]:
    if not existing or "Statement" not in existing:
        return new

    merged = existing.copy()
    existing_sids = {stmt.get("Sid") for stmt in merged.get("Statement", [])}

    for stmt in new.get("Statement", []):
        if stmt.get("Sid") not in existing_sids:
            merged["Statement"].append(stmt)
        else:
            for i, existing_stmt in enumerate(merged["Statement"]):
                if existing_stmt.get("Sid") == stmt.get("Sid"):
                    merged["Statement"][i] = stmt
                    break
    return merged


def fix_cloudtrail_bucket_policy_for_logging(event, _):
    boto_config = Config(retries={"mode": "standard", "max_attempts": 5})
    s3 = boto3.client("s3", config=boto_config)
    cloudtrail = boto3.client("cloudtrail", config=boto_config)

    trail_name = event["trail_name"]
    aws_partition = event["partition"]
    aws_account = event["account"]

    try:
        trail_response = cloudtrail.get_trail(Name=trail_name)
        trail = trail_response["Trail"]
        bucket = trail["S3BucketName"]
        trail_arn = trail["TrailARN"]

        if not bucket or not trail_arn:
            raise ValueError(f"Trail {trail_name} missing S3 bucket or ARN")

        existing_policy = None
        try:
            policy_response = s3.get_bucket_policy(Bucket=bucket)
            existing_policy = json.loads(policy_response["Policy"])
        except ClientError as e:
            if e.response["Error"]["Code"] != "NoSuchBucketPolicy":
                raise

        cloudtrail_policy = create_cloudtrail_bucket_policy(
            bucket, trail_arn, aws_partition, aws_account
        )
        final_policy = (
            merge_bucket_policies(existing_policy, cloudtrail_policy)
            if existing_policy
            else cloudtrail_policy
        )

        for attempt in range(3):
            try:
                s3.put_bucket_policy(Bucket=bucket, Policy=json.dumps(final_policy))
                break
            except ClientError as e:
                if attempt < 2 and e.response["Error"]["Code"] in [
                    "ServiceUnavailable",
                    "SlowDown",
                ]:
                    time.sleep(2**attempt)
                    continue
                raise

        return {
            "output": {
                "Message": f"Fixed bucket policy for {bucket}",
                "TrailArn": trail_arn,
                "BucketName": bucket,
                "PolicyMerged": existing_policy is not None,
            }
        }

    except Exception as e:
        return {
            "output": {
                "Message": f"Error fixing CloudTrail bucket policy: {str(e)}",
                "Error": str(e),
                "TrailName": trail_name,
            }
        }",
              },
              "name": "FixCloudTrailBucketPolicy",
              "outputs": [
                {
                  "Name": "BucketPolicyResult",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "description": "Validate that S3 bucket policy is correctly configured for CloudTrail",
              "inputs": {
                "Handler": "validate_cloudtrail_bucket_policy",
                "InputPayload": {
                  "account": "{{global:ACCOUNT_ID}}",
                  "partition": "{{global:AWS_PARTITION}}",
                  "trail_name": "{{TrailName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json
import time
from typing import Any, Dict

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


def _validate_policy_statement(
    stmt: Dict[str, Any], trail_arn: str, checks: Dict[str, bool]
) -> None:
    sid = stmt.get("Sid", "")

    if (
        "AWSCloudTrailAclCheck" in sid
        and stmt.get("Effect") == "Allow"
        and stmt.get("Principal", {}).get("Service") == "cloudtrail.amazonaws.com"
        and "s3:GetBucketAcl" in stmt.get("Action", [])
        and stmt.get("Condition", {}).get("StringEquals", {}).get("AWS:SourceArn")
        == trail_arn
    ):
        checks["acl_check"] = True
    elif (
        "AWSCloudTrailWrite" in sid
        and stmt.get("Effect") == "Allow"
        and stmt.get("Principal", {}).get("Service") == "cloudtrail.amazonaws.com"
        and "s3:PutObject" in stmt.get("Action", [])
        and stmt.get("Condition", {}).get("StringEquals", {}).get("AWS:SourceArn")
        == trail_arn
    ):
        checks["put_object"] = True
    elif (
        "AllowSSLRequestsOnly" in sid
        and stmt.get("Effect") == "Deny"
        and stmt.get("Condition", {}).get("Bool", {}).get("aws:SecureTransport")
        == "false"
    ):
        checks["ssl_only"] = True


def _get_missing_statements(checks: Dict[str, bool]) -> list:
    missing = []
    if not checks["acl_check"]:
        missing.append("CloudTrail ACL check statement")
    if not checks["put_object"]:
        missing.append("CloudTrail PutObject statement")
    if not checks["ssl_only"]:
        missing.append("SSL-only enforcement statement")
    return missing


def validate_cloudtrail_policy_statements(
    policy: Dict[str, Any], trail_arn: str
) -> Dict[str, Any]:
    result = {"valid": False, "missing_statements": [], "issues": []}

    if not policy or "Statement" not in policy:
        result["issues"].append("No policy statements found")
        return result

    checks = {"acl_check": False, "put_object": False, "ssl_only": False}

    for stmt in policy["Statement"]:
        _validate_policy_statement(stmt, trail_arn, checks)

    result["missing_statements"] = _get_missing_statements(checks)
    result["valid"] = all(checks.values())
    return result


def validate_cloudtrail_bucket_policy(event, _):
    boto_config = Config(retries={"mode": "standard", "max_attempts": 3})
    s3 = boto3.client("s3", config=boto_config)
    cloudtrail = boto3.client("cloudtrail", config=boto_config)

    trail_name = event["trail_name"]

    try:
        trail_response = cloudtrail.get_trail(Name=trail_name)
        trail = trail_response["Trail"]
        bucket = trail["S3BucketName"]
        trail_arn = trail["TrailARN"]

        if not bucket or not trail_arn:
            raise ValueError(f"Trail {trail_name} missing S3 bucket or ARN")

        current_policy = None
        for attempt in range(3):
            try:
                policy_response = s3.get_bucket_policy(Bucket=bucket)
                current_policy = json.loads(policy_response["Policy"])
                break
            except ClientError as e:
                if e.response["Error"]["Code"] == "NoSuchBucketPolicy":
                    return {
                        "output": {
                            "Valid": False,
                            "Message": f"No bucket policy found for {bucket}",
                            "BucketName": bucket,
                            "TrailArn": trail_arn,
                        }
                    }
                elif attempt < 2:
                    time.sleep(2)
                    continue
                else:
                    raise

        validation_result = validate_cloudtrail_policy_statements(
            current_policy, trail_arn
        )
        message = (
            "Bucket policy validation passed"
            if validation_result["valid"]
            else "Bucket policy validation failed"
        )

        if not validation_result["valid"]:
            message += (
                f". Missing: {', '.join(validation_result['missing_statements'])}"
            )
        if validation_result["issues"]:
            message += f". Issues: {', '.join(validation_result['issues'])}"

        return {
            "output": {
                "Valid": validation_result["valid"],
                "Message": message,
                "BucketName": bucket,
                "TrailArn": trail_arn,
                "MissingStatements": validation_result["missing_statements"],
                "Issues": validation_result["issues"],
            }
        }

    except Exception as e:
        return {
            "output": {
                "Valid": False,
                "Message": f"Error validating CloudTrail bucket policy: {str(e)}",
                "Error": str(e),
                "TrailName": trail_name,
            }
        }",
              },
              "name": "ValidateBucketPolicy",
              "outputs": [
                {
                  "Name": "ValidationResult",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "description": "Enable logging to CloudWatch Logs with enhanced error handling",
              "inputs": {
                "Handler": "update_trail_with_error_handling",
                "InputPayload": {
                  "cloudwatch_role_arn": "{{CloudWatchLogsRole}}",
                  "log_group_arn": "{{CreateOrGetLogGroup.CloudWatchLogsGroupArn}}",
                  "trail_name": "{{TrailName}}",
                  "validation_result": "{{ValidateBucketPolicy.ValidationResult}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import time

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


def update_trail_with_error_handling(event, _):
    boto_config = Config(retries={"mode": "standard", "max_attempts": 5})
    cloudtrail = boto3.client("cloudtrail", config=boto_config)

    trail_name = event["trail_name"]
    log_group_arn = event["log_group_arn"]
    cloudwatch_role_arn = event["cloudwatch_role_arn"]
    validation_result = event.get("validation_result", {})

    try:
        last_error = None
        for attempt in range(3):
            try:
                response = cloudtrail.update_trail(
                    Name=trail_name,
                    CloudWatchLogsLogGroupArn=log_group_arn,
                    CloudWatchLogsRoleArn=cloudwatch_role_arn,
                )

                return {
                    "output": {
                        "Message": f"Successfully enabled CloudWatch logging for CloudTrail: {trail_name}",
                        "TrailName": trail_name,
                        "LogGroupArn": log_group_arn,
                        "CloudWatchRoleArn": cloudwatch_role_arn,
                        "TrailArn": response.get("TrailARN"),
                        "Success": True,
                        "ValidationWarning": not validation_result.get("Valid", True),
                    }
                }

            except ClientError as e:
                last_error = e
                error_code = e.response["Error"]["Code"]

                if error_code == "InsufficientS3BucketPolicyException" and attempt < 2:
                    time.sleep(5 * (attempt + 1))
                    continue
                elif error_code in [
                    "InvalidCloudWatchLogsLogGroupArnException",
                    "InvalidCloudWatchLogsRoleArnException",
                ]:
                    break
                elif attempt < 2 and error_code in ["ServiceUnavailable", "Throttling"]:
                    time.sleep(2**attempt)
                    continue
                else:
                    break

        return {
            "output": {
                "Message": "Failed to update CloudTrail after 3 attempts",
                "TrailName": trail_name,
                "LastError": str(last_error) if last_error else "Unknown error",
                "Success": False,
                "ValidationResult": validation_result,
            }
        }

    except Exception as e:
        return {
            "output": {
                "Message": f"Unexpected error updating CloudTrail: {str(e)}",
                "TrailName": trail_name,
                "Error": str(e),
                "Success": False,
            }
        }",
              },
              "name": "UpdateTrailToCWLogs",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "UpdateTrailToCWLogs.Output",
            "FixCloudTrailBucketPolicy.BucketPolicyResult",
            "ValidateBucketPolicy.ValidationResult",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "CloudWatchLogsRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows CloudTrail to log to CloudWatch.",
              "type": "String",
            },
            "LogGroupName": {
              "allowedPattern": "^[a-zA-Z0-9-_./]{1,512}$",
              "description": "(Required) The name of the Log Group for CloudTrail logs.",
              "type": "String",
            },
            "TrailName": {
              "allowedPattern": "^[A-Za-z0-9._-]{3,128}$",
              "description": "(Required) The name of the CloudTrail.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableCloudTrailToCloudWatchLogging",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableCopyTagsToSnapshotOnRDSCluster": {
      "DependsOn": [
        "CreateWait6",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-EnableCopyTagsToSnapshotOnRDSCluster

## What does this document do?
The document enables CopyTagsToSnapshot on an Amazon RDS cluster using the [ModifyDBCluster API](https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ModifyDBCluster.html).  Please note, AWS Config is required to be enabled in this region for this document to work as it requires the Resource ID recorded by the AWS Config service.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* RDSClusterARN: (Required) ARN of the Amazon RDS Cluster for which CopyTagsToSnapshot needs to be enabled.
* ApplyImmediately: (Optional) A value that indicates whether the modifications in this request and any pending modifications are asynchronously applied as soon as possible, regardless of the PreferredMaintenanceWindow setting for the DB instance. By default, this parameter is disabled.
  * Default: false

## Output Parameters
* ModifyDBClusterResponse.Output: The response of the ModifyDBCluster API call.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## GetRDSInstanceIdentifier
Makes DescribeDBClusters API call using Amazon RDS Instance ARN to get DbClusterIdentifier.
## Outputs
* DbClusterIdentifier: Identifier of the Amazon RDS Cluster.
",
              "inputs": {
                "Api": "DescribeDBClusters",
                "DBClusterIdentifier": "{{ RDSClusterARN }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "DescribeDBClusters",
              "outputs": [
                {
                  "Name": "DbClusterIdentifier",
                  "Selector": "$.DBClusters[0].DBClusterIdentifier",
                  "Type": "String",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyStatus
Verifies if \`Status\` is available before proeeding to the next step.
",
              "inputs": {
                "Api": "DescribeDBClusters",
                "DBClusterIdentifier": "{{ RDSClusterARN }}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.DBClusters[0].Status",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "VerifyStatus",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableCopyTagsToSnapshot
Accepts the cluster name as input and modifies it to set true for \`CopyTagsToSnapshot\`.
## Outputs
* Output: Response from the ModifyDBCluster API call.
",
              "inputs": {
                "Api": "ModifyDBCluster",
                "ApplyImmediately": "{{ ApplyImmediately }}",
                "CopyTagsToSnapshot": true,
                "DBClusterIdentifier": "{{ DescribeDBClusters.DbClusterIdentifier }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "EnableCopyTagsToSnapshot",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyDBClusterCopyTagsToSnapshotEnabled
Verifies that \`CopyTagsToSnapshot\` has been enabled on the target resource.
## Outputs
* Output: A success message or failure exception.
",
              "inputs": {
                "Api": "DescribeDBClusters",
                "DBClusterIdentifier": "{{ DescribeDBClusters.DbClusterIdentifier }}",
                "DesiredValues": [
                  "True",
                ],
                "PropertySelector": "$.DBClusters[0].CopyTagsToSnapshot",
                "Service": "rds",
              },
              "isEnd": true,
              "name": "VerifyDBClusterCopyTagsToSnapshotEnabled",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableCopyTagsToSnapshot.Output",
          ],
          "parameters": {
            "ApplyImmediately": {
              "default": false,
              "description": "(Optional) A value that indicates whether the modifications in this request and any pending modifications are asynchronously applied as soon as possible, regardless of the PreferredMaintenanceWindow setting for the DB instance.  By default, this parameter is disabled.",
              "type": "Boolean",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RDSClusterARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):rds:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:cluster:.+$",
              "description": "(Required) Resource ID of the Amazon RDS Cluster for which CopyTagsToSnapshot needs to be enabled.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableCopyTagsToSnapshotOnRDSCluster",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableDefaultEncryptionS3": {
      "DependsOn": [
        "CreateWait1",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableDefaultEncryptionS3

## What does this document do?
This document configures default encryption for an Amazon S3 Bucket.

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* BucketName: (Required) Name of the bucket to modify.
* AccountId: (Required) Account to which the bucket belongs

## Output Parameters

* Remediation.Output - stdout messages from the remediation

## Security Standards / Controls
* AWS FSBP v1.0.0: S3.4
* CIS v1.2.0:      n/a
* PCI:             S3.4
",
          "mainSteps": [
            {
              "action": "aws:branch",
              "inputs": {
                "Choices": [
                  {
                    "NextStep": "EncryptWithAES",
                    "StringEquals": "default-s3-encryption",
                    "Variable": "{{KmsKeyAlias}}",
                  },
                ],
                "Default": "EncryptWithCMK",
              },
              "name": "ChooseEncryptionMethod",
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "PutBucketEncryption",
                "Bucket": "{{BucketName}}",
                "ExpectedBucketOwner": "{{AccountId}}",
                "ServerSideEncryptionConfiguration": {
                  "Rules": [
                    {
                      "ApplyServerSideEncryptionByDefault": {
                        "SSEAlgorithm": "AES256",
                      },
                      "BucketKeyEnabled": true,
                    },
                  ],
                },
                "Service": "s3",
              },
              "isEnd": true,
              "name": "EncryptWithAES",
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "PutBucketEncryption",
                "Bucket": "{{BucketName}}",
                "ExpectedBucketOwner": "{{AccountId}}",
                "ServerSideEncryptionConfiguration": {
                  "Rules": [
                    {
                      "ApplyServerSideEncryptionByDefault": {
                        "KMSMasterKeyID": "{{KmsKeyAlias}}",
                        "SSEAlgorithm": "aws:kms",
                      },
                      "BucketKeyEnabled": true,
                    },
                  ],
                },
                "Service": "s3",
              },
              "isEnd": true,
              "name": "EncryptWithCMK",
            },
          ],
          "parameters": {
            "AccountId": {
              "allowedPattern": "^[0-9]{12}$",
              "description": "Account ID of the account for the finding",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "Name of the bucket to have a policy added",
              "type": "String",
            },
            "KmsKeyAlias": {
              "allowedPattern": "^$|^[a-zA-Z0-9/_-]{1,256}$",
              "default": "default-s3-encryption",
              "description": "(Required) KMS Customer-Managed Key (CMK) alias or the default value which is created in the SSM parameter at solution deployment (default-s3-encryption) is used to identify that the s3 bucket encryption value should be set to AES-256.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableDefaultEncryptionS3",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableDeliveryStatusLoggingForSNSTopic": {
      "DependsOn": [
        "CreateWait8",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableDeliveryStatusLoggingForSNSTopic

## What does this document do?
 This document enables delivery status logging on given Amazon Simple Notification Service (Amazon SNS) topic using
 [SetTopicAttributes](https://docs.aws.amazon.com/sns/latest/api/API_SetTopicAttributes.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* LoggingRole: (Required) The ARN of the IAM Role used to log successful and failed deliveries.
* SNSTopicArn: (Required)  The ARN of the Amazon SNS Topic.

 ## Security Standards / Controls
 * AWS FSBP v1.0.0:   SNS.2
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "logging_role": "{{ LoggingRole }}",
                  "sample_rate": "{{ SampleRate }}",
                  "topic_arn": "{{ SNSTopicArn }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})

endpointTypes = ["HTTP", "Firehose", "Lambda", "Application", "SQS"]


def connect_to_sns():
    return boto3.client("sns", config=boto_config)


def lambda_handler(event, _):
    """
    Enable delivery status logging on a SNS topic

    \`event\` should have the following keys and values:
    \`logging_role\`: the ARN of the IAM Role used to log successful and failed deliveries
    \`topic_arn\`: the arn of the SNS Topic to enable delivery status logging on

    \`context\` is ignored
    """

    logging_role = event["logging_role"]
    topic_arn = event["topic_arn"]
    sample_rate = event["sample_rate"]

    add_roles_to_topic(logging_role, topic_arn)
    add_sample_rate_to_topic(topic_arn, sample_rate)

    topic_attributes = get_topic_attributes(topic_arn)

    return {
        "HTTPFailureFeedbackRoleArn": topic_attributes["Attributes"][
            "HTTPFailureFeedbackRoleArn"
        ],
        "HTTPSuccessFeedbackRoleArn": topic_attributes["Attributes"][
            "HTTPSuccessFeedbackRoleArn"
        ],
        "HTTPSuccessFeedbackSampleRate": topic_attributes["Attributes"][
            "HTTPSuccessFeedbackSampleRate"
        ],
        "FirehoseFailureFeedbackRoleArn": topic_attributes["Attributes"][
            "FirehoseFailureFeedbackRoleArn"
        ],
        "FirehoseSuccessFeedbackRoleArn": topic_attributes["Attributes"][
            "FirehoseSuccessFeedbackRoleArn"
        ],
        "FirehoseSuccessFeedbackSampleRate": topic_attributes["Attributes"][
            "FirehoseSuccessFeedbackSampleRate"
        ],
        "LambdaFailureFeedbackRoleArn": topic_attributes["Attributes"][
            "LambdaFailureFeedbackRoleArn"
        ],
        "LambdaSuccessFeedbackRoleArn": topic_attributes["Attributes"][
            "LambdaSuccessFeedbackRoleArn"
        ],
        "LambdaSuccessFeedbackSampleRate": topic_attributes["Attributes"][
            "LambdaSuccessFeedbackSampleRate"
        ],
        "ApplicationFailureFeedbackRoleArn": topic_attributes["Attributes"][
            "ApplicationFailureFeedbackRoleArn"
        ],
        "ApplicationSuccessFeedbackRoleArn": topic_attributes["Attributes"][
            "ApplicationSuccessFeedbackRoleArn"
        ],
        "ApplicationSuccessFeedbackSampleRate": topic_attributes["Attributes"][
            "ApplicationSuccessFeedbackSampleRate"
        ],
        "SQSFailureFeedbackRoleArn": topic_attributes["Attributes"][
            "SQSFailureFeedbackRoleArn"
        ],
        "SQSSuccessFeedbackRoleArn": topic_attributes["Attributes"][
            "SQSSuccessFeedbackRoleArn"
        ],
        "SQSSuccessFeedbackSampleRate": topic_attributes["Attributes"][
            "SQSSuccessFeedbackSampleRate"
        ],
    }


def add_roles_to_topic(logging_role, topic_arn):
    """
    Configures the IAM role \`logging_role\` that will log successful and failed deliveries to SNS Topic \`topic_arn\`
    """
    sns = connect_to_sns()
    try:
        for endpoint in endpointTypes:
            sns.set_topic_attributes(
                TopicArn=topic_arn,
                AttributeName=f"{endpoint}SuccessFeedbackRoleArn",
                AttributeValue=logging_role,
            )
            sns.set_topic_attributes(
                TopicArn=topic_arn,
                AttributeName=f"{endpoint}FailureFeedbackRoleArn",
                AttributeValue=logging_role,
            )

    except Exception as e:
        reset_to_recognized_state(topic_arn)
        exit(f"Failed to set success/failure role of topic {topic_arn}: {str(e)}")


def add_sample_rate_to_topic(topic_arn, sample_rate):
    """
    Configures the Success sample rate, the percentage of successful messages for which you want to receive CloudWatch Logs.
    """
    sns = connect_to_sns()
    try:
        for endpoint in endpointTypes:
            sns.set_topic_attributes(
                TopicArn=topic_arn,
                AttributeName=f"{endpoint}SuccessFeedbackSampleRate",
                AttributeValue=sample_rate,
            )

    except Exception as e:
        reset_to_recognized_state(topic_arn)
        exit(f"Failed to set success sample rate of SNS topic {topic_arn}: {str(e)}")


def get_topic_attributes(topic_arn):
    """
    Grabs Topic Attributes to verify topic values were set as expected.
    """
    sns = connect_to_sns()
    try:
        topic_attributes = sns.get_topic_attributes(TopicArn=topic_arn)
        return topic_attributes

    except Exception as e:
        exit(f"Failed to get attributes of SNS topic {topic_arn}: {str(e)}")


def reset_to_recognized_state(topic_arn):
    """
    Used in case of error, will unset all delivery status logging parameters.
    """
    sns = connect_to_sns()
    for endpoint in endpointTypes:
        try:
            sns.set_topic_attributes(
                TopicArn=topic_arn,
                AttributeName=f"{endpoint}SuccessFeedbackRoleArn",
                AttributeValue="",
            )
            sns.set_topic_attributes(
                TopicArn=topic_arn,
                AttributeName=f"{endpoint}FailureFeedbackRoleArn",
                AttributeValue="",
            )
        except Exception:
            print(
                f"There was an error while resetting SNS Topic {topic_arn}, please manually turn off delivery status logging for protocol {endpoint}"
            )",
              },
              "isEnd": true,
              "name": "EnableDeliveryStatusLogging",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableDeliveryStatusLogging.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "LoggingRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Delivery Status Logging.",
              "type": "String",
            },
            "SNSTopicArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):sns:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:([a-zA-Z0-9_-]{1,80}(?:\\.fifo)?)$",
              "description": "(Required) The ARN of the Amazon SNS Topic.",
              "type": "String",
            },
            "SampleRate": {
              "allowedPattern": "^(?:[0-9]|[1-9][0-9]|100)$",
              "default": "0",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableDeliveryStatusLoggingForSNSTopic",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableDynamoDBDeletionProtection": {
      "DependsOn": [
        "CreateWait10",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableDynamoDBDeletionProtection

## Overview
This document enables deletion protection on a DynamoDB Table.

## Pre-requisites
None.

## What does this document do?
Enables deletion protection the given DynamoDB Table.

## Input Parameters
* ResourceArn: (Required) DynamoDB Table to be tagged.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableDynamoDBDeletionProtection.Output
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "UpdateTable",
                "DeletionProtectionEnabled": true,
                "Service": "dynamodb",
                "TableName": "{{ ResourceArn }}",
              },
              "isEnd": true,
              "name": "EnableDynamoDBDeletionProtection",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "EnableDynamoDBDeletionProtection.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ResourceArn": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):dynamodb:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):(?:\\d{12}):table\\/([a-zA-Z0-9._-]{3,255})$",
              "description": "(Required) The DynamoDB Table resource ARN.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableDynamoDBDeletionProtection",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableEbsEncryptionByDefault": {
      "DependsOn": [
        "CreateWait5",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - AWSConfigRemediation-EnableEbsEncryptionByDefault

## What does this document do?
This document enables EBS encryption by default for an AWS account in the current region using the [EnableEbsEncryptionByDefault](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_EnableEbsEncryptionByDefault.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* ModifyAccount.EnableEbsEncryptionByDefaultResponse: JSON formatted response from the EnableEbsEncryptionByDefault API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## ModifyAccount
Enables EBS encryption by default for the account in the current region.
## Outputs
* EnableEbsEncryptionByDefaultResponse: Response from the EnableEbsEncryptionByDefault API.
",
              "inputs": {
                "Api": "EnableEbsEncryptionByDefault",
                "Service": "ec2",
              },
              "isEnd": false,
              "name": "ModifyAccount",
              "outputs": [
                {
                  "Name": "EnableEbsEncryptionByDefaultResponse",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyEbsEncryptionByDefault
Checks if EbsEncryptionByDefault is enabled correctly from the previous step.
",
              "inputs": {
                "Api": "GetEbsEncryptionByDefault",
                "DesiredValues": [
                  "True",
                ],
                "PropertySelector": "$.EbsEncryptionByDefault",
                "Service": "ec2",
              },
              "isEnd": true,
              "name": "VerifyEbsEncryptionByDefault",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "ModifyAccount.EnableEbsEncryptionByDefaultResponse",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableEbsEncryptionByDefault",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableElastiCacheBackups": {
      "DependsOn": [
        "CreateWait10",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableElastiCacheBackups

## Overview
This document enables automatic backups for the given ElastiCache cluster.

## Pre-requisites
None.

## What does this document do?
Takes the SnapshotRetentionPeriod provided by Security Hub control ElastiCache.1 and enables automatic backups for the given ElastiCache cluster/replication group.
If the resource is a replication group with Cluster Mode disabled, the first cluster listed in the group will be used as the SnapshottingClusterId.

## Input Parameters
* ResourceARN: (Required) ID of the ElastiCache cluster.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableAutomaticBackups.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Enables automatic backups for the given ElastiCache Cluster.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "ResourceARN": "{{ResourceARN}}",
                  "SnapshotRetentionPeriod": "{{SnapshotRetentionPeriod}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


def get_elasticache_client():
    return boto3.client("elasticache", config=boto_config)


class Event(TypedDict):
    ResourceARN: str
    SnapshotRetentionPeriod: int


class Response(TypedDict):
    Message: str
    Status: str


def handler(event: Event, _) -> Response:
    """
    Remediates ElastiCache.1 by enabling automatic backups.
    """
    try:
        resource_arn = event["ResourceARN"]
        snapshot_retention_period = event["SnapshotRetentionPeriod"]

        resource_type = resource_arn.split(":")[5]

        if resource_type.lower() == "cluster":
            cluster_id = resource_arn.split(":")[-1]
            enable_cluster_backups(cluster_id, snapshot_retention_period)
        elif resource_type.lower() == "replicationgroup":
            resource_group_id = resource_arn.split(":")[-1]
            enable_replication_group_backups(
                resource_group_id, snapshot_retention_period
            )
        else:
            raise RuntimeError(f"Invalid resource type: {resource_type}")
        return {
            "Message": (f"Successfully enabled backups for cluster {resource_arn}."),
            "Status": "success",
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error enabling automatic backups for ElastiCache cluster: {str(e)}"
        )


def enable_cluster_backups(
    cluster_identifier: str, snapshot_retention_period: int
) -> None:
    try:
        elasticache_client = get_elasticache_client()
        elasticache_client.modify_cache_cluster(
            CacheClusterId=cluster_identifier,
            SnapshotRetentionLimit=snapshot_retention_period,
        )
    except Exception as e:
        raise RuntimeError(
            f"Failed to enable backups for cluster {cluster_identifier}: {str(e)}"
        )


def enable_replication_group_backups(
    replication_group_id: str, snapshot_retention_period: int
) -> None:
    try:
        elasticache_client = get_elasticache_client()

        replication_group_details = elasticache_client.describe_replication_groups(
            ReplicationGroupId=replication_group_id
        )["ReplicationGroups"][0]

        if replication_group_details["ClusterMode"] == "disabled":
            snapshotting_cluster_id = replication_group_details["NodeGroups"][0][
                "NodeGroupMembers"
            ][0]["CacheClusterId"]
            elasticache_client.modify_replication_group(
                ReplicationGroupId=replication_group_id,
                SnapshotRetentionLimit=snapshot_retention_period,
                SnapshottingClusterId=snapshotting_cluster_id,
            )
        else:
            elasticache_client.modify_replication_group(
                ReplicationGroupId=replication_group_id,
                SnapshotRetentionLimit=snapshot_retention_period,
            )
    except Exception as e:
        raise RuntimeError(
            f"Failed to enable backups for replication group {replication_group_id}: {str(e)}"
        )",
              },
              "name": "EnableAutomaticBackups",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableAutomaticBackups.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ResourceARN": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):elasticache:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):(?:\\d{12}):(?:replicationgroup|serverlesscache|cluster):([a-zA-Z](?:(?!--)[a-zA-Z0-9-]){0,48}[a-zA-Z0-9]$|[a-zA-Z]$)",
              "description": "(Required) ID of the ElastiCache cluster.",
              "type": "String",
            },
            "SnapshotRetentionPeriod": {
              "description": "(Required) Minimum snapshot retention period in days.",
              "type": "Integer",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableElastiCacheBackups",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableElastiCacheReplicationGroupFailover": {
      "DependsOn": [
        "CreateWait11",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableElastiCacheReplicationGroupFailover

## Overview
This document enable automatic failover for an ElastiCache replication group.

## Pre-requisites
Replication group must have at least one read replica to enable autofailover.

## What does this document do?
Enables automatic failover for an ElastiCache replication group.

## Input Parameters
* GroupId: (Required) ID of the ElastiCache cluster.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableAutomaticFailover.Output
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "ModifyReplicationGroup",
                "AutomaticFailoverEnabled": true,
                "ReplicationGroupId": "{{ GroupId }}",
                "Service": "elasticache",
              },
              "isEnd": true,
              "name": "EnableAutomaticFailover",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "EnableAutomaticFailover.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "GroupId": {
              "allowedPattern": "^[a-zA-Z](?:(?!--)[a-zA-Z0-9-]){0,48}[a-zA-Z0-9]$|^[a-zA-Z]$",
              "description": "(Required) ID of the ElastiCache cluster.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableElastiCacheReplicationGroupFailover",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableElastiCacheVersionUpgrades": {
      "DependsOn": [
        "CreateWait11",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableElastiCacheVersionUpgrades

## Overview
This document enables automatic minor version upgrades for the given ElastiCache cluster.

## Pre-requisites
The cluster engine must be Valkey 7.2 or Redis OSS engine version 6.0 or later.

## What does this document do?
Updates the given ElastiCache cluster to enable automatic minor version upgrades.

## Input Parameters
* ClusterId: (Required) ID of the ElastiCache cluster.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableAutomaticMinorVersionUpgrades.Output
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "ModifyCacheCluster",
                "AutoMinorVersionUpgrade": true,
                "CacheClusterId": "{{ ClusterId }}",
                "Service": "elasticache",
              },
              "isEnd": true,
              "name": "EnableAutomaticMinorVersionUpgrades",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "EnableAutomaticMinorVersionUpgrades.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ClusterId": {
              "allowedPattern": "^[a-zA-Z](?:(?!--)[a-zA-Z0-9-]){0,48}[a-zA-Z0-9]$|^[a-zA-Z]$",
              "description": "(Required) ID of the ElastiCache cluster.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableElastiCacheVersionUpgrades",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableEncryptionForSNSTopic": {
      "DependsOn": [
        "CreateWait7",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableEncryptionForSNSTopic

## What does this document do?
 This document enables encryption on given Amazon Simple Notification Service (Amazon SNS) topic using
 [SetTopicAttributes](https://docs.aws.amazon.com/sns/latest/api/API_SetTopicAttributes.html) API.

 This document must only be used as a baseline to ensure that your Amazon SNS topics are encrypted with the minimum security best practice of using an AWS KMS customer managed CMK.
 Based on your data policy, Amazon SNS topic should be encrypted with different customer managed CMKs as documented [here](https://docs.aws.amazon.com/kms/latest/developerguide/best-practices.html).

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* TopicArn: (Required)  The ARN of the Amazon SNS Topic.
* KmsKeyArn: (Required) The ARN of AWS KMS Key.

 ## Security Standards / Controls
 * AWS FSBP v1.0.0:   SNS.1
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## EncryptSNSTopic
Makes SetTopicAttributes API call using the Amazon SNS Topic ARN to enables encyption.
## Outputs
* Response: The standard HTTP response from the SetTopicAttributes API call.
",
              "inputs": {
                "Api": "SetTopicAttributes",
                "AttributeName": "KmsMasterKeyId",
                "AttributeValue": "{{KmsKeyArn}}",
                "Service": "sns",
                "TopicArn": "{{TopicArn}}",
              },
              "isEnd": false,
              "name": "EncryptSNSTopic",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyTopicEncryption
Verifies the given Amazon SNS Topic is encrypted with AWS KMS Key ARN.
",
              "inputs": {
                "Api": "GetTopicAttributes",
                "DesiredValues": [
                  "{{ KmsKeyArn }}",
                ],
                "PropertySelector": "Attributes.KmsMasterKeyId",
                "Service": "sns",
                "TopicArn": "{{TopicArn}}",
              },
              "isEnd": true,
              "name": "VerifyTopicEncryption",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EncryptSNSTopic.Response",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "KmsKeyArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "default": "{{ssm:/Solutions/SO0111/CMK_REMEDIATION_ARN}}",
              "description": "The ARN of the KMS key created by ASR for this remediation",
              "type": "String",
            },
            "TopicArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):sns:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:([a-zA-Z0-9_-]{1,80}(?:\\.fifo)?)$",
              "description": "(Required) The ARN of the Amazon SNS Topic.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableEncryptionForSNSTopic",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableEncryptionForSQSQueue": {
      "DependsOn": [
        "CreateWait4",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableEncryptionForSQSQueue

## What does this document do?
This document enables encryption on given Amazon Simple Queue Service (Amazon SQS) queue using
[SetQueueAttributes](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_SetQueueAttributes.html) API.

This document must only be used as a baseline to ensure that your Amazon SQS queues are encrypted with the minimum security best practice of using an AWS KMS customer managed CMK.
Based on your data policy, Amazon SQS queues should be encrypted with different customer managed CMKs as documented [here](https://docs.aws.amazon.com/kms/latest/developerguide/best-practices.html).

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* SQSQueueName: (Required)  The name of the Amazon SQS Queue.
* KmsKeyArn: (Required) The ARN of AWS KMS Key.

## Security Standards / Controls
* AWS FSBP v1.0.0:   SQS.1
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "GetQueueUrl",
                "QueueName": "{{ SQSQueueName }}",
                "Service": "sqs",
              },
              "name": "GetQueueUrl",
              "outputs": [
                {
                  "Name": "QueueUrl",
                  "Selector": "$.QueueUrl",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "SetQueueAttributes",
                "Attributes": {
                  "KmsMasterKeyId": "{{ KmsKeyArn }}",
                },
                "QueueUrl": "{{ GetQueueUrl.QueueUrl }}",
                "Service": "sqs",
              },
              "name": "EncryptSQSQueue",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "inputs": {
                "Api": "GetQueueAttributes",
                "AttributeNames": [
                  "KmsMasterKeyId",
                ],
                "DesiredValues": [
                  "{{ KmsKeyArn }}",
                ],
                "PropertySelector": "$.Attributes.KmsMasterKeyId",
                "QueueUrl": "{{ GetQueueUrl.QueueUrl }}",
                "Service": "sqs",
              },
              "isEnd": true,
              "name": "VerifyQueueEncryption",
              "timeoutSeconds": 300,
            },
          ],
          "outputs": [
            "EncryptSQSQueue.Response",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "KmsKeyArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "default": "{{ssm:/Solutions/SO0111/CMK_REMEDIATION_ARN}}",
              "description": "The ARN of the KMS key created by ASR for this remediation",
              "type": "String",
            },
            "SQSQueueName": {
              "allowedPattern": "^[a-zA-Z0-9_-]{1,80}(?:\\.fifo)?$",
              "description": "(Required) The name of the Amazon SQS Queue.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableEncryptionForSQSQueue",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableEnhancedMonitoringOnRDSInstance": {
      "DependsOn": [
        "CreateWait5",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - AWSConfigRemediation-EnableEnhancedMonitoringOnRDSInstance

## What does this document do?
This document is used to enable enhanced monitoring on an RDS Instance using the input parameter DB Instance ARN.

## Input Parameters
* RDSInstanceARN: (Required) ARN of the RDS DB Instance.
* MonitoringInterval: (Optional)
   * The interval, in seconds, between points when Enhanced Monitoring metrics are collected for the DB instance.
   * If MonitoringRoleArn is specified, then you must also set MonitoringInterval to a value other than 0.
   * Valid Values: 1, 5, 10, 15, 30, 60
   * Default: 60
* MonitoringRoleArn: (Required) The ARN for the IAM role that permits RDS to send enhanced monitoring metrics to Amazon CloudWatch Logs.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableEnhancedMonitoring.DbInstance - The standard HTTP response from the ModifyDBInstance API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## DescribeDBInstances
  Makes describeDBInstances API call using RDS Instance ARN.
## Outputs
* DbInstanceIdentifier: DBInstance Identifier of the RDS Instance.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ RDSInstanceARN }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "DescribeDBInstances",
              "outputs": [
                {
                  "Name": "DbInstanceIdentifier",
                  "Selector": "$.DBInstances[0].DBInstanceIdentifier",
                  "Type": "String",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyDBInstanceStatus
Verifies if DB Instance status is available before enabling enhanced monitoring.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ DescribeDBInstances.DbInstanceIdentifier }}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.DBInstances[0].DBInstanceStatus",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "VerifyDBInstanceStatus",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableEnhancedMonitoring
  Makes ModifyDBInstance API call to enable Enhanced Monitoring on the RDS Instance
  using the DBInstanceId from the previous action.
## Outputs
  * DbInstance: The standard HTTP response from the ModifyDBInstance API.
",
              "inputs": {
                "Api": "ModifyDBInstance",
                "ApplyImmediately": false,
                "DBInstanceIdentifier": "{{ DescribeDBInstances.DbInstanceIdentifier }}",
                "MonitoringInterval": "{{ MonitoringInterval }}",
                "MonitoringRoleArn": "{{ MonitoringRoleArn }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "EnableEnhancedMonitoring",
              "outputs": [
                {
                  "Name": "DbInstance",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "description": "## VerifyEnhancedMonitoringEnabled
Checks that the enhanced monitoring is enabled on RDS Instance in the previous step exists.
## Outputs
* Output: The standard HTTP response from the ModifyDBInstance API.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "DBIdentifier": "{{ DescribeDBInstances.DbInstanceIdentifier }}",
                  "MonitoringInterval": "{{ MonitoringInterval }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


def connect_to_service(service):
    return boto3.client(service, config=boto_config)


class Event(TypedDict):
    MonitoringInterval: int
    DBIdentifier: str


class HandlerResponse(TypedDict):
    Status: str
    Message: str
    DBMonitoringInterval: str


def handler(event, _):
    """
    Verifies that the enhanced monitoring is enabled on the RDS Instance.
    """
    try:
        rds_client = connect_to_service("rds")
        db_instance_id = event["DBIdentifier"]
        monitoring_interval = event["MonitoringInterval"]

        rds_waiter = rds_client.get_waiter("db_instance_available")
        rds_waiter.wait(DBInstanceIdentifier=db_instance_id)

        db_instances = rds_client.describe_db_instances(
            DBInstanceIdentifier=db_instance_id
        )
        db_monitoring_interval = db_instances.get("DBInstances")[0].get(
            "MonitoringInterval"
        )

        if db_monitoring_interval == monitoring_interval:
            return {
                "Status": "Success",
                "Message": f"Verified enhanced monitoring on RDS Instance {db_instance_id}.",
                "DBMonitoringInterval": str(db_monitoring_interval),
            }
        else:
            return {
                "Status": "Failed",
                "Message": f"RDS Instance {db_instance_id} does not have correct monitoring interval.\\n "
                f"Expected: {monitoring_interval}\\n Actual: {db_monitoring_interval}",
                "DBMonitoringInterval": str(db_monitoring_interval),
            }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error verifying enhanced monitoring on RDS Instance: {str(e)}"
        )",
              },
              "isEnd": true,
              "name": "VerifyEnhancedMonitoringEnabled",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableEnhancedMonitoring.DbInstance",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "MonitoringInterval": {
              "allowedValues": [
                1,
                5,
                10,
                15,
                30,
                60,
              ],
              "default": 60,
              "description": "(Optional) The interval, in seconds, between points when Enhanced Monitoring metrics are collected for the DB instance.",
              "type": "Integer",
            },
            "MonitoringRoleArn": {
              "allowedPattern": "^arn:(aws[a-zA-Z-]*)?:iam::\\d{12}:role/[a-zA-Z0-9+=,.@_/-]+$",
              "description": "(Required) The ARN for the IAM role that permits RDS to send enhanced monitoring metrics to Amazon CloudWatch Logs.",
              "type": "String",
            },
            "RDSInstanceARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):rds:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:db:.+$",
              "description": "(Required) ARN of the Amazon RDS instance for which Enhanced Monitoring needs to be enabled.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableEnhancedMonitoringOnRDSInstance",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableGuardDuty": {
      "DependsOn": [
        "CreateWait12",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableGuardDuty

## What does this document do?
  This document enables Amazon GuardDuty.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Security Standards / Controls
* AFSBP v1.0.0:  GuardDuty.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

BOTO_CONFIG = Config(retries={"mode": "standard"})


def connect_to_guardduty(boto_config):
    return boto3.client("guardduty", config=boto_config)


def lambda_handler(_, __):
    guardduty = connect_to_guardduty(BOTO_CONFIG)

    detector_list = guardduty.list_detectors()["DetectorIds"]

    if detector_list == []:
        detector = guardduty.create_detector(
            Enable=True,
            DataSources={
                "S3Logs": {"Enable": True},
                "Kubernetes": {"AuditLogs": {"Enable": True}},
            },
        )

        return {
            "output": {
                "Message": f'GuardDuty Enabled. Detector {detector["DetectorId"]} created'
            }
        }

    else:
        for detector_id in detector_list:
            if guardduty.get_detector(DetectorId=detector_id)["Status"] == "DISABLED":
                guardduty.update_detector(
                    DetectorId=detector_id,
                    Enable=True,
                    DataSources={
                        "S3Logs": {"Enable": True},
                        "Kubernetes": {"AuditLogs": {"Enable": True}},
                    },
                )
                return {
                    "output": {
                        "Message": f"GuardDuty Enabled. Existing detector {detector_id} has been enabled."
                    }
                }

        return {"output": {"Message": "GuardDuty is already enabled."}}",
              },
              "maxAttempts": 3,
              "name": "EnableGuardDuty",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableGuardDuty.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableGuardDuty",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableIMDSV2OnInstance": {
      "DependsOn": [
        "CreateWait8",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableIMDSV2OnInstance

## What does this document do?
  This document enables IMDS V2 by using the
  [ModifyInstanceMetadataOptions](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyInstanceMetadataOptions.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* InstanceARN: (Required)  The ARN of the Amazon EC2 Instance.

## Security Standards / Controls
* AWS FSBP v1.0.0:  EC2.8
* NIST 800-53 Rev5: EC2.8
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "instance_arn": "{{ InstanceARN }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_ec2():
    return boto3.client("ec2", config=boto_config)


def lambda_handler(event, _):
    """
    Enable IMDSv2 on an EC2 Instance.

    \`event\` should have the following keys and values:
    \`instance_arn\`: the ARN of the instance that does not have IMDSv2 enabled.

    \`context\` is ignored
    """

    instance_arn = event["instance_arn"]

    instance_id = instance_arn.split("/")[1]

    enable_imdsv2(instance_id)

    instance_attributes = describe_instance(instance_id)

    imds_v2_attribute = instance_attributes["Reservations"][0]["Instances"][0][
        "MetadataOptions"
    ]

    if imds_v2_attribute["HttpTokens"] == "required":
        return imds_v2_attribute

    raise RuntimeError(
        f"ASR Remediation failed - {instance_id} did not have IMDSv2 enabled."
    )


def enable_imdsv2(instance_id):
    """
    Changes EC2 Instance metadata options to require IMDSv2
    """
    ec2 = connect_to_ec2()
    try:
        ec2.modify_instance_metadata_options(
            InstanceId=instance_id, HttpTokens="required", HttpEndpoint="enabled"
        )

    except Exception as e:
        exit("There was an error enabling IMDSv2: " + str(e))


def describe_instance(instance_id):
    """
    Grabs Instance Attributes to verify IMDSv2 values were set as expected.
    """
    ec2 = connect_to_ec2()
    try:
        instance_attributes = ec2.describe_instances(InstanceIds=[instance_id])
        return instance_attributes

    except Exception as e:
        exit("Failed to get attributes of instance: " + str(e))",
              },
              "name": "EnableIMDSV2OnInstance",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableIMDSV2OnInstance.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "InstanceARN": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):ec2:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:instance\\/(i-[0-9a-f]*)$",
              "description": "(Required) The ARN of the Amazon EC2 Instance.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableIMDSV2OnInstance",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableKeyRotation": {
      "DependsOn": [
        "CreateWait5",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-EnableKeyRotation

## What does this document do?
This document enables automatic key rotation for the given AWS Key Management Service (KMS) symmetric customer master key(CMK) using [EnableKeyRotation](https://docs.aws.amazon.com/kms/latest/APIReference/API_EnableKeyRotation.html) API.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* KeyId: (Required) The Key ID of the AWS KMS symmetric CMK.

## Output Parameters
* EnableKeyRotation.EnableKeyRotationResponse: The standard HTTP response from the EnableKeyRotation API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableKeyRotation
Enables automatic key rotation for the given AWS KMS CMK.
## Outputs
* EnableKeyRotationResponse: The standard HTTP response from the EnableKeyRotation API.
",
              "inputs": {
                "Api": "EnableKeyRotation",
                "KeyId": "{{ KeyId }}",
                "Service": "kms",
              },
              "isEnd": false,
              "name": "EnableKeyRotation",
              "outputs": [
                {
                  "Name": "EnableKeyRotationResponse",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyKeyRotation
Verifies that the KeyRotationEnabled is set to true for the given AWS KMS CMK.
",
              "inputs": {
                "Api": "GetKeyRotationStatus",
                "DesiredValues": [
                  "True",
                ],
                "KeyId": "{{ KeyId }}",
                "PropertySelector": "$.KeyRotationEnabled",
                "Service": "kms",
              },
              "isEnd": true,
              "name": "VerifyKeyRotation",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableKeyRotation.EnableKeyRotationResponse",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "KeyId": {
              "allowedPattern": "[a-z0-9-]{1,2048}",
              "description": "(Required) The Key ID of the AWS KMS symmetric CMK.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableKeyRotation",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableMacie": {
      "DependsOn": [
        "CreateWait14",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - EnableMacie

## What does this document do?
This document enables AWS Macie.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableMacie.Output: The standard HTTP response from the EnableMacie API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableMacie
Enables AWS Macie with publishing frequency set to the maximum allowed (six hours).
## Outputs
* EnableMacieResponse: The standard HTTP response from the EnableMacie API.
",
              "inputs": {
                "Api": "EnableMacie",
                "Service": "macie2",
                "findingPublishingFrequency": "SIX_HOURS",
                "status": "ENABLED",
              },
              "isEnd": true,
              "name": "EnableMacie",
              "outputs": [
                {
                  "Name": "EnableMacieResponse",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableMacie.EnableMacieResponse",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableMacie",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableMinorVersionUpgradeOnRDSDBInstance": {
      "DependsOn": [
        "CreateWait7",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-EnableMinorVersionUpgradeOnRDSDBInstance

## What does this document do?
This document enables AutoMinorVersionUpgrade on the Amazon Relational Database Service (Amazon RDS) instance using the [ModifyDBInstance](https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ModifyDBInstance.html) API.

## Input parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* RDSInstanceARN: (Required) ARN of the Amazon RDS instance to be modified.

## Output parameters
* ModifyDBInstance.Output: The standard HTTP response from the ModifyDBInstance API.
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "RDSInstanceARN": "{{RDSInstanceARN}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})

multi_az_cluster_engines = ["mysql", "postgres"]


def connect_to_rds():
    return boto3.client("rds", config=boto_config)


def lambda_handler(event, _):
    """
    Enable auto minor version upgrades on an instance or a Multi-AZ Cluster

    \`event\` should have the following keys and values:
    \`DBInstanceIdentifier\`: The identifier of the database instance that is to be modified.

    \`context\` is ignored
    """
    db_instance_arn = event["RDSInstanceARN"]

    rds = connect_to_rds()

    found_instance = rds.describe_db_instances(DBInstanceIdentifier=db_instance_arn)

    instance_info = found_instance["DBInstances"][0]
    db_instance_id = instance_info["DBInstanceIdentifier"]

    response = False

    if "DBClusterIdentifier" in instance_info.keys():
        if multi_az_check(instance_info["DBClusterIdentifier"]):
            cluster_id = instance_info["DBClusterIdentifier"]
            enable_minor_version_upgrade_cluster(cluster_id)
            response = verify_cluster_changes(cluster_id)
        else:
            enable_minor_version_upgrade_instance(db_instance_id)
            response = verify_instance_changes(db_instance_id)
    else:
        enable_minor_version_upgrade_instance(db_instance_id)
        response = verify_instance_changes(db_instance_id)

    if response is True:
        return {"AutoMinorVersionUpgrade": response}

    raise RuntimeError(
        f"ASR Remediation failed - {db_instance_arn} did not have enable auto minor version upgrades enabled."
    )


def multi_az_check(cluster_id):
    """
    Checks to see if the cluster is Multi-AZ. Instances within clusters that match this check are not able to be modified.
    """

    rds = connect_to_rds()
    try:
        found_cluster = rds.describe_db_clusters(DBClusterIdentifier=cluster_id)
        cluster_info = found_cluster["DBClusters"][0]
    except Exception as e:
        exit(f"Failed to get information about the cluster: {cluster_id}.  Error: {e}")

    return (cluster_info["MultiAZ"] is True) and (
        cluster_info["Engine"] in multi_az_cluster_engines
    )


def enable_minor_version_upgrade_cluster(cluster_id):
    """
    Enables automatic minor version upgrade for a Multi-AZ Cluster.
    """

    rds = connect_to_rds()
    try:
        rds.modify_db_cluster(
            DBClusterIdentifier=cluster_id, AutoMinorVersionUpgrade=True
        )
    except Exception as e:
        exit(f"Failed to modify the cluster: {cluster_id}. Error: {e}")


def enable_minor_version_upgrade_instance(instance_id):
    """
    Enables automatic minor version upgrade for an instance.
    """

    rds = connect_to_rds()
    try:
        rds.modify_db_instance(
            DBInstanceIdentifier=instance_id, AutoMinorVersionUpgrade=True
        )
    except Exception as e:
        exit(f"Failed to modify the instance: {instance_id}. Error: {e}")


def verify_cluster_changes(cluster_id):
    """
    Verifies automatic minor version upgrade for a Multi-AZ cluster.
    """
    rds = connect_to_rds()
    try:
        found_cluster = rds.describe_db_clusters(
            DBClusterIdentifier=cluster_id, MaxRecords=100
        )
        cluster_info = found_cluster["DBClusters"][0]

    except Exception as e:
        exit(f"Failed to verify cluster changes: {cluster_id}. Error: {e}")

    return cluster_info["AutoMinorVersionUpgrade"]


def verify_instance_changes(instance_id):
    """
    Verifies automatic minor version upgrade for an instance.
    """
    rds = connect_to_rds()
    try:
        found_instance = rds.describe_db_instances(
            DBInstanceIdentifier=instance_id, MaxRecords=100
        )
        instance_info = found_instance["DBInstances"][0]
    except Exception as e:
        exit(f"Failed to verify instance changes: {instance_id}. Error: {e}")

    return instance_info["AutoMinorVersionUpgrade"]",
              },
              "name": "ModifyDBInstance",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "ModifyDBInstance.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RDSInstanceARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):rds:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:db:.+$",
              "description": "(Required) Identifier of the Amazon RDS instance for which AutoMinorVersionUpgrade needs to be enabled.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableMinorVersionUpgradeOnRDSDBInstance",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableMultiAZOnRDSInstance": {
      "DependsOn": [
        "CreateWait6",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnableMultiAZOnRDSInstance

## What does this document do?
This document enables MultiAZ on an RDS instance.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* RDSInstanceARN: (Required) ARN of the RDS instance to be modified.
* ApplyImmediately: (Optional) The MultiAZ on an RDS instance change is applied during the next maintenance window unless the ApplyImmediately parameter is enabled (true) for this request. By default, this parameter is disabled (false).

## Output Parameters
* EnableMultiAZ.DBInstance: The standard HTTP response from the ModifyDBInstance API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## DescribeDBInstances
Makes DescribeDBInstances API call using RDS DB instance resource identifiers to get DBInstanceIdentifier.
## Outputs
* DBInstanceIdentifier: DBInstance identifier of the RDS instance.
* MultiAZ: MultiAZ state of the RDS instance.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ RDSInstanceARN }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "DescribeDBInstances",
              "outputs": [
                {
                  "Name": "DBInstanceIdentifier",
                  "Selector": "$.DBInstances[0].DBInstanceIdentifier",
                  "Type": "String",
                },
                {
                  "Name": "MultiAZ",
                  "Selector": "$.DBInstances[0].MultiAZ",
                  "Type": "Boolean",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyDBInstanceStatus
Verifies if DB instance status is available before enabling MultiAZ.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ DescribeDBInstances.DBInstanceIdentifier }}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.DBInstances[0].DBInstanceStatus",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "VerifyDBInstanceStatus",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:branch",
              "description": "## EndIfMultiAZAlreadyEnabled
Checks if MultiAZ is not enabled on the DB instance. If not enabled, proceed with EnableMultiAZ step. Otherwise, end the flow.
",
              "inputs": {
                "Choices": [
                  {
                    "BooleanEquals": false,
                    "NextStep": "EnableMultiAZ",
                    "Variable": "{{ DescribeDBInstances.MultiAZ }}",
                  },
                ],
              },
              "isEnd": true,
              "name": "EndIfMultiAZAlreadyEnabled",
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableMultiAZ
Makes ModifyDBInstance API call to enable MultiAZ on the RDS instance using the DBInstanceIdentifier from the previous step and MultiAZ as true.
## Outputs
* DBInstance: The standard HTTP response from the ModifyDBInstance API.
",
              "inputs": {
                "Api": "ModifyDBInstance",
                "ApplyImmediately": "{{ ApplyImmediately }}",
                "DBInstanceIdentifier": "{{ DescribeDBInstances.DBInstanceIdentifier }}",
                "MultiAZ": true,
                "Service": "rds",
              },
              "isEnd": false,
              "name": "EnableMultiAZ",
              "outputs": [
                {
                  "Name": "DBInstance",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyMultiAZEnabled
Verifies that the RDS Instance's \`PendingModifiedValues.MultiAZ\` value is \`True\`.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ DescribeDBInstances.DBInstanceIdentifier }}",
                "DesiredValues": [
                  "True",
                ],
                "PropertySelector": "$.DBInstances[0].PendingModifiedValues.MultiAZ",
                "Service": "rds",
              },
              "isEnd": true,
              "name": "VerifyMultiAZEnabled",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableMultiAZ.DBInstance",
          ],
          "parameters": {
            "ApplyImmediately": {
              "allowedValues": [
                true,
                false,
              ],
              "default": false,
              "description": "(Optional) MultiAZ on an RDS instance change is applied during the next maintenance window unless the ApplyImmediately parameter is enabled (true) for this request. By default, this parameter is disabled (false).",
              "type": "Boolean",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RDSInstanceARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):rds:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:db:.+$",
              "description": "(Required) ARN of the RDS instance for which MultiAZ needs to be enabled.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableMultiAZOnRDSInstance",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnablePrivateRepositoryScanning": {
      "DependsOn": [
        "CreateWait11",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-EnablePrivateRepositoryScanning

## What does this document do?
  This document enables image scanning configuration on a private ECR repository.
  [PutImageScanningConfiguration](https://docs.aws.amazon.com/AmazonECR/latest/APIReference/API_PutImageScanningConfiguration.html) API.


## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* RepositoryName: (Required) The name of the ECR private repository.

## Security Standards / Controls
* AFSBP v1.0.0:  ECR.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "RepositoryName": "{{ RepositoryName }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_ecr():
    return boto3.client("ecr", config=boto_config)


def lambda_handler(event, _):
    repository_name = event["RepositoryName"]
    ecr = connect_to_ecr()

    response = ecr.put_image_scanning_configuration(
        repositoryName=repository_name, imageScanningConfiguration={"scanOnPush": True}
    )

    return response",
              },
              "maxAttempts": 3,
              "name": "EnablePrivateRepositoryScanning",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnablePrivateRepositoryScanning.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RepositoryName": {
              "allowedPattern": "([a-z0-9._\\/\\-]+)$",
              "description": "(Required) The name of the ECR private repository.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnablePrivateRepositoryScanning",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableRDSClusterDeletionProtection": {
      "DependsOn": [
        "CreateWait6",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-EnableRDSClusterDeletionProtection

## What does this document do?
This document enables \`Deletion Protection\` on a given Amazon RDS cluster using the [ModifyDBCluster](https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ModifyDBCluster.html) API.
Please note, AWS Config is required to be enabled in this region for this document to work as it requires the resource ID recorded by the AWS Config service.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* RDSClusterARN: (Required) ARN of the Amazon RDS cluster.

## Output Parameters
* EnableRDSClusterDeletionProtection.ModifyDBClusterResponse: The standard HTTP response from the ModifyDBCluster API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## GetRDSInstanceIdentifier
Makes DescribeDBClusters API call using Amazon RDS Instance ARN to get DbClusterIdentifier.
## Outputs
* DbClusterIdentifier: Identifier of the Amazon RDS Cluster.
",
              "inputs": {
                "Api": "DescribeDBClusters",
                "DBClusterIdentifier": "{{ RDSClusterARN }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "DescribeDBClusters",
              "outputs": [
                {
                  "Name": "DbClusterIdentifier",
                  "Selector": "$.DBClusters[0].DBClusterIdentifier",
                  "Type": "String",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyStatus
Verifies if \`Status\` is available before proeeding to the next step.
",
              "inputs": {
                "Api": "DescribeDBClusters",
                "DBClusterIdentifier": "{{ RDSClusterARN }}",
                "DesiredValues": [
                  "available",
                ],
                "PropertySelector": "$.DBClusters[0].Status",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "VerifyStatus",
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableRDSClusterDeletionProtection
Enables deletion protection on the Amazon RDS Cluster.
## Outputs
* ModifyDBClusterResponse: The standard HTTP response from the ModifyDBCluster API.
",
              "inputs": {
                "Api": "ModifyDBCluster",
                "DBClusterIdentifier": "{{ DescribeDBClusters.DbClusterIdentifier }}",
                "DeletionProtection": true,
                "Service": "rds",
              },
              "isEnd": false,
              "name": "EnableRDSClusterDeletionProtection",
              "outputs": [
                {
                  "Name": "ModifyDBClusterResponse",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyDBClusterModification
Verifies that deletion protection has been enabled for the given Amazon RDS database cluster.
",
              "inputs": {
                "Api": "DescribeDBClusters",
                "DBClusterIdentifier": "{{ DescribeDBClusters.DbClusterIdentifier }}",
                "DesiredValues": [
                  "True",
                ],
                "PropertySelector": "$.DBClusters[0].DeletionProtection",
                "Service": "rds",
              },
              "isEnd": true,
              "name": "VerifyDBClusterModification",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableRDSClusterDeletionProtection.ModifyDBClusterResponse",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RDSClusterARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):rds:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:cluster:.+$",
              "description": "(Required) Amazon RDS cluster ARN for which deletion protection needs to be enabled.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableRDSClusterDeletionProtection",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableRDSInstanceDeletionProtection": {
      "DependsOn": [
        "CreateWait6",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableRDSInstanceDeletionProtection

## What does this document do?
This document enables \`Deletion Protection\` on a given Amazon RDS instance using the [ModifyDBInstance](https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_ModifyDBInstance.html) API.

## Input Parameters
* ApplyImmediately: (Optional) A value that indicates whether the modifications in this request and any pending modifications
  are asynchronously applied as soon as possible, regardless of the PreferredMaintenanceWindow setting for the DB instance.
  * Default: "false"
* RDSInstanceARN: (Required) Amazon RDS Instance ARN for which deletion protection needs to be enabled.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableRDSInstanceDeletionProtection.ModifyDBInstanceResponse - The standard HTTP response from the ModifyDBInstance API.
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## GetRDSInstanceIdentifier
Makes DescribeDBInstances API call using Amazon RDS Instance ARN to get DBInstance Identifier.
## Outputs
* DbInstanceIdentifier: DBInstance Identifier of the Amazon RDS Instance.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ RDSInstanceARN }}",
                "Service": "rds",
              },
              "isEnd": false,
              "name": "GetRDSInstanceIdentifier",
              "outputs": [
                {
                  "Name": "DbInstanceIdentifier",
                  "Selector": "$.DBInstances[0].DBInstanceIdentifier",
                  "Type": "String",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableRDSInstanceDeletionProtection
Makes ModifyDBInstance API call to enable deletion protection on the Amazon RDS Instance using the DBInstanceId from the previous action.
## Outputs
* DbInstance: The standard HTTP response from the ModifyDBInstance API.
",
              "inputs": {
                "Api": "ModifyDBInstance",
                "ApplyImmediately": "{{ ApplyImmediately }}",
                "DBInstanceIdentifier": "{{ GetRDSInstanceIdentifier.DbInstanceIdentifier }}",
                "DeletionProtection": true,
                "Service": "rds",
              },
              "isEnd": false,
              "name": "EnableRDSInstanceDeletionProtection",
              "outputs": [
                {
                  "Name": "ModifyDBInstanceResponse",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## VerifyDBInstanceModification
Checks whether deletion protection is enabled on Amazon RDS Instance.
",
              "inputs": {
                "Api": "DescribeDBInstances",
                "DBInstanceIdentifier": "{{ GetRDSInstanceIdentifier.DbInstanceIdentifier }}",
                "DesiredValues": [
                  "True",
                ],
                "PropertySelector": "$.DBInstances[0].DeletionProtection",
                "Service": "rds",
              },
              "isEnd": true,
              "name": "VerifyDBInstanceModification",
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableRDSInstanceDeletionProtection.ModifyDBInstanceResponse",
          ],
          "parameters": {
            "ApplyImmediately": {
              "default": false,
              "description": "(Optional) A value that indicates whether the modifications in this request and any pending modifications are asynchronously applied as soon as possible, regardless of the PreferredMaintenanceWindow setting for the DB instance.",
              "type": "Boolean",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RDSInstanceARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):rds:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:db:.+$",
              "description": "(Required) ARN of the Amazon RDS instance for which deletion protection needs to be enabled.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableRDSInstanceDeletionProtection",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableRedshiftClusterAuditLogging": {
      "DependsOn": [
        "CreateWait3",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-EnableRedshiftClusterAuditLogging

## What does this document do?
This automation document enables audit logging on the Amazon Redshift cluster using [EnableLogging](https://docs.aws.amazon.com/redshift/latest/APIReference/API_EnableLogging.html) API call with given bucket name and s3 key prefix.

## Input Parameters
* ClusterIdentifier: (Required) The unique identifier of the Amazon Redshift cluster on which logging to be started.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* BucketName: (Required) The name of an existing Amazon S3 bucket where the log files are to be stored.
* S3KeyPrefix: (Optional) The prefix applied to the log file names.

## Output Parameters
* EnableLoggingWithPrefix.Response: Standard HTTP response of the EnableLogging API.
* EnableLoggingWithoutPrefix.Response: Standard HTTP response of the EnableLogging API.
",
          "mainSteps": [
            {
              "action": "aws:branch",
              "description": "## CheckS3KeyPrefix
Checks whether S3KeyPrefix provided in the input parameters.
",
              "inputs": {
                "Choices": [
                  {
                    "NextStep": "EnableLoggingWithoutPrefix",
                    "StringEquals": "",
                    "Variable": "{{S3KeyPrefix}}",
                  },
                ],
                "Default": "EnableLoggingWithPrefix",
              },
              "isEnd": true,
              "name": "CheckS3KeyPrefix",
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableLoggingWithoutPrefix
Enables logging on the given Amazon Redshift cluster using the [EnableLogging](https://docs.aws.amazon.com/redshift/latest/APIReference/API_EnableLogging.html) API with given bucket name in input parameters.
## Outputs
* Response: Standard HTTP response of the EnableLogging API.
",
              "inputs": {
                "Api": "EnableLogging",
                "BucketName": "{{BucketName}}",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "Service": "redshift",
              },
              "name": "EnableLoggingWithoutPrefix",
              "nextStep": "AssertClusterLoggingEnabled",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## EnableLoggingWithPrefix
Enables logging on the given Amazon Redshift cluster using the [EnableLogging](https://docs.aws.amazon.com/redshift/latest/APIReference/API_EnableLogging.html) API with given bucket name and s3 key prefix in input parameters.
## Outputs
* Response: Standard HTTP response of the EnableLogging API.
",
              "inputs": {
                "Api": "EnableLogging",
                "BucketName": "{{BucketName}}",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "S3KeyPrefix": "{{S3KeyPrefix}}",
                "Service": "redshift",
              },
              "name": "EnableLoggingWithPrefix",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## AssertClusterBucketPrefix
Verifies whether the value of the "S3KeyPrefix" parameter is used for logging for the given Amazon Redshift cluster.
",
              "inputs": {
                "Api": "DescribeLoggingStatus",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "DesiredValues": [
                  "{{S3KeyPrefix}}/",
                ],
                "PropertySelector": "$.S3KeyPrefix",
                "Service": "redshift",
              },
              "name": "AssertClusterBucketPrefix",
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## AssertClusterLoggingEnabled
Verifies whether the "LoggingEnabled" property is set to "True" for the given Amazon Redshift cluster.
",
              "inputs": {
                "Api": "DescribeLoggingStatus",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "DesiredValues": [
                  "True",
                ],
                "PropertySelector": "$.LoggingEnabled",
                "Service": "redshift",
              },
              "name": "AssertClusterLoggingEnabled",
            },
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## AssertClusterLoggingBucket
Checks whether the value of the "BucketName" parameter is used for the audit logging configuration of the given Amazon Redshift cluster.
",
              "inputs": {
                "Api": "DescribeLoggingStatus",
                "ClusterIdentifier": "{{ ClusterIdentifier }}",
                "DesiredValues": [
                  "{{BucketName}}",
                ],
                "PropertySelector": "$.BucketName",
                "Service": "redshift",
              },
              "isEnd": true,
              "name": "AssertClusterLoggingBucket",
            },
          ],
          "outputs": [
            "EnableLoggingWithoutPrefix.Response",
            "EnableLoggingWithPrefix.Response",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "The name of an existing Amazon S3 bucket where the log files are to be stored.",
              "type": "String",
            },
            "ClusterIdentifier": {
              "allowedPattern": "^(?!.*--)[a-z][a-z0-9-]{0,62}(?<!-)$",
              "description": "The unique identifier of the Amazon Redshift cluster on which the logging logging to be started.",
              "type": "String",
            },
            "S3KeyPrefix": {
              "allowedPattern": "^[^"'\\\\ ]{0,512}$",
              "default": "",
              "description": "The prefix applied to the log file names.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableRedshiftClusterAuditLogging",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableSSMDocumentBlockPublicSharing": {
      "DependsOn": [
        "CreateWait9",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableSSMDocumentBlockPublicSharing

## What does this document do?
This document enables the block public sharing setting for AWS Systems Manager documents at the account level.
This prevents any SSM documents in the account from being shared publicly.

## Input Parameters
* AccountId: (Required) AWS Account ID where the setting will be enabled.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnableBlockPublicSharing.Output

## Security Control
* SSM.7 - SSM documents should have the block public sharing setting enabled
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Enables the block public sharing setting for SSM documents at the account level
",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "account_id": "{{AccountId}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config


class EventType(TypedDict):
    account_id: str


boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_ssm():
    return boto3.client("ssm", config=boto_config)


def lambda_handler(event: EventType, _):
    try:
        account_id = event["account_id"]
        ssm_client = connect_to_ssm()

        current_setting = get_service_setting(ssm_client)

        if current_setting == "Disable":
            return {
                "response": {
                    "message": f"Block public sharing is already enabled for account {account_id}",
                    "status": "NO_CHANGE_REQUIRED",
                    "setting_value": "Disable",
                }
            }

        update_service_setting(ssm_client)

        verify_setting = get_service_setting(ssm_client)

        if verify_setting == "Disable":
            return {
                "response": {
                    "message": f"Successfully enabled block public sharing for account {account_id}",
                    "status": "SUCCESS",
                    "setting_value": "Disable",
                }
            }
        else:
            raise RuntimeError(
                f"Failed to verify setting change. Expected 'Disable', got '{verify_setting}'"
            )

    except Exception as e:
        error_msg = f"Failed to enable block public sharing: {str(e)}"
        print(error_msg)
        raise RuntimeError(error_msg)


def get_service_setting(ssm_client):
    try:
        response = ssm_client.get_service_setting(
            SettingId="/ssm/documents/console/public-sharing-permission"
        )
        return response["ServiceSetting"]["SettingValue"]
    except Exception as e:
        raise RuntimeError(f"Failed to get service setting: {str(e)}")


def update_service_setting(ssm_client):
    try:
        ssm_client.update_service_setting(
            SettingId="/ssm/documents/console/public-sharing-permission",
            SettingValue="Disable",
        )
    except Exception as e:
        raise RuntimeError(f"Failed to update service setting: {str(e)}")",
              },
              "name": "EnableBlockPublicSharing",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnableBlockPublicSharing.Output",
          ],
          "parameters": {
            "AccountId": {
              "allowedPattern": "^\\d{12}$",
              "description": "(Required) The AWS Account ID.",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableSSMDocumentBlockPublicSharing",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnableVPCFlowLogs": {
      "DependsOn": [
        "CreateWait1",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnableVPCFlowLogs
## What does this document do?
Enables VPC Flow Logs for a given VPC

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* VPC: VPC Id of the VPC for which logs are to be enabled
* RemediationRole: role arn of the role to use for logging
* KMSKeyArn: Amazon Resource Name (ARN) of the KMS Customer-Managed Key to use to encrypt the log group

## Security Standards / Controls
* AWS FSBP v1.0.0:   CloudTrail.2
* CIS v1.2.0:     2.7
* PCI:            CloudTrail.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "enable_flow_logs",
                "InputPayload": {
                  "kms_key_arn": "{{KMSKeyArn}}",
                  "remediation_role": "{{RemediationRole}}",
                  "vpc": "{{VPC}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import time

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError


def connect_to_logs(boto_config):
    return boto3.client("logs", config=boto_config)


def connect_to_ec2(boto_config):
    return boto3.client("ec2", config=boto_config)


def log_group_exists(client, group):
    try:
        log_group_verification = client.describe_log_groups(logGroupNamePrefix=group)[
            "logGroups"
        ]
        if len(log_group_verification) >= 1:
            for existing_loggroup in log_group_verification:
                if existing_loggroup["logGroupName"] == group:
                    return 1
        return 0

    except Exception as e:
        exit(f"EnableVPCFlowLogs failed - unhandled exception {str(e)}")


def wait_for_seconds(wait_interval):
    time.sleep(wait_interval)


def wait_for_loggroup(client, wait_interval, max_retries, loggroup):
    attempts = 1
    while not log_group_exists(client, loggroup):
        wait_for_seconds(wait_interval)
        attempts += 1
        if attempts > max_retries:
            exit(f"Timeout waiting for log group {loggroup} to become active")


def flowlogs_active(client, loggroup):
    # searches for flow log status, filtered on unique CW Log Group created earlier
    try:
        flow_status = client.describe_flow_logs(
            DryRun=False,
            Filters=[
                {"Name": "log-group-name", "Values": [loggroup]},
            ],
        )["FlowLogs"]
        if len(flow_status) == 1 and flow_status[0]["FlowLogStatus"] == "ACTIVE":
            return 1
        else:
            return 0

    except Exception as e:
        exit(f"EnableVPCFlowLogs failed - unhandled exception {str(e)}")


def wait_for_flowlogs(client, wait_interval, max_retries, loggroup):
    attempts = 1
    while not flowlogs_active(client, loggroup):
        wait_for_seconds(wait_interval)
        attempts += 1
        if attempts > max_retries:
            exit(
                f"Timeout waiting for flowlogs to log group {loggroup} to become active"
            )


def enable_flow_logs(event, _):
    """
    remediates CloudTrail.2 by enabling SSE-KMS
    On success returns a string map
    On failure returns NoneType
    """
    max_retries = event.get(
        "retries", 12
    )  # max number of waits for actions to complete.
    wait_interval = event.get("wait", 5)  # how many seconds between attempts

    boto_config = Config(retries={"mode": "standard"})

    if (
        "vpc" not in event
        or "remediation_role" not in event
        or "kms_key_arn" not in event
    ):
        exit("Error: missing vpc from input")

    logs_client = connect_to_logs(boto_config)
    ec2_client = connect_to_ec2(boto_config)

    kms_key_arn = event["kms_key_arn"]  # for logs encryption at rest

    # set dynamic variable for CW Log Group for VPC Flow Logs
    vpc_flow_loggroup = "VPCFlowLogs/" + event["vpc"]
    # create cloudwatch log group
    try:
        logs_client.create_log_group(
            logGroupName=vpc_flow_loggroup, kmsKeyId=kms_key_arn
        )
    except ClientError as client_error:
        exception_type = client_error.response["Error"]["Code"]

        if exception_type in ["ResourceAlreadyExistsException"]:
            print(f"CloudWatch Logs group {vpc_flow_loggroup} already exists")
        else:
            exit(f"ERROR CREATING LOGGROUP {vpc_flow_loggroup}: {str(exception_type)}")

    except Exception as e:
        exit(f"ERROR CREATING LOGGROUP {vpc_flow_loggroup}: {str(e)}")

    # wait for CWL creation to propagate
    wait_for_loggroup(logs_client, wait_interval, max_retries, vpc_flow_loggroup)

    # create VPC Flow Logging
    try:
        ec2_client.create_flow_logs(
            DryRun=False,
            DeliverLogsPermissionArn=event["remediation_role"],
            LogGroupName=vpc_flow_loggroup,
            ResourceIds=[event["vpc"]],
            ResourceType="VPC",
            TrafficType="REJECT",
            LogDestinationType="cloud-watch-logs",
        )
    except ClientError as client_error:
        exception_type = client_error.response["Error"]["Code"]

        if exception_type in ["FlowLogAlreadyExists"]:
            return {
                "response": {
                    "message": f'VPC Flow Logs for {event["vpc"]} already enabled',
                    "status": "Success",
                }
            }
        else:
            exit(f"ERROR CREATING LOGGROUP {vpc_flow_loggroup}: {str(exception_type)}")
    except Exception as e:
        exit(f"create_flow_logs failed {str(e)}")

    # wait for Flow Log creation to propagate. Exits on timeout (no need to check results)
    wait_for_flowlogs(ec2_client, wait_interval, max_retries, vpc_flow_loggroup)

    # wait_for_flowlogs will exit if unsuccessful after max_retries * wait_interval (60 seconds by default)
    return {
        "response": {
            "message": f'VPC Flow Logs enabled for {event["vpc"]} to {vpc_flow_loggroup}',
            "status": "Success",
        }
    }",
              },
              "isEnd": true,
              "name": "Remediation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "Remediation.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "KMSKeyArn": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "default": "{{ssm:/Solutions/SO0111/CMK_REMEDIATION_ARN}}",
              "description": "The ARN of the KMS key created by ASR for remediations requiring encryption",
              "type": "String",
            },
            "RemediationRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "The ARN of the role that will allow VPC Flow Logs to log to CloudWatch logs",
              "type": "String",
            },
            "VPC": {
              "allowedPattern": "^vpc-[0-9a-f]{8,17}$",
              "description": "The VPC ID of the VPC",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnableVPCFlowLogs",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREncryptRDSSnapshot": {
      "DependsOn": [
        "CreateWait3",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{AutomationAssumeRole}}",
          "description": "### Document Name - ASR-EncryptRDSSnapshot

## What does this document do?
This document encrypts an RDS snapshot or cluster snapshot.

## Input Parameters
* SourceDBSnapshotIdentifier: (Required) The name of the unencrypted RDS snapshot. Note that this snapshot will be deleted as part of this document's execution.
* TargetDBSnapshotIdentifier: (Required) The name of the encrypted RDS snapshot to create.
* DBSnapshotType: (Required) The type of snapshot (DB or cluster).
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* KmsKeyId: (Optional) ID, ARN or Alias for the AWS KMS Customer-Managed Key (CMK) to use. If no key is specified, the default encryption key for snapshots (\`alias/aws/rds\`) will be used.

## Output Parameters
* CopyRdsSnapshotToEncryptedRdsSnapshot.EncryptedSnapshotId: The ID of the encrypted RDS snapshot.
* CopyRdsClusterSnapshotToEncryptedRdsClusterSnapshot.EncryptedClusterSnapshotId: The ID of the encrypted RDS cluster snapshot.

## Minimum Permissions Required
* \`rds:CopyDBSnapshot\`
* \`rds:CopyDBClusterSnapshot\`
* \`rds:DescribeDBSnapshots\`
* \`rds:DescribeDBClusterSnapshots\`
* \`rds:DeleteDBSnapshot\`
* \`rds:DeleteDBClusterSnapshot\`

### Key Permissions
If KmsKeyId is a Customer-Managed Key (CMK), then AutomationAssumeRole must have the following permissions on that key:
* \`kms:DescribeKey\`
* \`kms:CreateGrant\`
",
          "mainSteps": [
            {
              "action": "aws:branch",
              "inputs": {
                "Choices": [
                  {
                    "NextStep": "CopyRdsSnapshotToEncryptedRdsSnapshot",
                    "StringEquals": "snapshot",
                    "Variable": "{{DBSnapshotType}}",
                  },
                  {
                    "NextStep": "CopyRdsClusterSnapshotToEncryptedRdsClusterSnapshot",
                    "Or": [
                      {
                        "StringEquals": "cluster-snapshot",
                        "Variable": "{{DBSnapshotType}}",
                      },
                      {
                        "StringEquals": "dbclustersnapshot",
                        "Variable": "{{DBSnapshotType}}",
                      },
                    ],
                  },
                ],
              },
              "name": "ChooseSnapshotOrClusterSnapshot",
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "CopyDBSnapshot",
                "CopyTags": true,
                "KmsKeyId": "{{KmsKeyId}}",
                "Service": "rds",
                "SourceDBSnapshotIdentifier": "{{SourceDBSnapshotIdentifier}}",
                "TargetDBSnapshotIdentifier": "{{TargetDBSnapshotIdentifier}}",
              },
              "name": "CopyRdsSnapshotToEncryptedRdsSnapshot",
              "outputs": [
                {
                  "Name": "EncryptedSnapshotId",
                  "Selector": "$.DBSnapshot.DBSnapshotIdentifier",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "inputs": {
                "Api": "DescribeDBSnapshots",
                "DesiredValues": [
                  "available",
                ],
                "Filters": [
                  {
                    "Name": "db-snapshot-id",
                    "Values": [
                      "{{CopyRdsSnapshotToEncryptedRdsSnapshot.EncryptedSnapshotId}}",
                    ],
                  },
                ],
                "PropertySelector": "$.DBSnapshots[0].Status",
                "Service": "rds",
              },
              "name": "VerifyRdsEncryptedSnapshot",
              "timeoutSeconds": 14400,
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "DeleteDBSnapshot",
                "DBSnapshotIdentifier": "{{SourceDBSnapshotIdentifier}}",
                "Service": "rds",
              },
              "isEnd": true,
              "name": "DeleteUnencryptedRdsSnapshot",
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "CopyDBClusterSnapshot",
                "CopyTags": true,
                "KmsKeyId": "{{KmsKeyId}}",
                "Service": "rds",
                "SourceDBClusterSnapshotIdentifier": "{{SourceDBSnapshotIdentifier}}",
                "TargetDBClusterSnapshotIdentifier": "{{TargetDBSnapshotIdentifier}}",
              },
              "name": "CopyRdsClusterSnapshotToEncryptedRdsClusterSnapshot",
              "outputs": [
                {
                  "Name": "EncryptedClusterSnapshotId",
                  "Selector": "$.DBClusterSnapshot.DBClusterSnapshotIdentifier",
                  "Type": "String",
                },
              ],
            },
            {
              "action": "aws:waitForAwsResourceProperty",
              "inputs": {
                "Api": "DescribeDBClusterSnapshots",
                "DesiredValues": [
                  "available",
                ],
                "Filters": [
                  {
                    "Name": "db-cluster-snapshot-id",
                    "Values": [
                      "{{CopyRdsClusterSnapshotToEncryptedRdsClusterSnapshot.EncryptedClusterSnapshotId}}",
                    ],
                  },
                ],
                "PropertySelector": "$.DBClusterSnapshots[0].Status",
                "Service": "rds",
              },
              "name": "VerifyRdsEncryptedClusterSnapshot",
              "timeoutSeconds": 14400,
            },
            {
              "action": "aws:executeAwsApi",
              "inputs": {
                "Api": "DeleteDBClusterSnapshot",
                "DBSnapshotIdentifier": "{{SourceDBSnapshotIdentifier}}",
                "Service": "rds",
              },
              "isEnd": true,
              "name": "DeleteUnencryptedRdsClusterSnapshot",
            },
          ],
          "outputs": [
            "CopyRdsSnapshotToEncryptedRdsSnapshot.EncryptedSnapshotId",
            "CopyRdsClusterSnapshotToEncryptedRdsClusterSnapshot.EncryptedClusterSnapshotId",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "DBSnapshotType": {
              "allowedValues": [
                "snapshot",
                "cluster-snapshot",
                "dbclustersnapshot",
              ],
              "type": "String",
            },
            "KmsKeyId": {
              "allowedPattern": "^(?:arn:(?:aws|aws-us-gov|aws-cn):kms:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:)?(?:(?:^(alias/)[a-zA-Z0-9:/_-]+$)|(?:key/(?i:[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})))$",
              "default": "alias/aws/rds",
              "description": "(Optional) ID, ARN or Alias for the AWS KMS Customer-Managed Key (CMK) to use to encrypt the snapshot.",
              "type": "String",
            },
            "SourceDBSnapshotIdentifier": {
              "allowedPattern": "^(?:rds:|awsbackup:)?(?!.*--.*)(?!.*-$)[a-zA-Z][a-zA-Z0-9-]{0,254}$",
              "description": "(Required) The name of the unencrypted RDS snapshot or cluster snapshot to copy.",
              "type": "String",
            },
            "TargetDBSnapshotIdentifier": {
              "allowedPattern": "^(?!.*--.*)(?!.*-$)[a-zA-Z][a-zA-Z0-9-]{0,254}$",
              "description": "(Required) The name of the encrypted RDS snapshot or cluster snapshot to create.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EncryptRDSSnapshot",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASREnforceHTTPSForALB": {
      "DependsOn": [
        "CreateWait10",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-EnforceHTTPSForALB

## Overview
This document adds HTTPS enforcement for an ALB.

## Pre-requisites
* The ALB must *not* have a listener on port 80 with a protocol other than HTTP.

## What does this document do?
Creates a new listener for HTTP on port 80 which redirects traffic to HTTPS on port 443. If a listener already exists for HTTP on port 80, 
it will overwrite the default action to redirect to HTTPS on port 443.

## Input Parameters
* ResourceARN: (Required) ARN of the ALB.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* EnforceHTTPSForALB.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Configures a listener on port 80 to redirect HTTP traffic to HTTPS on port 443.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "ResourceARN": "{{ResourceARN}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


def get_elbv2_client():
    return boto3.client("elbv2", config=boto_config)


class Event(TypedDict):
    ResourceARN: str


class Response(TypedDict):
    Message: str
    Status: str


def handler(event: Event, _) -> Response:
    """
    Remediates ELB.1 by adding a listener rule to route HTTP requests to HTTPS.
    """
    try:
        resource_arn = event["ResourceARN"]

        existing_http_listeners = get_existing_http_listener(resource_arn)

        if not existing_http_listeners:
            setup_http_to_https_listener_rule(resource_arn, "")

        for listener_arn in existing_http_listeners:
            setup_http_to_https_listener_rule(resource_arn, listener_arn)
        return {
            "Message": f"Successfully configured HTTPS listener rule for ALB {resource_arn}.",
            "Status": "success",
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error configuring HTTPS listener rule for ALB: {str(e)}"
        )


def get_existing_http_listener(load_balancer_arn: str) -> list[str]:
    try:
        elbv2_client = get_elbv2_client()
        listeners = elbv2_client.describe_listeners(LoadBalancerArn=load_balancer_arn)[
            "Listeners"
        ]
        result = []

        for listener in listeners:
            if listener["Protocol"] == "HTTP":
                result.append(listener["ListenerArn"])
        return result
    except Exception as e:
        raise RuntimeError(
            f"Failed to get existing port 80 rule for ALB {load_balancer_arn}: {str(e)}"
        )


def setup_http_to_https_listener_rule(
    load_balancer_arn: str, listener_arn: str
) -> None:
    try:
        elbv2_client = get_elbv2_client()
        if not listener_arn:
            elbv2_client.create_listener(
                LoadBalancerArn=load_balancer_arn,
                Protocol="HTTP",
                Port=80,
                DefaultActions=[
                    {
                        "Type": "redirect",
                        "RedirectConfig": {
                            "Protocol": "HTTPS",
                            "Port": "443",
                            "Host": "#{host}",
                            "Path": "/#{path}",
                            "Query": "#{query}",
                            "StatusCode": "HTTP_301",
                        },
                    },
                ],
            )
        else:
            elbv2_client.modify_listener(
                ListenerArn=listener_arn,
                DefaultActions=[
                    {
                        "Type": "redirect",
                        "RedirectConfig": {
                            "Protocol": "HTTPS",
                            "Port": "443",
                            "Host": "#{host}",
                            "Path": "/#{path}",
                            "Query": "#{query}",
                            "StatusCode": "HTTP_301",
                        },
                    }
                ],
            )
    except Exception as e:
        raise RuntimeError(
            f"Failed to setup HTTPS listener rule for ALB {load_balancer_arn}: {str(e)}"
        )",
              },
              "name": "EnforceHTTPSForALB",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "EnforceHTTPSForALB.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ResourceARN": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):elasticloadbalancing:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:loadbalancer/app/(?:.+)$",
              "description": "(Required) The Application Load Balancer ARN.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-EnforceHTTPSForALB",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRLimitECSRootFilesystemAccess": {
      "DependsOn": [
        "CreateWait10",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-LimitECSRootFilesystemAccess

## Overview
This document limits root filesystem access to readonly for the ECS Task Definition.

## Pre-requisites
* None

## What does this document do?
Creates a new revision for the non-compliant task definition and sets root filesystem access to readonly.

## Input Parameters
* TaskDefinitionId: (Required) VPC to be remediated.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* LimitECSRootFilesystemAccess.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Creates a new task definition revision with root filesystem access to readonly.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "TaskDefinitionId": "{{TaskDefinitionId}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


class Event(TypedDict):
    TaskDefinitionId: str


class Response(TypedDict):
    message: str
    status: str


def get_ecs_client():
    return boto3.client("ecs", config=boto_config)


def handler(event, _) -> Response:
    """
    Remediates ECS.5 Security Hub finding by creating a new
    revision for the non-compliant Task Definition with readonlyRootFilesystem.
    """
    try:
        task_definition_id = event["TaskDefinitionId"]

        task_definition = get_task_definition(task_definition_id)
        stripped_task_defintion = strip_task_definition(task_definition)

        set_readonly_root_filesystem(stripped_task_defintion)

        new_revision_arn = register_new_revision(stripped_task_defintion)
        return {
            "message": f"Successfully registered new task definition {new_revision_arn}.",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(f"Failed to Limit Root Filesystem access: {str(e)}")


def get_task_definition(task_definition_id: str) -> dict:
    ecs_client = get_ecs_client()
    task_definition = ecs_client.describe_task_definition(
        taskDefinition=task_definition_id,
        include=[
            "TAGS",
        ],
    )
    return task_definition["taskDefinition"]


def strip_task_definition(task_definition: dict) -> dict:
    """
    Creates a new dictionary with only the keys accepted by the RegisterTaskDefinition API.
    """
    accepted_keys = set(task_definition.keys()) - {
        "taskDefinitionArn",
        "revision",
        "compatibilities",
        "status",
        "requiresAttributes",
        "registeredAt",
        "registeredBy",
    }
    return {key: task_definition[key] for key in accepted_keys}


def set_readonly_root_filesystem(task_definition: dict):
    container_definitions = task_definition["containerDefinitions"]
    for container_definition in container_definitions:
        container_definition["readonlyRootFilesystem"] = True


def register_new_revision(task_definition: dict) -> str:
    ecs_client = get_ecs_client()
    response = ecs_client.register_task_definition(**task_definition)
    return response["taskDefinition"]["taskDefinitionArn"]",
              },
              "name": "LimitECSRootFilesystemAccess",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "LimitECSRootFilesystemAccess.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "TaskDefinitionId": {
              "allowedPattern": "^[a-zA-Z0-9_-]{1,255}:\\d$",
              "description": "(Required) The ECS task definition Id.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-LimitECSRootFilesystemAccess",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRMakeEBSSnapshotsPrivate": {
      "DependsOn": [
        "CreateWait1",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-MakeEBSSnapshotPrivate

## What does this document do?
This runbook works an the account level to remove public share on all EBS snapshots

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.

## Output Parameters

* Remediation.Output - stdout messages from the remediation

## Security Standards / Controls
* AWS FSBP v1.0.0: EC2.1
* CIS v1.2.0:   n/a
* PCI:          EC2.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "get_public_snapshots",
                "InputPayload": {
                  "account_id": "{{AccountId}}",
                  "region": "{{global:REGION}}",
                  "testmode": "{{TestMode}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_ec2(boto_config):
    return boto3.client("ec2", config=boto_config)


def get_public_snapshots(event, _):
    account_id = event["account_id"]

    if "testmode" in event and event["testmode"]:
        return [
            "snap-12341234123412345",
            "snap-12341234123412345",
            "snap-12341234123412345",
            "snap-12341234123412345",
            "snap-12341234123412345",
        ]

    return list_public_snapshots(account_id)


def list_public_snapshots(account_id):
    ec2 = connect_to_ec2(boto_config)
    control_token = "start"
    try:
        public_snapshot_ids = []

        while control_token:
            if (
                control_token == "start"
            ):  # needed a value to start the loop. Now reset it
                control_token = ""

            kwargs = {
                "MaxResults": 100,
                "OwnerIds": [account_id],
                "RestorableByUserIds": ["all"],
            }
            if control_token:
                kwargs["NextToken"] = control_token

            response = ec2.describe_snapshots(**kwargs)

            for snapshot in response["Snapshots"]:
                public_snapshot_ids.append(snapshot["SnapshotId"])

            if "NextToken" in response:
                control_token = response["NextToken"]
            else:
                control_token = ""

        return public_snapshot_ids

    except Exception as e:
        print(e)
        exit("Failed to describe_snapshots")",
              },
              "name": "GetPublicSnapshotIds",
              "outputs": [
                {
                  "Name": "Snapshots",
                  "Selector": "$.Payload",
                  "Type": "StringList",
                },
              ],
            },
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "make_snapshots_private",
                "InputPayload": {
                  "region": "{{global:REGION}}",
                  "snapshots": "{{GetPublicSnapshotIds.Snapshots}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config


def connect_to_ec2(boto_config):
    return boto3.client("ec2", config=boto_config)


def make_snapshots_private(event, _):
    boto_config = Config(retries={"mode": "standard", "max_attempts": 10})
    ec2 = connect_to_ec2(boto_config)

    remediated = []
    snapshots = event["snapshots"]

    success_count = 0

    for snapshot_id in snapshots:
        try:
            ec2.modify_snapshot_attribute(
                Attribute="CreateVolumePermission",
                CreateVolumePermission={"Remove": [{"Group": "all"}]},
                SnapshotId=snapshot_id,
            )
            print(f"Snapshot {snapshot_id} permissions set to private")

            remediated.append(snapshot_id)
            success_count += 1
        except Exception as e:
            print(e)
            print(f"FAILED to remediate Snapshot {snapshot_id}")

    result = json.dumps(
        ec2.describe_snapshots(SnapshotIds=remediated), indent=2, default=str
    )
    print(result)

    return {
        "response": {
            "message": f"{success_count} of {len(snapshots)} Snapshot permissions set to private",
            "status": "Success",
        }
    }",
              },
              "name": "Remediation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "Remediation.Output",
          ],
          "parameters": {
            "AccountId": {
              "allowedPattern": "^[0-9]{12}$",
              "description": "Account ID of the account for which snapshots are to be checked.",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "TestMode": {
              "default": false,
              "description": "Enables test mode, which generates a list of fake volume Ids",
              "type": "Boolean",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-MakeEBSSnapshotsPrivate",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRMakeRDSSnapshotPrivate": {
      "DependsOn": [
        "CreateWait2",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-MakeRDSSnapshotPrivate

## What does this document do?
This runbook removes public access to an RDS Snapshot

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* DBSnapshotId: identifier of the public snapshot
* DBSnapshotType: snapshot or cluster-snapshot

## Output Parameters

* Remediation.Output - stdout messages from the remediation

## Security Standards / Controls
* AWS FSBP v1.0.0: RDS.1
* CIS v1.2.0:   n/a
* PCI:          RDS.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "make_snapshot_private",
                "InputPayload": {
                  "DBSnapshotId": "{{DBSnapshotId}}",
                  "DBSnapshotType": "{{DBSnapshotType}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config


def connect_to_rds():
    boto_config = Config(retries={"mode": "standard"})
    return boto3.client("rds", config=boto_config)


def make_snapshot_private(event, _):
    rds_client = connect_to_rds()
    snapshot_id = event["DBSnapshotId"]
    snapshot_type = event["DBSnapshotType"]
    try:
        if snapshot_type == "snapshot":
            rds_client.modify_db_snapshot_attribute(
                DBSnapshotIdentifier=snapshot_id,
                AttributeName="restore",
                ValuesToRemove=["all"],
            )
        elif snapshot_type == "cluster-snapshot":
            rds_client.modify_db_cluster_snapshot_attribute(
                DBClusterSnapshotIdentifier=snapshot_id,
                AttributeName="restore",
                ValuesToRemove=["all"],
            )
        else:
            exit(f"Unrecognized snapshot_type {snapshot_type}")

        print(f"Remediation completed: {snapshot_id} public access removed.")
        return {
            "response": {
                "message": f"Snapshot {snapshot_id} permissions set to private",
                "status": "Success",
            }
        }
    except Exception as e:
        exit(f"Remediation failed for {snapshot_id}: {str(e)}")",
              },
              "name": "MakeRDSSnapshotPrivate",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "MakeRDSSnapshotPrivate.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "DBSnapshotId": {
              "allowedPattern": "^[a-zA-Z](?:[0-9a-zA-Z]+[-]{1})*[0-9a-zA-Z]{1,}$",
              "type": "String",
            },
            "DBSnapshotType": {
              "allowedValues": [
                "cluster-snapshot",
                "snapshot",
              ],
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-MakeRDSSnapshotPrivate",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRRemoveCodeBuildPrivilegedMode": {
      "DependsOn": [
        "CreateWait8",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-RemoveCodeBuildPrivilegedMode

## What does this document do?
This document removes CodeBuild project privileged mode to remove a build project's Docker container access to all devices.

## Input Parameters
* ProjectName: (Required) Name of the CodeBuild project (not the ARN).
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Security Standards / Controls
* AWS FSBP v1.0.0:  CodeBuild.5
* NIST 800-53 Rev5: CodeBuild.5

## Output Parameters
* RemoveCodeBuildPrivilegedMode.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "project_name": "{{ProjectName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_codebuild():
    return boto3.client("codebuild", config=boto_config)


def lambda_handler(event, _):
    """
    Removes CodeBuild privileged mode from a project.

    \`event\` should have the following keys and values:
    \`project_name\`: the name of the codebuild project with privileged mode enabled.

    \`context\` is ignored
    """
    project_name = event["project_name"]

    project_attributes = get_project_info(project_name)

    initial_environment = project_attributes["projects"][0]["environment"]

    initial_environment["privilegedMode"] = False

    remove_privileged_mode(project_name, initial_environment)

    updated_project_attributes = get_project_info(project_name)

    privileged_status = updated_project_attributes["projects"][0]["environment"][
        "privilegedMode"
    ]

    if privileged_status is False:
        return {"privilegedMode": privileged_status}

    raise RuntimeError(
        f"ASR Remediation failed - {project_name} did not have privileged mode removed from project."
    )


def remove_privileged_mode(project_name, environment):
    """
    Removes privileged_status from CodeBuild Project
    """
    codebuild = connect_to_codebuild()
    try:
        codebuild.update_project(name=project_name, environment=environment)

    except Exception as e:
        exit("There was an error updating codebuild project: " + str(e))


def get_project_info(project_name):
    """
    Gets CodeBuild Project info
    """
    codebuild = connect_to_codebuild()
    try:
        project_attributes = codebuild.batch_get_projects(names=[project_name])
        return project_attributes

    except Exception as e:
        exit("Failed to get attributes of project: " + str(e))",
              },
              "name": "RemoveCodeBuildPrivilegedMode",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "RemoveCodeBuildPrivilegedMode.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ProjectName": {
              "allowedPattern": "^[A-Za-z0-9][A-Za-z0-9\\-_]{1,254}$",
              "description": "(Required) The project name (not the ARN).",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-RemoveCodeBuildPrivilegedMode",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRRemoveLambdaPublicAccess": {
      "DependsOn": [
        "CreateWait2",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-RemoveLambdaPublicAccess

## What does this document do?
This document removes the public resource policy. A public resource policy
contains a principal "*" or AWS: "*", which allows public access to the
function. The remediation is to remove the SID of the public policy.

## Input Parameters
* FunctionName: name of the AWS Lambda function that has open access policies
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.

## Output Parameters

* RemoveLambdaPublicAccess.Output - stdout messages from the remediation

## Security Standards / Controls
* AWS FSBP v1.0.0: Lambda.1
* CIS v1.2.0:      n/a
* PCI:             Lambda.1
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "remove_lambda_public_access",
                "InputPayload": {
                  "FunctionName": "{{FunctionName}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_lambda(boto_config):
    return boto3.client("lambda", config=boto_config)


def print_policy_before(policy):
    print("Resource Policy to be deleted:")
    print(json.dumps(policy, indent=2, default=str))


def public_s3_statement_check(statement, principal):
    """
    This function checks if the user has given access to an S3 bucket without providing an AWS account.
    """
    try:
        empty_source_account_check = False
        if "StringEquals" in statement["Condition"]:
            empty_source_account_check = (
                "AWS:SourceAccount" not in statement["Condition"]["StringEquals"]
            )
        else:
            empty_source_account_check = True
        return (
            principal.get("Service", "") == "s3.amazonaws.com"
            and empty_source_account_check
        )
    except KeyError:
        return principal.get("Service", "") == "s3.amazonaws.com"


def remove_resource_policy(functionname, sid, client):
    try:
        client.remove_permission(FunctionName=functionname, StatementId=sid)
        print(f"SID {sid} removed from Lambda function {functionname}")
    except Exception as e:
        exit(
            f"FAILED: SID {sid} was NOT removed from Lambda function {functionname} - {str(e)}"
        )


def remove_public_statement(client, functionname, statement, principal):
    if principal == "*" or (
        isinstance(principal, dict)
        and (
            principal.get("AWS", "") == "*"
            or public_s3_statement_check(statement, principal)
        )
    ):
        print_policy_before(statement)
        remove_resource_policy(functionname, statement["Sid"], client)


def remove_lambda_public_access(event, _):
    client = connect_to_lambda(boto_config)

    functionname = event["FunctionName"]
    try:
        response = client.get_policy(FunctionName=functionname)
        policy = response["Policy"]
        policy_json = json.loads(policy)
        statements = policy_json["Statement"]

        print("Scanning for public resource policies in " + functionname)

        for statement in statements:
            remove_public_statement(
                client, functionname, statement, statement["Principal"]
            )

        client.get_policy(FunctionName=functionname)

        verify(functionname)
    except ClientError as ex:
        exception_type = ex.response["Error"]["Code"]
        if exception_type in ["ResourceNotFoundException"]:
            print("Remediation completed. Resource policy is now empty.")
        else:
            exit(f"ERROR: Remediation failed for RemoveLambdaPublicAccess: {str(ex)}")
    except Exception as e:
        exit(f"ERROR: Remediation failed for RemoveLambdaPublicAccess: {str(e)}")


def verify(function_name_to_check):
    client = connect_to_lambda(boto_config)

    try:
        response = client.get_policy(FunctionName=function_name_to_check)

        print("Remediation executed successfully. Policy after:")
        print(json.dumps(response, indent=2, default=str))

    except ClientError as ex:
        exception_type = ex.response["Error"]["Code"]
        if exception_type in ["ResourceNotFoundException"]:
            print("Remediation completed. Resource policy is now empty.")
        else:
            exit(f"ERROR: {exception_type} on get_policy")
    except Exception as e:
        exit(f"Exception while retrieving lambda function policy: {str(e)}")",
              },
              "name": "RemoveLambdaPublicAccess",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "RemoveLambdaPublicAccess.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "FunctionName": {
              "allowedPattern": "^[a-zA-Z0-9\\-_]{1,64}$",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-RemoveLambdaPublicAccess",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRRemoveUnusedSecret": {
      "DependsOn": [
        "CreateWait13",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-RemoveUnusedSecret

## What does this document do?
  This document deletes a secret that has been unused for the number of days specified in the unusedForDays parameter (Default: 90 days).
  There is a 30 day period to recover the secret after it is deleted.
  [DeleteSecret](https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_DeleteSecret.html) API.


## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* SecretARN: (Required) The ARN of the Secrets Manager secret.
* UnusedForDays: (Optional) Maximum number of days that a secret can remain unused.

## Security Standards / Controls
* AFSBP v1.0.0:  SecretsManager.3
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "SecretARN": "{{ SecretARN }}",
                  "UnusedForDays": "{{ UnusedForDays }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

from datetime import datetime, timezone

import boto3
from botocore.config import Config

BOTO_CONFIG = Config(retries={"mode": "standard", "max_attempts": 10})

# Current date in the same format SecretsManager tracks LastAccessedDate
DATE_TODAY = datetime.now().replace(
    hour=0, minute=0, second=0, microsecond=0, tzinfo=timezone.utc
)


def connect_to_secretsmanager():
    return boto3.client("secretsmanager", config=BOTO_CONFIG)


def lambda_handler(event, _):
    secret_arn = event["SecretARN"]
    unused_for_days = event["UnusedForDays"]

    secretsmanager = connect_to_secretsmanager()

    # Describe the secret
    response = secretsmanager.describe_secret(SecretId=secret_arn)

    # Confirm the secret has been unused for more days than UnusedForDays parameter specifies
    if "LastAccessedDate" in response and (
        DATE_TODAY - response["LastAccessedDate"]
    ).days > int(unused_for_days):
        # Delete the secret, with 30 day recovery window
        response = secretsmanager.delete_secret(
            SecretId=secret_arn,
            RecoveryWindowInDays=30,
        )

        # Confirm secret was scheduled for deletion
        if "DeletionDate" in response:
            return {
                "message": "Deleted the unused secret.",
                "status": "Success",
            }
        else:
            exit(f"Failed to delete the unused secret: {secret_arn}")

    exit(
        f"The secret {secret_arn} cannot be deleted because it has been accessed within the past {unused_for_days} days."
    )",
              },
              "maxAttempts": 3,
              "name": "RemoveUnusedSecret",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "RemoveUnusedSecret.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "SecretARN": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):secretsmanager:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:secret:([A-Za-z0-9\\/_+=.@-]+)$",
              "description": "(Required) The ARN of the Secrets Manager secret.",
              "type": "String",
            },
            "UnusedForDays": {
              "allowedPattern": "^\\d{0,3}$",
              "default": 90,
              "description": "(Optional) Maximum number of days that a secret can remain unused.",
              "type": "Integer",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-RemoveUnusedSecret",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRRemoveVPCDefaultSecurityGroupRules": {
      "DependsOn": [
        "CreateWait6",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-RemoveVPCDefaultSecurityGroupRules

## What does this document do?
This document removes all inbound and outbound rules from the default security group in an Amazon VPC. A default security group is defined as any security group whose name is \`default\`. If the security group ID passed to this automation document belongs to a non-default security group, this document does not perform any changes to the AWS account.

## Input Parameters
* GroupId: (Required) The unique ID of the security group.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* RemoveRulesAndVerify.Output - Success message or failure exception.
",
          "mainSteps": [
            {
              "action": "aws:assertAwsResourceProperty",
              "description": "## CheckDefaultSecurityGroup
Verifies that the security group name does match \`default\`. If the group name does match \`default\`, go to the next step: DescribeSecurityGroups.
",
              "inputs": {
                "Api": "DescribeSecurityGroups",
                "DesiredValues": [
                  "default",
                ],
                "GroupIds": [
                  "{{ GroupId }}",
                ],
                "PropertySelector": "$.SecurityGroups[0].GroupName",
                "Service": "ec2",
              },
              "isCritical": true,
              "maxAttempts": 3,
              "name": "CheckDefaultSecurityGroup",
              "nextStep": "RemoveRulesAndVerify",
              "onFailure": "Abort",
              "timeoutSeconds": 20,
            },
            {
              "action": "aws:executeScript",
              "description": "## RemoveRulesAndVerify
Removes all rules from the default security group.
## Outputs
* Output: Success message or failure exception.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "GroupId": "{{ GroupId }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import Optional, TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


def connect_to_service(service):
    return boto3.client(service, config=boto_config)


class Event(TypedDict):
    GroupId: str


class GetPermissionsResponse(TypedDict):
    IngressPermissions: Optional[list]
    EgressPermissions: Optional[list]


class HandlerResponse(TypedDict):
    Status: str
    Message: str


def handler(event: Event, _) -> HandlerResponse:
    try:
        ec2_client = connect_to_service("ec2")
        group_id = event.get("GroupId")

        ip_permissions = get_permissions(group_id)
        ingress_permissions = ip_permissions.get("IngressPermissions")
        egress_permissions = ip_permissions.get("EgressPermissions")

        if ingress_permissions:
            ec2_client.revoke_security_group_ingress(
                GroupId=group_id, IpPermissions=ingress_permissions
            )
        if egress_permissions:
            ec2_client.revoke_security_group_egress(
                GroupId=group_id, IpPermissions=egress_permissions
            )

        return {
            "Status": "Success",
            "Message": f"Removed VPC default security group rules from group {group_id}",
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error removing VPC default security group rules: {str(e)}"
        )


def get_permissions(group_id: str) -> GetPermissionsResponse:
    ec2_client = connect_to_service("ec2")
    try:
        default_group = ec2_client.describe_security_groups(GroupIds=[group_id]).get(
            "SecurityGroups"
        )[0]
        return {
            "IngressPermissions": default_group.get("IpPermissions"),
            "EgressPermissions": default_group.get("IpPermissionsEgress"),
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error fetching permissions for security group {group_id}: {str(e)}"
        )",
              },
              "isCritical": true,
              "isEnd": true,
              "maxAttempts": 3,
              "name": "RemoveRulesAndVerify",
              "onFailure": "Abort",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 180,
            },
          ],
          "outputs": [
            "RemoveRulesAndVerify.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "GroupId": {
              "allowedPattern": "sg-[a-z0-9]+$",
              "description": "(Required) The unique ID of the security group.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-RemoveVPCDefaultSecurityGroupRules",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRReplaceCodeBuildClearTextCredentials": {
      "DependsOn": [
        "CreateWait2",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-ReplaceCodeBuildClearTextCredentials

## What does this document do?
This document is used to replace environment variables containing clear text credentials in a CodeBuild project with Amazon EC2 Systems Manager Parameters.

## Input Parameters
* ProjectName: (Required) Name of the CodeBuild project (not the ARN).
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* CreateParameters.Parameters - results of the API calls to create SSM parameters
* CreateParameters.Policy - result of the API call to create an IAM policy for the project to access the new parameters
* CreateParameters.AttachResponse - result of the API call to attach the new IAM policy to the project service role
* UpdateProject.Output - result of the API call to update the project environment with the new parameters
",
          "mainSteps": [
            {
              "action": "aws:executeAwsApi",
              "description": "## BatchGetProjects
Gets information about one or more build projects.
",
              "inputs": {
                "Api": "BatchGetProjects",
                "Service": "codebuild",
                "names": [
                  "{{ ProjectName }}",
                ],
              },
              "isCritical": true,
              "maxAttempts": 2,
              "name": "BatchGetProjects",
              "outputs": [
                {
                  "Name": "ProjectInfo",
                  "Selector": "$.projects[0]",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeScript",
              "description": "## CreateParameters
Parses project environment variables for credentials.
Creates SSM parameters.
Returns new project environment variables and SSM parameter information (without values).
",
              "inputs": {
                "Handler": "replace_credentials",
                "InputPayload": {
                  "ProjectInfo": "{{ BatchGetProjects.ProjectInfo }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import re
from json import dumps

from boto3 import client
from botocore.config import Config
from botocore.exceptions import ClientError

boto_config = Config(retries={"mode": "standard"})

CREDENTIAL_NAMES_UPPER = ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]


def connect_to_ssm(boto_config):
    return client("ssm", config=boto_config)


def connect_to_iam(boto_config):
    return client("iam", config=boto_config)


def is_clear_text_credential(env_var):
    if env_var.get("type") != "PLAINTEXT":
        return False
    return any(
        env_var.get("name").upper() == credential_name
        for credential_name in CREDENTIAL_NAMES_UPPER
    )


def get_project_ssm_namespace(project_name):
    return f"/CodeBuild/{project_name}"


def create_parameter(project_name, env_var):
    env_var_name = env_var.get("name")
    parameter_name = f"{get_project_ssm_namespace(project_name)}/env/{env_var_name}"

    ssm_client = connect_to_ssm(boto_config)
    try:
        response = ssm_client.put_parameter(
            Name=parameter_name,
            Description="Automatically created by ASR",
            Value=env_var.get("value"),
            Type="SecureString",
            Overwrite=False,
            DataType="text",
        )
    except ClientError as client_exception:
        exception_type = client_exception.response["Error"]["Code"]
        if exception_type == "ParameterAlreadyExists":
            print(
                f"Parameter {parameter_name} already exists. This remediation may have been run before."
            )
            print("Ignoring exception - remediation continues.")
            response = None
        else:
            exit(f"ERROR: Unhandled client exception: {client_exception}")
    except Exception as e:
        exit(f"ERROR: could not create SSM parameter {parameter_name}: {str(e)}")

    return response, parameter_name


def create_policy(region, account, partition, project_name):
    iam_client = connect_to_iam(boto_config)
    policy_resource_filter = f"arn:{partition}:ssm:{region}:{account}:parameter{get_project_ssm_namespace(project_name)}/*"
    policy_document = {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow",
                "Action": ["ssm:GetParameter", "ssm:GetParameters"],
                "Resource": policy_resource_filter,
            }
        ],
    }
    policy_name = f"CodeBuildSSMParameterPolicy-{project_name}-{region}"
    try:
        response = iam_client.create_policy(
            Description="Automatically created by ASR",
            PolicyDocument=dumps(policy_document),
            PolicyName=policy_name,
        )
    except ClientError as client_exception:
        exception_type = client_exception.response["Error"]["Code"]
        if exception_type == "EntityAlreadyExists":
            print(
                f'Policy {""} already exists. This remediation may have been run before.'
            )
            print("Ignoring exception - remediation continues.")
            # Attach needs to know the ARN of the created policy
            response = {
                "Policy": {
                    "Arn": f"arn:{partition}:iam::{account}:policy/{policy_name}"
                }
            }
        else:
            exit(f"ERROR: Unhandled client exception: {client_exception}")
    except Exception as e:
        exit(f"ERROR: could not create access policy {policy_name}: {str(e)}")
    return response


def attach_policy(policy_arn, service_role_name):
    iam_client = connect_to_iam(boto_config)
    try:
        response = iam_client.attach_role_policy(
            PolicyArn=policy_arn, RoleName=service_role_name
        )
    except ClientError as client_exception:
        exit(f"ERROR: Unhandled client exception: {client_exception}")
    except Exception as e:
        exit(
            f"ERROR: could not attach policy {policy_arn} to role {service_role_name}: {str(e)}"
        )
    return response


def parse_project_arn(arn):
    pattern = re.compile(
        r"arn:(aws[a-zA-Z-]*):codebuild:([a-z]{2}(?:-gov)?-[a-z]+-\\d):(\\d{12}):project/[A-Za-z0-9][A-Za-z0-9\\-_]{1,254}$"
    )
    match = pattern.match(arn)
    if match:
        partition = match.group(1)
        region = match.group(2)
        account = match.group(3)
        return partition, region, account
    else:
        raise ValueError


def replace_credentials(event, _):
    project_info = event.get("ProjectInfo")
    project_name = project_info.get("name")
    project_env = project_info.get("environment")
    project_env_vars = project_env.get("environmentVariables")
    updated_project_env_vars = []
    parameters = []

    for env_var in project_env_vars:
        if is_clear_text_credential(env_var):
            parameter_response, parameter_name = create_parameter(project_name, env_var)
            updated_env_var = {
                "name": env_var.get("name"),
                "type": "PARAMETER_STORE",
                "value": parameter_name,
            }
            updated_project_env_vars.append(updated_env_var)
            parameters.append(parameter_response)
        else:
            updated_project_env_vars.append(env_var)

    updated_project_env = project_env
    updated_project_env["environmentVariables"] = updated_project_env_vars

    partition, region, account = parse_project_arn(project_info.get("arn"))
    policy = create_policy(region, account, partition, project_name)
    service_role_arn = project_info.get("serviceRole")
    service_role_name = service_role_arn[service_role_arn.rfind("/") + 1 :]
    attach_response = attach_policy(policy["Policy"]["Arn"], service_role_name)

    # datetimes are not serializable, so convert them to ISO 8601 strings
    policy_datetime_keys = ["CreateDate", "UpdateDate"]
    for key in policy_datetime_keys:
        if key in policy["Policy"]:
            policy["Policy"][key] = policy["Policy"][key].isoformat()

    return {
        "UpdatedProjectEnv": updated_project_env,
        "Parameters": parameters,
        "Policy": policy,
        "AttachResponse": attach_response,
    }",
              },
              "isCritical": true,
              "name": "CreateParameters",
              "outputs": [
                {
                  "Name": "UpdatedProjectEnv",
                  "Selector": "$.Payload.UpdatedProjectEnv",
                  "Type": "StringMap",
                },
                {
                  "Name": "Parameters",
                  "Selector": "$.Payload.Parameters",
                  "Type": "MapList",
                },
                {
                  "Name": "Policy",
                  "Selector": "$.Payload.Policy",
                  "Type": "StringMap",
                },
                {
                  "Name": "AttachResponse",
                  "Selector": "$.Payload.AttachResponse",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## UpdateProject
Changes the settings of a build project.
",
              "inputs": {
                "Api": "UpdateProject",
                "Service": "codebuild",
                "environment": "{{ CreateParameters.UpdatedProjectEnv }}",
                "name": "{{ ProjectName }}",
              },
              "isCritical": true,
              "isEnd": true,
              "maxAttempts": 2,
              "name": "UpdateProject",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "CreateParameters.Parameters",
            "CreateParameters.Policy",
            "CreateParameters.AttachResponse",
            "UpdateProject.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "ProjectName": {
              "allowedPattern": "^[A-Za-z0-9][A-Za-z0-9\\-_]{1,254}$",
              "description": "(Required) The project name (not the ARN).",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-ReplaceCodeBuildClearTextCredentials",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRRevokeUnauthorizedInboundRules": {
      "DependsOn": [
        "CreateWait13",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-RevokeUnauthorizedInboundRules

## What does this document do?
This document revokes inbound security group rules that allow unrestricted access to ports that are not authorized.
Authorized ports are listed in authorizedTcpPorts and authorizedUdpPorts parameters.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* SecurityGroupId: (Required)  The ID of the Seurity Group.
* AuthorizedTcpPorts: (Optional) List of TCP ports authorized to be open to 0.0.0.0/0 or ::/0.
* AuthorizedUdpPorts: (Optional) List of UDP ports authorized to be open to 0.0.0.0/0 or ::/0.

## Security Standards / Controls
* AFSBP v1.0.0:  EC2.18
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "AuthorizedTcpPorts": "{{ AuthorizedTcpPorts }}",
                  "AuthorizedUdpPorts": "{{ AuthorizedUdpPorts }}",
                  "SecurityGroupId": "{{ SecurityGroupId }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config

BOTO_CONFIG = Config(retries={"mode": "standard", "max_attempts": 10})

# IPV4 and IPV6 open access
OPENIPV4 = "0.0.0.0/0"
OPENIPV6 = "::/0"

PROTOCOLS = {"tcp", "udp", "-1"}


def connect_to_ec2():
    return boto3.client("ec2", config=BOTO_CONFIG)


class Event(TypedDict):
    SecurityGroupId: str
    AuthorizedTcpPorts: list
    AuthorizedUdpPorts: list


def lambda_handler(event: Event, _):
    rules_deleted = []
    try:
        security_group_id = event["SecurityGroupId"]
        authorized_tcp_ports = set(map(int, event["AuthorizedTcpPorts"]))
        authorized_udp_ports = set(map(int, event["AuthorizedUdpPorts"]))

        security_group_rules = get_security_group_rules(security_group_id)

        rules_deleted = revoke_unauthorized_rules(
            security_group_id,
            security_group_rules,
            authorized_tcp_ports,
            authorized_udp_ports,
        )
    except Exception as e:
        raise RuntimeError("Failed to remove security group rules: " + str(e))

    if not rules_deleted:
        raise RuntimeError(
            f"Could not find rules to delete for Security Group {security_group_id}. Please check the inbound "
            f"rules manually."
        )

    return {
        "message": "Successfully removed security group rules on " + security_group_id,
        "status": "Success",
        "rules_deleted": rules_deleted,
    }


def get_security_group_rules(security_group_id: str) -> list:
    ec2 = connect_to_ec2()
    try:
        paginator = ec2.get_paginator("describe_security_group_rules")
        page_iterator = paginator.paginate(
            Filters=[
                {
                    "Name": "group-id",
                    "Values": [security_group_id],
                },
            ]
        )

        security_group_rules = []
        for page in page_iterator:
            security_group_rules.extend(page.get("SecurityGroupRules", []))

        return security_group_rules
    except Exception as e:
        exit("Failed to describe security group rules: " + str(e))


def check_unauthorized_ports(authorized_ports: set, rule: dict) -> bool:
    for port in range(rule["FromPort"], rule["ToPort"] + 1):
        if (port not in authorized_ports) and (
            ("CidrIpv4" in rule and rule["CidrIpv4"] == OPENIPV4)
            or ("CidrIpv6" in rule and rule["CidrIpv6"] == OPENIPV6)
        ):
            return True
    return False


def revoke_unauthorized_rules(
    security_group_id: str,
    security_group_rules: list,
    authorized_tcp_ports: set,
    authorized_udp_ports: set,
) -> list:
    ec2 = connect_to_ec2()
    rules_deleted = []
    for rule in security_group_rules:
        if rule["IpProtocol"] in PROTOCOLS and not rule["IsEgress"]:
            authorized_ports = (
                authorized_tcp_ports
                if rule["IpProtocol"] == "tcp"
                else authorized_udp_ports
            )
            if (rule["FromPort"] == rule["ToPort"] == -1) or check_unauthorized_ports(
                authorized_ports, rule
            ):
                try:
                    ec2.revoke_security_group_ingress(
                        GroupId=security_group_id,
                        SecurityGroupRuleIds=[
                            rule["SecurityGroupRuleId"],
                        ],
                    )
                    rules_deleted.append(rule["SecurityGroupRuleId"])
                except Exception as e:
                    print(
                        f"Failed to delete rule {rule['SecurityGroupRuleId']}: {str(e)}"
                    )
    return rules_deleted",
              },
              "maxAttempts": 3,
              "name": "RevokeUnauthorizedInboundRules",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "RevokeUnauthorizedInboundRules.Output",
          ],
          "parameters": {
            "AuthorizedTcpPorts": {
              "default": [
                "80",
                "443",
              ],
              "description": "(Optional) List of TCP ports authorized to be open to 0.0.0.0/0 or ::/0.",
              "type": "StringList",
            },
            "AuthorizedUdpPorts": {
              "default": [],
              "description": "(Optional) List of UDP ports authorized to be open to 0.0.0.0/0 or ::/0.",
              "type": "StringList",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "SecurityGroupId": {
              "allowedPattern": "^sg-[a-z0-9\\-]+$",
              "description": "(Required) The ID of the Seurity Group.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-RevokeUnauthorizedInboundRules",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRRevokeUnrotatedKeys": {
      "DependsOn": [
        "CreateWait2",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-RevokeUnrotatedKeys

## What does this document do?
This document disables active keys that have not been rotated for more than 90 days. Note that this remediation is **DISRUPTIVE**. It will disabled keys that have been used within the previous 90 days by have not been rotated by using the [UpdateAccessKey API](https://docs.aws.amazon.com/IAM/latest/APIReference/API_UpdateAccessKey.html). Please note, this automation document requires AWS Config to be enabled.

## Input Parameters
* IAMUserName: (Required) User Name for the non-compliant IAM User.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* MaxCredentialUsageAge: (Optional) Maximum number of days a key is allowed to be unrotated before revoking it. DEFAULT: 90

## Output Parameters
* RevokeUnrotatedKeys.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## RevokeUnrotatedKeys

This step deactivates IAM user access keys that have not been rotated in more than MaxCredentialUsageAge days
## Outputs
* Output: Success message or failure Exception.
",
              "inputs": {
                "Handler": "unrotated_key_handler",
                "InputPayload": {
                  "IAMUserName": "{{ IAMUserName }}",
                  "MaxCredentialUsageAge": "{{ MaxCredentialUsageAge }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from datetime import datetime, timezone
from typing import TYPE_CHECKING, Dict, List, Literal, TypedDict

import boto3
from botocore.config import Config

if TYPE_CHECKING:
    from mypy_boto3_iam.type_defs import EmptyResponseMetadataTypeDef
else:
    EmptyResponseMetadataTypeDef = object

boto_config = Config(retries={"mode": "standard"})


class Response(TypedDict):
    AccessKeyId: str
    Response: EmptyResponseMetadataTypeDef


responses: Dict[Literal["DeactivateUnusedKeysResponse"], List[Response]] = {}
responses["DeactivateUnusedKeysResponse"] = []


def connect_to_iam(boto_config):
    return boto3.client("iam", config=boto_config)


def list_access_keys(user_name, include_inactive=False):
    iam_client = connect_to_iam(boto_config)
    active_keys = []
    keys = iam_client.list_access_keys(UserName=user_name).get("AccessKeyMetadata", [])
    for key in keys:
        if include_inactive or key.get("Status") == "Active":
            active_keys.append(key)
    return active_keys


def deactivate_unused_keys(access_keys, max_credential_usage_age, user_name):
    iam_client = connect_to_iam(boto_config)
    for key in access_keys:
        print(key)
        last_used = iam_client.get_access_key_last_used(
            AccessKeyId=key.get("AccessKeyId")
        ).get("AccessKeyLastUsed")
        deactivate = False

        now = datetime.now(timezone.utc)
        days_since_creation = (now - key.get("CreateDate")).days
        last_used_days = (now - last_used.get("LastUsedDate", now)).days

        print(
            f'Key {key.get("AccessKeyId")} is {days_since_creation} days old and last used {last_used_days} days ago'
        )

        if days_since_creation > max_credential_usage_age:
            deactivate = True

        if last_used_days > max_credential_usage_age:
            deactivate = True

        if deactivate:
            deactivate_key(user_name, key.get("AccessKeyId"))


def deactivate_key(user_name, access_key):
    iam_client = connect_to_iam(boto_config)
    responses["DeactivateUnusedKeysResponse"].append(
        {
            "AccessKeyId": access_key,
            "Response": iam_client.update_access_key(
                UserName=user_name, AccessKeyId=access_key, Status="Inactive"
            ),
        }
    )


def verify_expired_credentials_revoked(responses, user_name):
    if responses.get("DeactivateUnusedKeysResponse"):
        for key in responses.get("DeactivateUnusedKeysResponse"):
            # fmt: off
            key_data = next(filter(lambda x: x.get("AccessKeyId") == key.get("AccessKeyId"), list_access_keys(user_name, True),))  # NOSONAR The value key should change at the next loop iteration as we're cycling through each response.
            # fmt: on
            if key_data.get("Status") != "Inactive":
                error_message = (
                    "VERIFICATION FAILED. ACCESS KEY {} NOT DEACTIVATED".format(
                        key_data.get("AccessKeyId")
                    )
                )
                raise RuntimeError(error_message)

    return {
        "output": "Verification of unrotated access keys is successful.",
        "http_responses": responses,
    }


def unrotated_key_handler(event, _):
    user_name = event.get("IAMUserName")
    max_credential_usage_age = int(event.get("MaxCredentialUsageAge"))
    access_keys = list_access_keys(user_name)
    deactivate_unused_keys(access_keys, max_credential_usage_age, user_name)
    return verify_expired_credentials_revoked(responses, user_name)",
              },
              "isEnd": true,
              "name": "RevokeUnrotatedKeys",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "RevokeUnrotatedKeys.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "IAMUserName": {
              "allowedPattern": "^[\\w+=,.@_-]{1,128}$",
              "description": "(Required) IAM resource unique identifier.",
              "type": "String",
            },
            "MaxCredentialUsageAge": {
              "allowedPattern": "^(?:[1-9]\\d{0,3}|10000)$",
              "default": "90",
              "description": "(Optional) Maximum number of days within which a credential must be used. The default value is 90 days.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-RevokeUnrotatedKeys",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRRevokeUnusedIAMUserCredentials": {
      "DependsOn": [
        "CreateWait7",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - AWSConfigRemediation-RevokeUnusedIAMUserCredentials

## What does this document do?
This document revokes unused IAM passwords and active access keys. This document will deactivate expired access keys by using the [UpdateAccessKey API](https://docs.aws.amazon.com/IAM/latest/APIReference/API_UpdateAccessKey.html) and delete expired login profiles by using the [DeleteLoginProfile API](https://docs.aws.amazon.com/IAM/latest/APIReference/API_DeleteLoginProfile.html). Please note, this automation document requires AWS Config to be enabled.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* IAMUserName: (Required) User Name for the non-compliant IAM User.
* MaxCredentialUsageAge: (Required) Maximum number of days within which a credential must be used. The default value is 90 days.

## Output Parameters
* RevokeUnusedIAMUserCredentials.Output - Success message or failure Exception.
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## RevokeUnusedIAMUserCredentials
This step deactivates expired IAM User access keys and deletes expired login profiles
## Outputs
* Output: Success message or failure Exception.
",
              "inputs": {
                "Handler": "handler",
                "InputPayload": {
                  "IAMUserName": "{{ IAMUserName }}",
                  "MaxCredentialUsageAge": "{{ MaxCredentialUsageAge }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from datetime import datetime, timezone
from typing import Optional, TypedDict

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

boto_config = Config(retries={"mode": "standard"})


def connect_to_service(service):
    return boto3.client(service, config=boto_config)


class Event(TypedDict):
    IAMUserName: str
    MaxCredentialUsageAge: str


class HandlerResponse(TypedDict):
    Message: str
    Status: str
    DeactivatedKeys: str
    DeletedProfile: Optional[str]


class LoginProfile(TypedDict):
    UserName: str
    CreateDate: datetime
    PasswordResetRequired: bool


def handler(event, _) -> HandlerResponse:
    try:
        user_name = event.get("IAMUserName")

        max_credential_usage_age = int(event.get("MaxCredentialUsageAge"))

        access_keys = list_access_keys(user_name)
        deactivated_keys = deactivate_unused_keys(
            access_keys, max_credential_usage_age, user_name
        )

        deleted_profile = delete_unused_password(user_name, max_credential_usage_age)

        return {
            "Message": "Successfully revoked unused IAM user credentials",
            "Status": "Success",
            "DeactivatedKeys": str(deactivated_keys),
            "DeletedProfile": deleted_profile,
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error while revoking unusued IAM user credentials: {str(e)}"
        )


def list_access_keys(user_name: str) -> list:
    iam_client = connect_to_service("iam")
    try:
        paginator = iam_client.get_paginator("list_access_keys")
        access_keys = []

        for page in paginator.paginate(UserName=user_name):
            access_keys.extend(page.get("AccessKeyMetadata", []))

        return access_keys
    except Exception as e:
        raise RuntimeError(
            f"Encountered error listing access keys for user {user_name}: {str(e)}"
        )


def deactivate_key(user_name: str, access_key: str) -> Optional[str]:
    iam_client = connect_to_service("iam")
    try:
        iam_client.update_access_key(
            UserName=user_name, AccessKeyId=access_key, Status="Inactive"
        ),
        return access_key
    except ClientError:
        return None
    except Exception as e:
        raise RuntimeError(
            f"Encountered error deactivating access key {access_key} for user {user_name}: {str(e)}"
        )


def deactivate_unused_keys(
    access_keys: list, max_credential_usage_age: int, user_name: str
) -> list[str]:
    iam_client = connect_to_service("iam")
    try:
        deactivated_keys = []
        for key in access_keys:
            last_used = iam_client.get_access_key_last_used(
                AccessKeyId=key.get("AccessKeyId")
            ).get("AccessKeyLastUsed")
            if last_used.get("LastUsedDate"):
                last_used_date = last_used.get("LastUsedDate")
                days_since_last_used = (
                    datetime.now(timezone.utc) - last_used_date
                ).days
                if days_since_last_used >= max_credential_usage_age:
                    deactivated_keys.append(
                        deactivate_key(user_name, key.get("AccessKeyId"))
                    )
            else:
                create_date = key.get("CreateDate")
                days_since_creation = (datetime.now(timezone.utc) - create_date).days
                if days_since_creation >= max_credential_usage_age:
                    deactivated_keys.append(
                        deactivate_key(user_name, key.get("AccessKeyId"))
                    )
        return [key for key in deactivated_keys if key]
    except Exception as e:
        raise RuntimeError(
            f"Encountered error deactivating unused access keys: {str(e)}"
        )


def get_login_profile(user_name: str) -> Optional[LoginProfile]:
    iam_client = connect_to_service("iam")
    try:
        return iam_client.get_login_profile(UserName=user_name)["LoginProfile"]
    except iam_client.exceptions.NoSuchEntityException:
        return None


def delete_unused_password(
    user_name: str, max_credential_usage_age: int
) -> Optional[str]:
    iam_client = connect_to_service("iam")
    try:
        user = iam_client.get_user(UserName=user_name).get("User")

        days_since_password_last_used = 0
        login_profile = get_login_profile(user_name)

        if login_profile and user.get("PasswordLastUsed"):
            password_last_used = user.get("PasswordLastUsed")
            days_since_password_last_used = (
                datetime.now(timezone.utc) - password_last_used
            ).days
        elif login_profile and not user.get("PasswordLastUsed"):
            password_creation_date = login_profile.get("CreateDate")
            days_since_password_last_used = (
                datetime.now(timezone.utc) - password_creation_date
            ).days
        if days_since_password_last_used >= max_credential_usage_age:
            iam_client.delete_login_profile(UserName=user_name)
            return user_name
    except Exception as e:
        raise RuntimeError(
            f"Encountered error deleting unused password for user {user_name}: {str(e)}"
        )",
              },
              "isEnd": true,
              "name": "RevokeUnusedIAMUserCredentials",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "RevokeUnusedIAMUserCredentials.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "IAMUserName": {
              "allowedPattern": "^[\\w+=,.@_-]{1,128}$",
              "description": "(Required) User Name for the non-compliant IAM User.",
              "type": "String",
            },
            "MaxCredentialUsageAge": {
              "allowedPattern": "^(\\b([0-9]|[1-8][0-9]|9[0-9]|[1-8][0-9]{2}|9[0-8][0-9]|99[0-9]|[1-8][0-9]{3}|9[0-8][0-9]{2}|99[0-8][0-9]|999[0-9]|10000)\\b)$",
              "default": "90",
              "description": "(Required) Maximum number of days within which a credential must be used. The default value is 90 days.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-RevokeUnusedIAMUserCredentials",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRS3BlockDenylist": {
      "DependsOn": [
        "CreateWait3",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-S3BlockDenyList

## What does this document do?
This document adds an explicit DENY to the bucket policy to prevent cross-account access to specific sensitive API calls. By default these are s3:DeleteBucketPolicy, s3:PutBucketAcl, s3:PutBucketPolicy, s3:PutEncryptionConfiguration, and s3:PutObjectAcl.

## Input Parameters
* BucketName: (Required) Bucket whose bucket policy is to be restricted.
* DenyList: (Required) List of permissions to be explicitly denied when the Principal contains a role or user in another account.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* PutS3BucketPolicyDeny.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## PutS3BucketPolicyDeny
Adds an explicit deny to the bucket policy for specific restricted permissions.
",
              "inputs": {
                "Handler": "update_bucket_policy",
                "InputPayload": {
                  "accountid": "{{global:ACCOUNT_ID}}",
                  "bucket": "{{BucketName}}",
                  "denylist": "{{DenyList}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
"""
Given a bucket name and list of "sensitive" IAM permissions that shall not be
allowed cross-account, create an explicit deny policy for all cross-account
principals, denying access to all IAM permissions in the deny list for all
resources.

Note:
- The deny list is a comma-separated list configured on the Config rule in parameter blacklistedActionPattern
"""
import copy
import json
from typing import Any, Dict

import boto3
from botocore.config import Config

BOTO_CONFIG = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_s3():
    return boto3.client("s3", config=BOTO_CONFIG)


def get_partition():
    return (
        boto3.client("sts", config=BOTO_CONFIG)
        .get_caller_identity()
        .get("Arn")
        .split(":")[1]
    )


class BucketToRemediate:
    def __init__(self, bucket_name):
        self.bucket_name = bucket_name
        self.get_partition_where_running()
        self.initialize_bucket_policy_to_none()

    def __str__(self):
        return json.dumps(self.__dict__)

    def initialize_bucket_policy_to_none(self):
        self.bucket_policy = None

    def get_partition_where_running(self):
        self.partition = get_partition()

    def set_account_id_from_event(self, event):
        self.account_id = event.get("accountid") or exit("AWS Account not specified")

    def set_denylist_from_event(self, event):
        self.denylist = event.get("denylist").split(",") or exit(
            "DenyList is empty or not a comma-delimited string"
        )  # Expect a comma seperated list in a string

    def get_current_bucket_policy(self):
        try:
            self.bucket_policy = (
                connect_to_s3()
                .get_bucket_policy(
                    Bucket=self.bucket_name, ExpectedBucketOwner=self.account_id
                )
                .get("Policy")
            )

        except Exception as e:
            print(e)
            exit(
                f"Failed to retrieve the bucket policy: {self.account_id} {self.bucket_name}"
            )

    def update_bucket_policy(self):
        try:
            connect_to_s3().put_bucket_policy(
                Bucket=self.bucket_name,
                ExpectedBucketOwner=self.account_id,
                Policy=self.bucket_policy,
            )
        except Exception as e:
            print(e)
            exit(
                f"Failed to store the new bucket policy: {self.account_id} {self.bucket_name}"
            )

    def __principal_is_asterisk(self, principals):
        return True if isinstance(principals, str) and principals == "*" else False

    def get_account_principals_from_bucket_policy_statement(self, statement_principals):
        aws_account_principals = []
        for principal_type, principal in statement_principals.items():
            if principal_type != "AWS":
                continue  # not an AWS account
            aws_account_principals = (
                principal if isinstance(principal, list) else [principal]
            )
        return aws_account_principals

    def create_explicit_deny_in_bucket_policy(self):
        new_bucket_policy = json.loads(self.bucket_policy)  # type: ignore[arg-type]
        deny_statement = DenyStatement(self)
        for statement in new_bucket_policy["Statement"]:
            principals = statement.get("Principal", None)
            if principals and not self.__principal_is_asterisk(principals):
                account_principals = (
                    self.get_account_principals_from_bucket_policy_statement(
                        copy.deepcopy(principals)
                    )
                )
                deny_statement.add_next_principal_to_deny(
                    account_principals, self.account_id
                )

        if (
            deny_statement.deny_statement_json
            and len(deny_statement.deny_statement_json["Principal"]["AWS"]) > 0
        ):
            new_bucket_policy["Statement"].append(deny_statement.deny_statement_json)
            self.bucket_policy = json.dumps(new_bucket_policy)
            return True


class DenyStatement:
    def __init__(self, bucket_object):
        self.bucket_object = bucket_object
        self.initialize_deny_statement()

    def initialize_deny_statement(self):
        self.deny_statement_json: Dict[str, Any] = {}
        self.deny_statement_json["Effect"] = "Deny"
        self.deny_statement_json["Principal"] = {"AWS": []}
        self.deny_statement_json["Action"] = self.bucket_object.denylist
        self.deny_statement_json["Resource"] = [
            f"arn:{self.bucket_object.partition}:s3:::{self.bucket_object.bucket_name}",
            f"arn:{self.bucket_object.partition}:s3:::{self.bucket_object.bucket_name}/*",
        ]

    def __str__(self):
        return json.dumps(self.deny_statement_json)

    def add_next_principal_to_deny(self, principals_to_deny, bucket_account):
        if len(principals_to_deny) == 0:
            return
        this_principal = principals_to_deny.pop()
        principal_account = this_principal.split(":")[4]
        if principal_account and principal_account != bucket_account:
            self.add_deny_principal(this_principal)

        self.add_next_principal_to_deny(principals_to_deny, bucket_account)

    def add_deny_principal(self, principal_arn):
        if principal_arn not in self.deny_statement_json["Principal"]["AWS"]:
            self.deny_statement_json["Principal"]["AWS"].append(principal_arn)


def update_bucket_policy(event, _):
    def __get_bucket_from_event(event):
        bucket = event.get("bucket") or exit("Bucket not specified")
        return bucket

    bucket_to_update = BucketToRemediate(__get_bucket_from_event(event))
    bucket_to_update.set_denylist_from_event(event)
    bucket_to_update.set_account_id_from_event(event)
    bucket_to_update.get_current_bucket_policy()
    if bucket_to_update.create_explicit_deny_in_bucket_policy():
        bucket_to_update.update_bucket_policy()
    else:
        exit(
            f"Unable to create an explicit deny statement for {bucket_to_update.bucket_name}"
        )",
              },
              "name": "PutS3BucketPolicyDeny",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.output",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "PutS3BucketPolicyDeny.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "(Required) The bucket name (not the ARN).",
              "type": "String",
            },
            "DenyList": {
              "allowedPattern": ".*",
              "description": "(Required) Comma-delimited list (string) of permissions to be explicitly denied when the Principal contains a role or user in another account.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-S3BlockDenylist",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRSetCloudFrontOriginDomain": {
      "DependsOn": [
        "CreateWait11",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-SetCloudFrontOriginDomain

## What does this document do?
  This document updates the origin domain on a given CloudFront distribution to prevent a malicious third party from creating the referenced bucket and serving their own content through your distribution.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* DistributionId: (Required)  ID of the CloudFront Distribution to be updated.

## Security Standards / Controls
* NIST80053 v5.0.0:  CloudFront.12
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "Id": "{{ DistributionId }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import boto3


def lambda_handler(event, _):
    # Initialize the CloudFront client
    cloudfront_client = boto3.client("cloudfront")

    # The ID of the CloudFront distribution you want to update
    distribution_id = event["Id"]

    # Intentionally invalid special-use TLD
    new_origin_domain = "cloudfront12remediation.example"

    # Get the current distribution configuration
    distribution_config = cloudfront_client.get_distribution_config(Id=distribution_id)

    # Update the origin domain in the distribution configuration
    distribution_config["DistributionConfig"]["Origins"]["Items"][0][
        "DomainName"
    ] = new_origin_domain

    # Check if distribution is enabled and disable it
    if distribution_config["DistributionConfig"]["Enabled"]:
        distribution_config["DistributionConfig"]["Enabled"] = False

    # If using an S3 origin type, need to update to custom origin type
    if (
        "S3OriginConfig"
        in distribution_config["DistributionConfig"]["Origins"]["Items"][0]
    ):
        # Remove S3OriginConfig key
        del distribution_config["DistributionConfig"]["Origins"]["Items"][0][
            "S3OriginConfig"
        ]

        # Add CustomOriginConfig key
        distribution_config["DistributionConfig"]["Origins"]["Items"][0][
            "CustomOriginConfig"
        ] = {
            "HTTPPort": 80,
            "HTTPSPort": 443,
            "OriginProtocolPolicy": "http-only",
            "OriginSslProtocols": {"Quantity": 1, "Items": ["TLSv1.2"]},
            "OriginReadTimeout": 30,
            "OriginKeepaliveTimeout": 5,
        }

    # Update the distribution configuration
    cloudfront_client.update_distribution(
        DistributionConfig=distribution_config["DistributionConfig"],
        Id=distribution_id,
        IfMatch=distribution_config["ETag"],
    )

    updated_distribution = cloudfront_client.get_distribution_config(Id=distribution_id)
    updated_origin_domain = updated_distribution["DistributionConfig"]["Origins"][
        "Items"
    ][0]["DomainName"]

    if updated_origin_domain == "cloudfront12remediation.example":
        return {
            "message": "Origin domain updated successfully.",
            "status": "Success",
        }
    else:
        raise RuntimeError(
            "Failed to update the origin domain. Updated origin domain did not match 'cloudfront12remediation.example'"
        )",
              },
              "name": "SetCloudFrontOriginDomain",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "SetCloudFrontOriginDomain.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "DistributionId": {
              "allowedPattern": "^[A-Za-z0-9]*$",
              "description": "(Required) The Distribution ID of the CloudFront distribution.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-SetCloudFrontOriginDomain",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRSetIAMPasswordPolicy": {
      "DependsOn": [
        "CreateWait7",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - AWSConfigRemediation-SetIAMPasswordPolicy

## What does this document do?
This document sets the AWS Identity and Access Management (IAM) user password policy for the AWS account using the [UpdateAccountPasswordPolicy](https://docs.aws.amazon.com/IAM/latest/APIReference/API_UpdateAccountPasswordPolicy.html) API.

## Input Parameters
* AllowUsersToChangePassword: (Optional) Allows all IAM users in your account to use the AWS Management Console to change their own passwords.
* HardExpiry: (Optional) Prevents IAM users from setting a new password after their password has expired.
* MaxPasswordAge: (Optional) The number of days that an IAM user password is valid.
* MinimumPasswordLength: (Optional) The minimum number of characters allowed in an IAM user password.
* PasswordReusePrevention: (Optional) Specifies the number of previous passwords that IAM users are prevented from reusing.
* RequireLowercaseCharacters: (Optional) Specifies whether IAM user passwords must contain at least one lowercase character from the ISO basic Latin alphabet (a to z).
* RequireNumbers: (Optional) Specifies whether IAM user passwords must contain at least one numeric character (0 to 9).
* RequireSymbols: (Optional) pecifies whether IAM user passwords must contain at least one of the following non-alphanumeric characters :! @ \\# $ % ^ * ( ) _ + - = [ ] { } | '
* RequireUppercaseCharacters: (Optional) Specifies whether IAM user passwords must contain at least one uppercase character from the ISO basic Latin alphabet (A to Z).
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
## Output Parameters
* UpdateIAMUserPasswordPolicy.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## UpdateIAMUserPasswordPolicy
Sets or updates the AWS account password policy using input parameters using UpdateAccountPasswordPolicy API.
Verify AWS account password policy using GetAccountPasswordPolicy API.
## Outputs
* Output: Success message with HTTP Response from GetAccountPasswordPolicy API call or failure exception.
",
              "inputs": {
                "Handler": "set_iam_password_policy",
                "InputPayload": {
                  "AllowUsersToChangePassword": "{{ AllowUsersToChangePassword }}",
                  "HardExpiry": "{{ HardExpiry }}",
                  "MaxPasswordAge": "{{ MaxPasswordAge }}",
                  "MinimumPasswordLength": "{{ MinimumPasswordLength }}",
                  "PasswordReusePrevention": "{{ PasswordReusePrevention }}",
                  "RequireLowercaseCharacters": "{{ RequireLowercaseCharacters }}",
                  "RequireNumbers": "{{ RequireNumbers }}",
                  "RequireSymbols": "{{ RequireSymbols }}",
                  "RequireUppercaseCharacters": "{{ RequireUppercaseCharacters }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


def connect_to_service(service):
    return boto3.client(service, config=boto_config)


class Event(TypedDict):
    AllowUsersToChangePassword: bool
    HardExpiry: bool
    MaxPasswordAge: int
    MinimumPasswordLength: int
    PasswordReusePrevention: int
    RequireLowercaseCharacters: bool
    RequireNumbers: bool
    RequireSymbols: bool
    RequireUppercaseCharacters: bool


class Response(TypedDict):
    Status: str
    Message: str
    PasswordPolicy: Event


def set_iam_password_policy(event: Event, _) -> Response:
    iam_client = connect_to_service("iam")
    try:
        iam_client.update_account_password_policy(**event)
        return {
            "Status": "Success",
            "Message": "IAM user password policy updated successfully",
            "PasswordPolicy": event,
        }
    except Exception as e:
        raise RuntimeError(
            f"Encountered error while updating IAM user password policy: {str(e)}"
        )",
              },
              "isEnd": true,
              "name": "UpdateIAMUserPasswordPolicy",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "UpdateIAMUserPasswordPolicy.Output",
          ],
          "parameters": {
            "AllowUsersToChangePassword": {
              "default": false,
              "description": "(Optional) Allows all IAM users in your AWS account to use the AWS Management Console to change their own passwords.",
              "type": "Boolean",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "HardExpiry": {
              "default": false,
              "description": "(Optional) Prevents IAM users from setting a new password after their password has expired.",
              "type": "Boolean",
            },
            "MaxPasswordAge": {
              "allowedPattern": "^\\d{0,3}$|^10[0-8]\\d$|^109[0-5]$",
              "default": 0,
              "description": "(Optional) The number of days that an IAM user password is valid.",
              "type": "Integer",
            },
            "MinimumPasswordLength": {
              "allowedPattern": "^[6-9]$|^[1-9]\\d$|^1[01]\\d$|^12[0-8]$",
              "default": 6,
              "description": "(Optional) The minimum number of characters allowed in an IAM user password.",
              "type": "Integer",
            },
            "PasswordReusePrevention": {
              "allowedPattern": "^\\d{0,1}$|^1\\d$|^2[0-4]$",
              "default": 1,
              "description": "(Optional) Specifies the number of previous passwords that IAM users are prevented from reusing.",
              "type": "Integer",
            },
            "RequireLowercaseCharacters": {
              "default": false,
              "description": "(Optional) Specifies whether IAM user passwords must contain at least one lowercase character from the ISO basic Latin alphabet (a to z).",
              "type": "Boolean",
            },
            "RequireNumbers": {
              "default": false,
              "description": "(Optional) Specifies whether IAM user passwords must contain at least one numeric character (0 to 9).",
              "type": "Boolean",
            },
            "RequireSymbols": {
              "default": false,
              "description": "(Optional) Specifies whether IAM user passwords must contain at least one of the following non-alphanumeric characters :! @ \\# $ % ^ * ( ) _ + - = [ ] { } | '.",
              "type": "Boolean",
            },
            "RequireUppercaseCharacters": {
              "default": false,
              "description": "(Optional) Specifies whether IAM user passwords must contain at least one uppercase character from the ISO basic Latin alphabet (A to Z).",
              "type": "Boolean",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-SetIAMPasswordPolicy",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRSetLogGroupRetentionDays": {
      "DependsOn": [
        "CreateWait13",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-SetLogGroupRetentionDays

## What does this document do?
  This document Sets the CloudWatch Log Group Retention setting to the provided value.

## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* LogGroupArn: (Required)  The ARN of the Log Group.
* RetentionDays: (Optional) The number of days to retain the log events in the specified log group.

## Output Parameters
* SetLogGroupRetentionDays.Output

## Security Standards / Controls
* NIST 800-53 Rev5: CloudWatch.16
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "get_log_group_name_from_arn",
                "InputPayload": {
                  "Arn": "{{ LogGroupArn }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import TypedDict


class GetLogGroupNameFromArnEvent(TypedDict):
    Arn: str


def get_log_group_name_from_arn(event: GetLogGroupNameFromArnEvent, _) -> str:
    return event["Arn"].split(":")[6]",
              },
              "maxAttempts": 3,
              "name": "GetLogGroupNameFromArn",
              "outputs": [
                {
                  "Name": "LogGroupName",
                  "Selector": "$.Payload",
                  "Type": "String",
                },
              ],
              "timeoutSeconds": 600,
            },
            {
              "action": "aws:executeAwsApi",
              "description": "## SetLogGroupRetentionDays
Sets the retention days for a Log Group.
",
              "inputs": {
                "Api": "PutRetentionPolicy",
                "Service": "logs",
                "logGroupName": "{{ GetLogGroupNameFromArn.LogGroupName }}",
                "retentionInDays": "{{ RetentionDays }}",
              },
              "isCritical": true,
              "isEnd": false,
              "maxAttempts": 3,
              "name": "SetLogGroupRetentionDays",
              "outputs": [
                {
                  "Name": "Response",
                  "Selector": "$",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "SetLogGroupRetentionDays.Response",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "LogGroupArn": {
              "allowedPattern": "^(arn:(?:aws|aws-cn|aws-us-gov):logs:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:log-group:[A-Za-z0-9\\/\\-_#]{1,512}:\\*)$",
              "description": "(Required) The name of the log group.",
              "type": "String",
            },
            "RetentionDays": {
              "allowedPattern": "^\\d{0,4}$",
              "default": 365,
              "description": "(Optional) The number of days for the log group's retention period.",
              "type": "Integer",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-SetLogGroupRetentionDays",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRSetS3LifecyclePolicy": {
      "DependsOn": [
        "CreateWait12",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-SetS3LifecyclePolicy

## What does this document do?
  This document sets an example lifecycle policy that transfers objects greater than 10 GB to S3 Intelligent Tiering after 90 days. 
  It is recommended to set lifecycle policies appropriate for the objects stored in your S3 bucket.
  [PutBucketLifecycleConfiguration](https://docs.aws.amazon.com/AmazonS3/latest/API/API_PutBucketLifecycleConfiguration.html) API.


## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* BucketName: (Required)  The name of the S3 bucket.

## Security Standards / Controls
* AFSBP v1.0.0:  S3.13
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "BucketName": "{{ BucketName }}",
                  "TargetExpirationDays": "{{ TargetExpirationDays }}",
                  "TargetTransitionDays": "{{ TargetTransitionDays }}",
                  "TargetTransitionStorageClass": "{{ TargetTransitionStorageClass }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import boto3
from botocore.config import Config

BOTO_CONFIG = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_s3():
    return boto3.client("s3", config=BOTO_CONFIG)


def lambda_handler(event, _):
    bucket_name = event["BucketName"]
    # Convert to int to handle cases where SSM passes these as floats
    target_transition_days = int(event["TargetTransitionDays"])
    target_expiration_days = int(event["TargetExpirationDays"])
    target_transition_storage_class = event["TargetTransitionStorageClass"]
    rule_id = "S3.13 Remediation Example"
    s3 = connect_to_s3()

    lifecycle_policy = {}
    if target_expiration_days != 0:
        lifecycle_policy = {
            "Rules": [
                {
                    "ID": rule_id,
                    "Status": "Enabled",
                    "Expiration": {
                        "Days": target_expiration_days,
                    },
                    "Transitions": [
                        {
                            "Days": target_transition_days,
                            "StorageClass": target_transition_storage_class,
                        },
                    ],
                    "Filter": {
                        "ObjectSizeGreaterThan": 131072,
                    },
                },
            ],
        }
    else:
        lifecycle_policy = {
            "Rules": [
                {
                    "ID": rule_id,
                    "Status": "Enabled",
                    "Transitions": [
                        {
                            "Days": target_transition_days,
                            "StorageClass": target_transition_storage_class,
                        },
                    ],
                    "Filter": {
                        "ObjectSizeGreaterThan": 131072,
                    },
                },
            ],
        }

    # Set example lifecycle policy
    # Moves objects larger than 128 KB to Intelligent Tiering storage class after 30 days
    s3.put_bucket_lifecycle_configuration(
        Bucket=bucket_name, LifecycleConfiguration=lifecycle_policy
    )

    # Get new lifecycle configuration
    lifecycle_config = s3.get_bucket_lifecycle_configuration(
        Bucket=bucket_name,
    )

    if lifecycle_config["Rules"][0]["ID"] == rule_id:
        return {
            "message": "Successfully set example S3 lifecycle policy. Review and update as needed.",
            "status": "Success",
        }

    else:
        raise RuntimeError(
            "Failed to set S3 lifecycle policy. Lifecycle rule ID did not match 'S3.13 Remediation Example'"
        )",
              },
              "maxAttempts": 3,
              "name": "SetS3LifecyclePolicy",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "SetS3LifecyclePolicy.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "(Required) The name of the S3 bucket.",
              "type": "String",
            },
            "TargetExpirationDays": {
              "default": 0,
              "description": "(Optional) The number of days until expiration used for the lifecycle policy.",
              "type": "Integer",
            },
            "TargetTransitionDays": {
              "default": 30,
              "description": "(Optional) The number of days until transition used for the lifecycle policy.",
              "type": "Integer",
            },
            "TargetTransitionStorageClass": {
              "allowedPattern": ".*",
              "default": "INTELLIGENT_TIERING",
              "description": "(Optional) The name of the storage class that will be used for the lifecycle policy.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-SetS3LifecyclePolicy",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRSetSSLBucketPolicy": {
      "DependsOn": [
        "CreateWait2",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-SetSSLBucketPolicy

## What does this document do?
This document adds a bucket policy to require transmission over HTTPS for the given S3 bucket by adding a policy statement to the bucket policy.

## Input Parameters
* AutomationAssumeRole: (Required) The Amazon Resource Name (ARN) of the AWS Identity and Access Management (IAM) role that allows Systems Manager Automation to perform the actions on your behalf.
* BucketName: (Required) Name of the bucket to modify.
* AccountId: (Required) Account to which the bucket belongs

## Output Parameters

* Remediation.Output - stdout messages from the remediation

## Security Standards / Controls
* AWS FSBP v1.0.0: S3.5
* CIS v1.2.0:      n/a
* PCI:             S3.5
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "add_ssl_bucket_policy",
                "InputPayload": {
                  "accountid": "{{AccountId}}",
                  "bucket": "{{BucketName}}",
                  "partition": "{{global:AWS_PARTITION}}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
import json

import boto3
from botocore.config import Config
from botocore.exceptions import ClientError

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_s3():
    return boto3.client("s3", config=boto_config)


def policy_to_add(bucket, partition):
    return {
        "Sid": "AllowSSLRequestsOnly",
        "Action": "s3:*",
        "Effect": "Deny",
        "Resource": [
            f"arn:{partition}:s3:::{bucket}",
            f"arn:{partition}:s3:::{bucket}/*",
        ],
        "Condition": {"Bool": {"aws:SecureTransport": "false"}},
        "Principal": "*",
    }


def new_policy():
    return {"Id": "BucketPolicy", "Version": "2012-10-17", "Statement": []}


def add_ssl_bucket_policy(event, _):
    bucket_name = event["bucket"]
    account_id = event["accountid"]
    aws_partition = event["partition"]
    s3 = connect_to_s3()
    bucket_policy = {}
    try:
        existing_policy = s3.get_bucket_policy(
            Bucket=bucket_name, ExpectedBucketOwner=account_id
        )
        bucket_policy = json.loads(existing_policy["Policy"])
    except ClientError as ex:
        exception_type = ex.response["Error"]["Code"]
        # delivery channel already exists - return
        if exception_type not in ["NoSuchBucketPolicy"]:
            exit(f"ERROR: Boto3 s3 ClientError: {exception_type} - {str(ex)}")
    except Exception as e:
        exit(f"ERROR getting bucket policy for {bucket_name}: {str(e)}")

    if not bucket_policy:
        bucket_policy = new_policy()

    print(f"Existing policy: {bucket_policy}")
    bucket_policy["Statement"].append(policy_to_add(bucket_name, aws_partition))

    try:
        result = s3.put_bucket_policy(
            Bucket=bucket_name,
            Policy=json.dumps(bucket_policy, indent=4, default=str),
            ExpectedBucketOwner=account_id,
        )
        print(result)
    except ClientError as ex:
        exception_type = ex.response["Error"]["Code"]
        exit(f"ERROR: Boto3 s3 ClientError: {exception_type} - {str(ex)}")
    except Exception as e:
        exit(f"ERROR putting bucket policy for {bucket_name}: {str(e)}")

    print(f"New policy: {bucket_policy}")",
              },
              "name": "Remediation",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
            },
          ],
          "outputs": [
            "Remediation.Output",
          ],
          "parameters": {
            "AccountId": {
              "allowedPattern": "^[0-9]{12}$",
              "description": "Account ID of the account for the finding",
              "type": "String",
            },
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "BucketName": {
              "allowedPattern": "(?=^.{3,63}$)(?!^(\\d+\\.)+\\d+$)(^(([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])\\.)*([a-z0-9]|[a-z0-9][a-z0-9\\-]*[a-z0-9])$)",
              "description": "Name of the bucket to have a policy added",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-SetSSLBucketPolicy",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRTagDynamoDBTableResource": {
      "DependsOn": [
        "CreateWait12",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-TagDynamoDBTableResource

## Overview
This document adds required tags to a non-compliant DynamoDB Table.

## Pre-requisites
* (Optional) Configure the tags you would like to add in the Security Hub settings for the DynamoDB.5 control.

## What does this document do?
Tags the given DynamoDB Table with the required tags specified in Security Hub. If no required tags are specified, adds a default tag to the resource.

## Input Parameters
* ResourceArn: (Required) DynamoDB Table to be tagged.
* RequiredTagKeys: (Optional) Security Control Parameters for DynamoDB.5.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* TagDynamoDBTableResource.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Adds required tags to a non-compliant DynamoDB Table.
",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "RequiredTagKeys": "{{RequiredTagKeys}}",
                  "ResourceArn": "{{ResourceArn}}",
                  "ResourceType": "DynamoDBTable",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import List, TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


class Event(TypedDict):
    ResourceArn: str
    ResourceType: str
    RequiredTagKeys: List[str]


def get_guardduty_client():
    return boto3.client("guardduty", config=boto_config)


def get_dynamodb_client():
    return boto3.client("dynamodb", config=boto_config)


def lambda_handler(event, _):
    """
    Remediates untagged resources by tagging them with
    the required tag keys. If no required tag keys are
    specified, tags them with a default key, e.g. "SO0111-ASR-GuardDutyResource".
    """
    try:
        required_tags = get_required_tags_from_event(event)
        resource_arn = event["ResourceArn"]
        resource_type = event["ResourceType"]

        if resource_type == "GuardDuty":
            tag_guardduty_resource(tags=required_tags, resource_arn=resource_arn)
        elif resource_type == "DynamoDBTable":
            tag_dynamodb_table_resource(tags=required_tags, resource_arn=resource_arn)
        return {
            "message": f"Successfully tagged resource {resource_arn}.",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(f"Failed to tag resource: {str(e)}")


def tag_guardduty_resource(tags: List[str], resource_arn: str):
    guardduty_client = get_guardduty_client()
    tags_dict = {tag: "" for tag in tags}
    guardduty_client.tag_resource(ResourceArn=resource_arn, Tags=tags_dict)


def tag_dynamodb_table_resource(tags: List[str], resource_arn: str):
    dynamodb_client = get_dynamodb_client()
    tags_list = [{"Key": tag, "Value": ""} for tag in tags]
    dynamodb_client.tag_resource(ResourceArn=resource_arn, Tags=tags_list)


def get_required_tags_from_event(event):
    required_tag_keys = event.get("RequiredTagKeys", {})
    if required_tag_keys:
        return [tag.strip() for tag in required_tag_keys]
    return None",
              },
              "name": "TagDynamoDBTableResource",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "TagDynamoDBTableResource.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RequiredTagKeys": {
              "description": "Required tag keys for DynamoDB Tables",
              "type": "StringList",
            },
            "ResourceArn": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):dynamodb:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):(?:\\d{12}):table\\/([a-zA-Z0-9._-]{3,255})$",
              "description": "(Required) The DynamoDB Table resource ARN.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-TagDynamoDBTableResource",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRTagGuardDutyResource": {
      "DependsOn": [
        "CreateWait13",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document Name - ASR-TagGuardDutyResource

## Overview
This document adds required tags to a non-compliant GuardDuty resource.

## Pre-requisites
* None

## What does this document do?
Tags the given GuardDuty resource with the required tags specified in Security Hub. If no required tags are specified, adds a default tag to the resource.

## Input Parameters
* ResourceArn: (Required) GuardDuty resource to be tagged.
* RequiredTagKeys: (Optional) Security Control Parameters for GuardDuty.2.
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.

## Output Parameters
* TagGuardDutyResource.Output
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "description": "## Remediation
Adds required tags to a non-compliant GuardDuty resource.
",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "RequiredTagKeys": "{{RequiredTagKeys}}",
                  "ResourceArn": "{{ResourceArn}}",
                  "ResourceType": "GuardDuty",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0
from typing import List, TypedDict

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard"})


class Event(TypedDict):
    ResourceArn: str
    ResourceType: str
    RequiredTagKeys: List[str]


def get_guardduty_client():
    return boto3.client("guardduty", config=boto_config)


def get_dynamodb_client():
    return boto3.client("dynamodb", config=boto_config)


def lambda_handler(event, _):
    """
    Remediates untagged resources by tagging them with
    the required tag keys. If no required tag keys are
    specified, tags them with a default key, e.g. "SO0111-ASR-GuardDutyResource".
    """
    try:
        required_tags = get_required_tags_from_event(event)
        resource_arn = event["ResourceArn"]
        resource_type = event["ResourceType"]

        if resource_type == "GuardDuty":
            tag_guardduty_resource(tags=required_tags, resource_arn=resource_arn)
        elif resource_type == "DynamoDBTable":
            tag_dynamodb_table_resource(tags=required_tags, resource_arn=resource_arn)
        return {
            "message": f"Successfully tagged resource {resource_arn}.",
            "status": "Success",
        }
    except Exception as e:
        raise RuntimeError(f"Failed to tag resource: {str(e)}")


def tag_guardduty_resource(tags: List[str], resource_arn: str):
    guardduty_client = get_guardduty_client()
    tags_dict = {tag: "" for tag in tags}
    guardduty_client.tag_resource(ResourceArn=resource_arn, Tags=tags_dict)


def tag_dynamodb_table_resource(tags: List[str], resource_arn: str):
    dynamodb_client = get_dynamodb_client()
    tags_list = [{"Key": tag, "Value": ""} for tag in tags]
    dynamodb_client.tag_resource(ResourceArn=resource_arn, Tags=tags_list)


def get_required_tags_from_event(event):
    required_tag_keys = event.get("RequiredTagKeys", {})
    if required_tag_keys:
        return [tag.strip() for tag in required_tag_keys]
    return None",
              },
              "name": "TagGuardDutyResource",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload.response",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "TagGuardDutyResource.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "RequiredTagKeys": {
              "default": [
                "SO0111-ASR-GuardDutyResource",
              ],
              "description": "Required tag keys for GuardDuty.2",
              "type": "StringList",
            },
            "ResourceArn": {
              "allowedPattern": "(arn:(?:aws|aws-cn|aws-us-gov):guardduty:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:detector\\/.*)",
              "description": "(Required) The GuardDuty resource ARN.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-TagGuardDutyResource",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "ASRUpdateSecretRotationPeriod": {
      "DependsOn": [
        "CreateWait12",
      ],
      "Properties": {
        "Content": {
          "assumeRole": "{{ AutomationAssumeRole }}",
          "description": "### Document name - ASR-UpdateSecretRotationPeriod

## What does this document do?
  This document rotates a secret and sets its rotation period to 90 days.
  [RotateSecret](https://docs.aws.amazon.com/secretsmanager/latest/apireference/API_RotateSecret.html) API.


## Input Parameters
* AutomationAssumeRole: (Required) The ARN of the role that allows Automation to perform the actions on your behalf.
* SecretARN: (Required)  The ARN of the Secrets Manager secret.

## Security Standards / Controls
* AFSBP v1.0.0:  SecretsManager.4
",
          "mainSteps": [
            {
              "action": "aws:executeScript",
              "inputs": {
                "Handler": "lambda_handler",
                "InputPayload": {
                  "MaxDaysSinceRotation": "{{ MaxDaysSinceRotation }}",
                  "SecretARN": "{{ SecretARN }}",
                },
                "Runtime": "python3.11",
                "Script": "# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
# SPDX-License-Identifier: Apache-2.0

import boto3
from botocore.config import Config

boto_config = Config(retries={"mode": "standard", "max_attempts": 10})


def connect_to_secretsmanager():
    return boto3.client("secretsmanager", config=boto_config)


def lambda_handler(event, _):
    secret_arn = event["SecretARN"]
    max_days_since_rotation = event["MaxDaysSinceRotation"]

    secretsmanager = connect_to_secretsmanager()

    try:
        # Rotate secret and set rotation schedule
        secretsmanager.rotate_secret(
            SecretId=secret_arn,
            RotationRules={
                "AutomaticallyAfterDays": max_days_since_rotation,
            },
            RotateImmediately=True,
        )

        # Verify secret rotation schedule updated.
        response = secretsmanager.describe_secret(SecretId=secret_arn)

        if "RotationRules" in response:
            if (
                response["RotationRules"]["AutomaticallyAfterDays"]
                <= max_days_since_rotation
            ):
                return {
                    "message": f"Rotated secret and set rotation schedule to {max_days_since_rotation} days.",
                    "status": "Success",
                }
        else:
            return {
                "message": "Failed to rotate secret and set rotation schedule.",
                "status": "Failed",
            }

    # If secret was already rotated, an exception will be thrown.
    except Exception as e:
        # Verify secret rotation schedule updated.
        response = secretsmanager.describe_secret(SecretId=secret_arn)

        if "RotationRules" in response:
            if (
                response["RotationRules"]["AutomaticallyAfterDays"]
                <= max_days_since_rotation
            ):
                return {
                    "message": f"Set rotation schedule to {max_days_since_rotation} days. Secret is already being rotated.",
                    "status": "Success",
                }
        else:
            return {
                "message": f"Failed to rotate secret and set rotation schedule: {str(e)}",
                "status": "Failed",
            }",
              },
              "maxAttempts": 3,
              "name": "UpdateSecretRotationPeriod",
              "outputs": [
                {
                  "Name": "Output",
                  "Selector": "$.Payload",
                  "Type": "StringMap",
                },
              ],
              "timeoutSeconds": 600,
            },
          ],
          "outputs": [
            "UpdateSecretRotationPeriod.Output",
          ],
          "parameters": {
            "AutomationAssumeRole": {
              "allowedPattern": "^arn:(?:aws|aws-us-gov|aws-cn):iam::\\d{12}:role/[\\w+=,.@-]+$",
              "description": "(Required) The ARN of the role that allows Automation to perform the actions on your behalf.",
              "type": "String",
            },
            "MaxDaysSinceRotation": {
              "allowedPattern": "^\\d{0,3}$",
              "default": 90,
              "description": "(Optional) The number of days set for the secret's rotation period.",
              "type": "Integer",
            },
            "SecretARN": {
              "allowedPattern": "^arn:(?:aws|aws-cn|aws-us-gov):secretsmanager:(?:[a-z]{2}(?:-gov)?-[a-z]+-\\d):\\d{12}:secret:([A-Za-z0-9\\/_+=.@-]+)$",
              "description": "(Required) The ARN of the Secrets Manager secret.",
              "type": "String",
            },
          },
          "schemaVersion": "0.3",
        },
        "DocumentFormat": "YAML",
        "DocumentType": "Automation",
        "Name": "ASR-UpdateSecretRotationPeriod",
        "UpdateMethod": "NewVersion",
      },
      "Type": "AWS::SSM::Document",
    },
    "CreateWait0": {
      "DeletionPolicy": "Delete",
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait1": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait0",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait10": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait9",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait11": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait10",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait12": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait11",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait13": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait12",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait14": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait13",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait15": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait14",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait2": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait1",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait3": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait2",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait4": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait3",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait5": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait4",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait6": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait5",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait7": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait6",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait8": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait7",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "CreateWait9": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "CreateWait8",
      ],
      "Properties": {
        "CreateIntervalSeconds": 1,
        "DeleteIntervalSeconds": 0,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 1,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait0": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRCreateCloudTrailMultiRegionTrail",
        "ASRCreateLogMetricFilterAndAlarm",
        "ASREnableAutoScalingGroupELBHealthCheck",
        "ASREnableAWSConfig",
        "ASREnableCloudTrailToCloudWatchLogging",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait1": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRCreateAccessLoggingBucket",
        "ASREnableCloudTrailEncryption",
        "ASREnableDefaultEncryptionS3",
        "ASREnableVPCFlowLogs",
        "ASRMakeEBSSnapshotsPrivate",
        "DeletWait0",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait10": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRConfigureDynamoDBAutoScaling",
        "ASREnableDynamoDBDeletionProtection",
        "ASREnableElastiCacheBackups",
        "ASREnforceHTTPSForALB",
        "ASRLimitECSRootFilesystemAccess",
        "DeletWait9",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait11": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRDisableUnrestrictedAccessToHighRiskPorts",
        "ASREnableElastiCacheReplicationGroupFailover",
        "ASREnableElastiCacheVersionUpgrades",
        "ASREnablePrivateRepositoryScanning",
        "ASRSetCloudFrontOriginDomain",
        "DeletWait10",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait12": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRDisableTGWAutoAcceptSharedAttachments",
        "ASREnableGuardDuty",
        "ASRSetS3LifecyclePolicy",
        "ASRTagDynamoDBTableResource",
        "ASRUpdateSecretRotationPeriod",
        "DeletWait11",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait13": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASREnableAutoSecretRotation",
        "ASRRemoveUnusedSecret",
        "ASRRevokeUnauthorizedInboundRules",
        "ASRSetLogGroupRetentionDays",
        "ASRTagGuardDutyResource",
        "DeletWait12",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait14": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRConfigureAutoScalingLaunchConfigNoPublicIP",
        "ASRConfigureAutoScalingLaunchConfigToRequireIMDSv2",
        "ASREnableAPIGatewayCacheDataEncryption",
        "ASREnableAPIGatewayExecutionLogs",
        "ASREnableMacie",
        "DeletWait13",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait15": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASREnableAthenaWorkGroupLogging",
        "DeletWait14",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait2": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRMakeRDSSnapshotPrivate",
        "ASRRemoveLambdaPublicAccess",
        "ASRReplaceCodeBuildClearTextCredentials",
        "ASRRevokeUnrotatedKeys",
        "ASRSetSSLBucketPolicy",
        "DeletWait1",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait3": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRDisablePublicAccessToRedshiftCluster",
        "ASREnableAutomaticVersionUpgradeOnRedshiftCluster",
        "ASREnableRedshiftClusterAuditLogging",
        "ASREncryptRDSSnapshot",
        "ASRS3BlockDenylist",
        "DeletWait2",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait4": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRConfigureS3BucketPublicAccessBlock",
        "ASRConfigureSNSTopicForStack",
        "ASRCreateIAMSupportRole",
        "ASREnableAutomaticSnapshotsOnRedshiftCluster",
        "ASREnableEncryptionForSQSQueue",
        "DeletWait3",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait5": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRConfigureS3PublicAccessBlock",
        "ASREnableCloudTrailLogFileValidation",
        "ASREnableEbsEncryptionByDefault",
        "ASREnableEnhancedMonitoringOnRDSInstance",
        "ASREnableKeyRotation",
        "DeletWait4",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait6": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASREnableCopyTagsToSnapshotOnRDSCluster",
        "ASREnableMultiAZOnRDSInstance",
        "ASREnableRDSClusterDeletionProtection",
        "ASREnableRDSInstanceDeletionProtection",
        "ASRRemoveVPCDefaultSecurityGroupRules",
        "DeletWait5",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait7": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRDisablePublicAccessToRDSInstance",
        "ASREnableEncryptionForSNSTopic",
        "ASREnableMinorVersionUpgradeOnRDSDBInstance",
        "ASRRevokeUnusedIAMUserCredentials",
        "ASRSetIAMPasswordPolicy",
        "DeletWait6",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait8": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRDisablePublicIPAutoAssign",
        "ASREnableCloudFrontDefaultRootObject",
        "ASREnableDeliveryStatusLoggingForSNSTopic",
        "ASREnableIMDSV2OnInstance",
        "ASRRemoveCodeBuildPrivilegedMode",
        "DeletWait7",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
    "DeletWait9": {
      "DeletionPolicy": "Delete",
      "DependsOn": [
        "ASRAttachServiceVPCEndpoint",
        "ASRAttachSSMPermissionsToEC2",
        "ASRBlockSSMDocumentPublicAccess",
        "ASREnableBucketEventNotifications",
        "ASREnableSSMDocumentBlockPublicSharing",
        "DeletWait8",
      ],
      "Properties": {
        "CreateIntervalSeconds": 0,
        "DeleteIntervalSeconds": 0.5,
        "DocumentPropertiesHash": "Omitted to remove snapshot dependency on document hash",
        "ServiceToken": {
          "Ref": "WaitProviderServiceToken",
        },
        "UpdateIntervalSeconds": 0,
      },
      "Type": "Custom::Wait",
      "UpdateReplacePolicy": "Delete",
    },
  },
}
`;
